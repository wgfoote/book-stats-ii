% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode=true}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Stats II - Course Notebook},
  pdfauthor={Bill Foote},
  hidelinks,
}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage[]{natbib}
\bibliographystyle{plainnat}

\title{Stats II - Course Notebook}
\author{Bill Foote}
\date{2026-01-11}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{definition}
\newtheorem{hypothesis}{Hypothesis}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{preamble}{%
\chapter*{Preamble}\label{preamble}}
\addcontentsline{toc}{chapter}{Preamble}

This book is a compilation of many years of experience with replying to clients (CFO, CEO, business unit leader, project manager, etc., board members) and their exclamations and desire for the number. The number might be the next quarter's earnings per share after a failed product launch, customer switching cost and churn with the entry of a new competitor, stock return volatility and attracting managerial talent, fuel switch timing in a fleet of power plants, fastest route to ship soy from Sao Paolo to Lianyungang, launch date for refractive surgery services, relocation for end state renal care facilities, and so on, and so much so forth.

Here is a highly redacted scene of a segment of a board meeting of a medical devices company in the not so distant past. All of the characters are of course fictions of a vivid imagination. The scene and the company are all too real.

Sara, a board member, has the floor.

\begin{quote}
Sara: Bill, just give us the number!
\end{quote}

Bill (no relation) is the consultant that the CFO hired to provide some insight into a business issue: how many product units can be shipped in 45 days from a port on the Gulf of Mexico, USA to a port in Jiangsu Provice, PRC.

\begin{quote}
Bill: Sara, the most plausible number is 42 units through the Canal in 30 days. However, \ldots{}
\end{quote}

Sara interjects.

\begin{quote}
?Sara: Finally! We have a number \ldots{}
\end{quote}

Harrumphing a bit, but quietly and thinking ``Not so fast!'',

\begin{quote}
Bill: Yes, 42 is plausible, but so are 36 and 45, in fact the odds are that these results, given the data and assumptions you provided our team, as well as your own beliefs about the possibilities in the first place, are even. You could bet on any of them and be consistent with the data.
\end{quote}

Jeri the CFO grins, and says to George the CEO in a loud enough whisper so that other board members perked up from their respective trances,

\begin{quote}
Jeri, CFO: That's why we hired Bill and his team. He is giving us the range of the plausible. Now it's up to us to pull the trigger.
\end{quote}

Sara winces, yields the floor to Jeri, who gently explains to the board,

\begin{quote}
Jeri: Thank you Bill and give our regards to your team. Excellent and extensive work in a very short time frame!
\end{quote}

Bill leaves the board room, thinking ``Phew!, made through that gauntlet,
again.''

\begin{quote}
Jeri: Now our work really begins. George, please lead us in our deliberations.
\end{quote}

Pan through the closed board room door outside to the waiting room and
watch Bill, wiping the sweat from his brow, prepare the invoice on his laptop.

A watchful administrative assistant nods his approval.

\hypertarget{why-this-book}{%
\section*{Why this book}\label{why-this-book}}
\addcontentsline{toc}{section}{Why this book}

This book is an analytical palimpsest which attempts to recast and reimagine the Bayesian Stan-based analytics presented by Richard \citet{McElreath_2020} for the data analytics that help business managers and executives make decisions. Richard McElreath's vision and examples, though reworked, are nearly immediately evident throughout. But again this work is a palimpsest with its own quirks, examples, but surely built on McElreath's analytical architecture.

The data for the models is wrangled through the R tidyverse ecosystem and approaches that Solomon Kurz uses in recasting Richard McElreath's text with the brms R package developed by Paul BÃ¼rkner's brms package. The concepts, even the \emph{zeitgeist} of a Danielle Navarro might be evident along with examples from her work on learning and reasoning in the behavioral sciences. And if there was ever a pool of behavior to dip into it is in the business domain.

\hypertarget{premises}{%
\section*{Premise(s)}\label{premises}}
\addcontentsline{toc}{section}{Premise(s)}

The one premise of this book is that
\emph{Learning is inference}.

By inference we mean reaching a conclusion. Conclusions can be either true or false. They are reached by a process of reflection on what is understood and by what is experienced.

Let's clarify these terms in this journey courtesy of Potter (1994):

\begin{itemize}
\item
  Ignorance is simply lack of knowledge.
\item
  Opinion is a very tentative, if not also hesitant, subject to change, assent to a conclusion.
\item
  Belief is a firm conviction.
\item
  Knowledge is belief with sufficient evidence to justify assent.
\end{itemize}

= Doubt suspends assent when evidence is lacking or just too weak.

\begin{itemize}
\tightlist
\item
  Learning can only occur with doubt or ignorance or even opinion; learning generates new knowledge.
\end{itemize}

\citet{lonergan_insight} develops the heuristic structure of human knowing about three successive movements from experience, through understanding, onto to reflection about what is and is not (plausibly of course!). The terms of each movement wind their way into the methodology of any excursus that implements human knowing about anything, including the knower, whom we often tag the analyst or decision maker. The knower observes and experiences, yes, with a bias.

The course will boil down to the statistics (minimum, maximum, mean, quantiles, deviations, skewness, kurtosis) and the probability that the evidence we have to support any proposition(s) we claim. The evidence is the strength (for example in decibels, base 10) of our hypothesis or claim. The measure of evidence is the measure of surprise and its complement informativeness of the data, current and underlying, inherent in the claim. In the interests of putting the bottom line up front Here is the formula for our measure of evidence \(e\) of a hypothesis \(H\) comes from Edwin T. \citet{Jaynes2004}.

\begin{equation}
e(H \mid DX) = e(H \mid X ) + 10 lpg_{10} \left[ \frac{Pr(D \mid HX)}
{Pr(D \mid \overline{H}X)} \right]
\end{equation}

Let's parse this relationship. First \(H\) is the claim, say for example, the typical null hypothesis \(H_0\)
that the status quo is true. If \(H = H_0\) then \(\overline{H}\) (literally not \(H\)) must be
the alternative hypothesis logically so that \(\overline{H} = H_1\).

= \(D\) is the data at hand we
observe. \(X\) is the data, including data about beliefs, we already have at our disposal before we observe \(D\).

Now we look at the combinations. \(DX\) is the logical conjunction of the two data sets. This conjunction represents the proposition that both the new data \(D\) and the old data exist \(X\) and are true. \(HX\) is the phrase both the claim \(H\) is true and the available data \(X\) is true. \(\overline{H}X\) is the phrase: both the claim is false and the available data \(X\) are true. Here are the conditions of the contract.

\begin{itemize}
\item
  We look for evidence \(e(H \mid DX)\) that \(H\) is true given the existence both of new data ð· and available data \(X\), that is \(H \mid DX\) where the \(A \mid B\) reads \emph{A given B}, a conditional statement.
\item
  This compound (\(D X\)) depends on the evidence \(ð‘’(H \mid X)\) that \(H\) is true given knowledge of available data \(X\).
\item
  The evidence impounded in the odds of finding data ð· when the hypothesis is true ð» relative to when the hypothesis is not true ð».
\end{itemize}

Everything is in \(log_{10}\) measures to allow us the luxury of simply adding up evidence scores. What is \(ð»\)? These are the hypotheses, ideas, explanations, parameters (like intercept and slope, means, standard deviations, you name it) we think when we work through answers to questions.

So many questions and too little time Where in the business domain do questions arise? The usual suspects are the components of any organization's, stakeholders', value chain. This chain extends from questions about the who, when, where, how, how long, how much, how often, how many, for whom, for what, and why of each component. There are decisions in each link. In planning an organization's strategy the decisions might be to examine new markets or not, invest in assets or retire them, fund innovation with shareholder equity or stand fast. In each of these decisions are questions about the size and composition of markets and competitors, the generation of cash flow from operations, the capture of value for stakeholders. It is this font from which questions will arise and analysis will attempt to guide.

\hypertarget{dont-we-know-everything-we-need-to-know}{%
\section*{Don't we know everything we need to know?}\label{dont-we-know-everything-we-need-to-know}}
\addcontentsline{toc}{section}{Don't we know everything we need to know?}

We don't seem to know everything all of the time, although we certainly have both many doubts and opinions. These hurdles are often managed through the collection of data against hypotheses, explanations, theories, and ideas. There are two bookend inferences we can make, that is, deduce plausibly.

The first is the glass is half-full approach, \emph{modus ponens}. If A is true, then B is true. We then observe that B is true. We conclude that A becomes more plausible.

Then there is the glass is half-empty view, \emph{modus tollens} We begin again with the implication that if A is true, then B is true. We then add data: A is false. The inference then concludes that B becomes less plausible.

There is no guarantee here, just plausible, but justified, belief. We will call plausibility a measure of belief, also known as probability.

The first chapter will detail the inner workings and cognitional operations at work in these inferences. But let us remember that learning is inference.

\hypertarget{what-we-desire}{%
\section*{What we desire}\label{what-we-desire}}
\addcontentsline{toc}{section}{What we desire}

These two ways of inferring plausible, justified, true belief will be the bedrock of this book. They imply three desiderata of our methods:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Include both the data of sensible observation and the data of assumptions and beliefs.
\item
  Condition ideas, hypotheses, theories, explanations with the data
  of experience and belief.
\item
  Measure the impact of data on hypotheses using a measure of plausibility.
\end{enumerate}

What we will call rational will be the consistency, the compatibility, of data with hypotheses measured by plausibility, to be called posterior probability. The data of beliefs will be contained in what we will call prior probability. The data of beliefs include our assumptions about the distribution of hypotheses.

The conditioning of hypotheses with data is what we will call likelihood. Ultimately what we call uncertainty will be the range and plausibility of the impact of data on hypotheses. What we will solve for is the most plausible explanation given data and belief. Frequentist or probabilistic? Both.

The moving parts of our reasoning demand at least a probabilistic approach if we are to summarily, if not completely ever, deal with uncertainty in our descriptions and explanations of reality. For the business decision maker this mindset becomes critical as the way in which any decision must be made if it is to be compatible with the data of experience, understanding and reflection. Practically speaking the probabilistic approach directly aligns with the heuristic structure of human knowing: experience, understanding, reflection. All are fulfilled virtually (we don't know everything through bias, omission, ignorance, malice) a priori to the decision. Because our understanding of the future is imperfect we insert the word and idea of plausibility into every statement.

Frequentism is a quasi-objective approach to analysis. It is objective in that it focuses on only the data. It is quasi-objective in that the data is collected by human knowers and doers. An assumption of the analysis is always that the data are equally likely to occur. These beings do not always have access to or even want to collect all or the right data for further deliberation. That is an interesting historical, and a priori, fact of human existence. The collection itself rails against the time, space, and resources needed for collection and thus the epithet of garbage-in and garbage-out.

However, frequentist approaches contradict their supposed objectivism in that both the data selected, collected, and deliberated up are subject (yes the subjective begins to enter) to, conditional on, the collector and the deliberator. Both frequentist and probabilistic reasoning seem to intersect when prior knowledge is all but unknown (the uninformative or diffuse prior) or might as well assign equally plausible weights (probabilities) to any hypothesis the analyst might propose about the value of a moment of a distribution. They all but diverge when uneven, lumpy, step function priors on the values of the supposed estimators as hypotheses collide with the likelihood that the data is compatible with any of these hypotheses. Such divergence is not the destruction of objectivity, rather the transparent inclusion into a complete objective description and explanation of a tradition, a set of assumptions, a font of knowledge to date. \footnote{Charles Sanders Pierce ( \citet{Peirce1884} ) is perhaps the seminal study that incorporates prior knowledge into what would appear to be a frequentist null hypothesis approach, also known as \emph{modus tollens}.}

\hypertarget{a-work-in-progress}{%
\section*{A work in progress}\label{a-work-in-progress}}
\addcontentsline{toc}{section}{A work in progress}

This book will expand and contract over the days ahead. Email me with comments, errors, omissions, etc.

Thanks!

\begin{flushright}
Bill Foote,
Fordham University,
Bronx, NY
\end{flushright}

\hypertarget{topic-1-the-basics}{%
\chapter*{Topic 1 -- The Basics}\label{topic-1-the-basics}}
\addcontentsline{toc}{chapter}{Topic 1 -- The Basics}

\citet{borgesjlJardinSenderosQue1941} sends a man through a labyrinth of a garden of ``forked paths.'' By analogy Bayesian inference is really just counting and comparing of possibilities, the \emph{forked paths}. Like Borges' character, we encounter numerous logical, causal, practical paradoxes and contraditions, often galore. FOr each possibility we decide to go left, go right, ah, we choose left. We reach another contradition, go left again, right or stand still? As we move outwards from any position, any possibility we continue to branch into an ever expanding, emergent, garden of forked paths.

Well, we are here to continue to learn to make reasonable inferences about what might happen given what has already been revealed as happening. In this way Bayesian analysis helps us structure our understanding about what could have happened, and might happen again. Borges' forks are alternative sequences of events in schemes of recurrence. It is interesting that as we proceed through the garden we discard paths along the way. We go left, forever never taking that particular right path. We keep going until we reach a condition which Bernard \citet{lonergan_insight} calls the virtually unconditioned judgment of what is logically at the end of a seemingly endless sequence of forks. We arrive at the end of the current journey where what might be is what is logically consistent with what we observed. The result is virtual so there is no guarantee we have an absolutely correct, right answer. We have the best answer reasonably available in the here and now which can be derived from the data and the conjectures fed into our understanding.

We explore these areas from our past and extend them into our current work.

\begin{itemize}
\item
  Basic logic to help us form consistent arguments, conjectures, hypotheses, and interpretations of analytical results.
\item
  The BayesBox, the workhorse of our statistical inference engine, which mashes together data with hypotheses. This device, a true robot, emerges from our work with contingency tables.
\item
  Measures of probabilistic expectations. This handle some our summaries of risk. But we will need to wait a bit to handle our view of uncertainty, the known unknown.
\end{itemize}

\hypertarget{counting-the-ways}{%
\chapter{Counting the Ways}\label{counting-the-ways}}

\hypertarget{plausibility-probability-and-information}{%
\section{Plausibility, probability and information}\label{plausibility-probability-and-information}}

According to Aristotle, if two claims are well-founded, their truth values can be ascertained. A plate is either green, or it is not. When thrown, the plate will land without a chip or it will break. If I claim that when I throw the plate, it will land with out a chip and you disagree, I can simply throw the plate to find out who was correct. One part of this is a mind game, a thought experiment with a potential outcome. The other is the reality of actually throwing the plate and observing its status on landing. We will eventually call the mind-game a hypothesis and the reality a datum. Actually both are data: the hypothesis is unobserved data, and the so-called observations are observed data.

It is in disagreement that logical deduction might (plausibly) break down. There is no guarantee that the plate will break, or, for that matter, that it will chip. We must simply experiment with plate(s), green, blue or otherwise, to support the claim (or not). These claims arise in everyday life. For example, despite my poor performance in plate throwing in the past, there is no cogent reason to believe that it is absolutely, positively false that the plate I throw would land without a chip. There is a degree of acceptance, of plausibility, in one side of the claim, and on the other as well. Certainly it is not as false as the claim that \(2+2=5\) in base 10 arithmetic, or the patently spurious claim that true is false or my cat is a dog.

Claims about things that are neither definitely true nor definitely false arise in matters both mundane and consequential: producing weather reports, catching the bus, predicting the outcomes of elections, interpreting experimental vaccine results, and betting on sports games, throwing the plate, to name just a few.

So we would benefit from a method of comparing claims in these situations -- which atmospheric model produces better predictions? What is the best source for predicting elections? Should I blow three times on my lucky dice, or is this all just a figment of my denial based imagination?

\hypertarget{some-surprise}{%
\section{Some Surprise}\label{some-surprise}}

The goal, in all the cases above, is to guess about something that we don't or can't know directly, like the future, or the fundamental structure of the economy, or reasons why customer preferences change, on the basis of things we do know, like the present and the past, or the results of an existing or past experiment.

Mostly we guess. Some us try to systematically consider and attempt to support with evidence the guess. Lacking precision, and sometimes even accuracy, we try to avoid bad surprises. Goods ones are often welcome. If I use a government weather application on my smart phone it might not surprise me to see rain pelting down at 1630 (this afternoon). After all the app indicated as much. In advance of the rain I brought in chair pads and anything else that might get ruined with rain. An airline pilot knows all the defects of her aircraft. That knowledge saves lives.

Our mortal inferences, clever or dumb as they are, must have a surprise somewhere between \emph{totally expected}, or zero surprises and thus certain 100\% of the ways to make the statement, and \emph{totally surprising} and 0\% chance of anticipation. We will generally be making statements like: \emph{it will probably rain tomorrow}, or \emph{nine times out of ten, the team with a better defense wins}. This motivates us to express our surprise in terms of plausibility and we hanker for more precision with probability.

\hypertarget{how-many-ways}{%
\section{How many ways?}\label{how-many-ways}}

Let's use a simple example. We have four voters in an upcoming election. They may be red or blue voters. Three of us go out and talk to three voters at random, that is, indiscriminately. One of us happens to come upon a blue voter, another of us, independently, happens to find a red voter, and the other separately finds a blue voter. This is the very definition of a random sample. Each of the finders does not know what the other is doing, all three do know that there are four voters out there and they happened to have independently talked to two blue and one red voter. How many red voters and how many blue voters are there?

Here are all of the possible conjectures we can make for \(blue = {\color{blue}\Box}\) and \(red = {\color{red}\Box}\) voters.

\begin{table}
\centering
\caption{\label{tab:unnamed-chunk-3}voter conjectures}
\centering
\begin{tabular}[t]{c|c|c|c}
\hline
1 & 2 & 3 & 4\\
\hline
${\color{red}\Box}$ & ${\color{red}\Box}$ & ${\color{red}\Box}$ & ${\color{red}\Box}$\\
\hline
${\color{blue}\Box}$ & ${\color{red}\Box}$ & ${\color{red}\Box}$ & ${\color{red}\Box}$\\
\hline
${\color{blue}\Box}$ & ${\color{blue}\Box}$ & ${\color{red}\Box}$ & ${\color{red}\Box}$\\
\hline
${\color{blue}\Box}$ & ${\color{blue}\Box}$ & ${\color{blue}\Box}$ & ${\color{red}\Box}$\\
\hline
${\color{blue}\Box}$ & ${\color{blue}\Box}$ & ${\color{blue}\Box}$ & ${\color{blue}\Box}$\\
\hline
\end{tabular}
\end{table}

Reading this we see that there are 4 voters and 5 different voter compositions ranging from all red to all blue. Our sample is 2 blue and 1 red voter, so we can very safely eliminate the first and fifth conjectures from our analysis, but for the moment just keep them for completeness sake.

For each of the three remaining conjectures we may ask how many ways is the conjecture \emph{consistent} with the collected data. For this task a tree is very helpful. Let's take the first realistic conjecture the \({\color{blue}\Box}\), \({\color{red}\Box}\), \({\color{red}\Box}\), \({\color{red}\Box}\) hypothesis and check if, when we sample all of the four voters, what are all of the ways this conjecture fans out. So here we go.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We sampled a \({\color{blue}\Box}\) first. How many \({\color{blue}\Box}\)'s are in this version of the composition of voters? only 1.
\item
  We then sampled independently a \({\color{red}\Box}\). How many \({\color{red}\Box}\)'s are in this conjecture? Quite a few, 3.
\item
  Finally we sampled a \({\color{blue}\Box}\) at random. We know there is only one \({\color{blue}\Box}\) in this version of the truth.
\end{enumerate}

So, it is just counting the ways: 1 \({\color{blue}\Box}\) way \(\times\) 3 \({\color{red}\Box}\) ways \(\times\) 1 \({\color{blue}\Box}\) way = \(1 \times 3 \times 1 = 3\) ways altogether.

When asked, many surmise that the 2 blue and 3 red conjecture is the right one. Are they right? Here is a table of the ways each conjecture pans out. We then in a separate column compute the contribution of each conjecture to the total number of ways across the conjectures, which is 3 + 8 + 9 = 20 ways. Also each of the conjecture propose a proportion \(p\) of the successes, that is, the blue voters in this context.

\begin{table}
\centering
\caption{\label{tab:unnamed-chunk-4}ways voter conjectures turn out}
\centering
\begin{tabular}[t]{c|c|c|c|c|c|c}
\hline
1 & 2 & 3 & 4 & proportion & ways & plausibility\\
\hline
${\color{red}\Box}$ & ${\color{red}\Box}$ & ${\color{red}\Box}$ & ${\color{red}\Box}$ & 0.00 & 0 x 4 x 0 = 0 & 0.00\\
\hline
${\color{blue}\Box}$ & ${\color{red}\Box}$ & ${\color{red}\Box}$ & ${\color{red}\Box}$ & 0.25 & 1 x 3 x 1 = 3 & 0.15\\
\hline
${\color{blue}\Box}$ & ${\color{blue}\Box}$ & ${\color{red}\Box}$ & ${\color{red}\Box}$ & 0.50 & 2 x 2 x 2 = 8 & 0.40\\
\hline
${\color{blue}\Box}$ & ${\color{blue}\Box}$ & ${\color{blue}\Box}$ & ${\color{red}\Box}$ & 0.75 & 3 x 1 x 3 = 9 & 0.45\\
\hline
${\color{blue}\Box}$ & ${\color{blue}\Box}$ & ${\color{blue}\Box}$ & ${\color{blue}\Box}$ & 1.00 & 4 x 0 x 4 = 0 & 0.00\\
\hline
\end{tabular}
\end{table}

We cannot help but note that the proportion of ways for each conjecture can range from 0, perhaps to 1, since the proportions add up to 1. The number of ways also expresses the number of true consistencies of the data with the conjecture, an enumeration of the quality of the logical compatibility of conjectures with what we observe.

We might now revise our commonsense surmise that 2 blue and 2 red is the better conjecture. However, if we use the criterion that the conjecture with the most ways consistent with the data is the best choice for a conjecture, then clearly here we would say that there are 3 blues and 1 red. Perhaps we have a better criterion that would choose our equinanimous choice of 2 blues and 2 reds? It does not appear to be so.

Ways are the count, the frequencies of logical occurrence of a hypothesis \emph{given} the data. The data includes the knowledge that there are possibly blues and reds, that there are 4 voters, and that we sampled 2 blues and 1 red. The relative frequency of the ways in which our conjectures are consistent with the data is what we will finally call \emph{probability}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The plausibility is a measure between and including 0 and 1.
\item
  The sum of all plausibilities is 1.
\item
  The plausibility must be compatible with common sense.
\end{enumerate}

That last requirement, in turn, requires a sketch of why it is important.

\begin{itemize}
\item
  Let's suppose the plausibility of a proposition A (Jaynie is a runner, for example) is changed by another proposition B (Jaynie participated in a marathon).
\item
  Let's mix this up with the plausibility that proposition C (Jaynie is a business analyst, for example) has nothing to do with proposition B.
\item
  Then common sense (as opposed to common non-sense) dictates that the plausibility of A and C together (Jaynie is a business analyst runner) is also increased by proposition B.
\end{itemize}

We have just quantified the \emph{plausibility} of logical truth values. We have also found a very compelling criterion for the choice of a conjecture given the data and circumstances surrounding our inquiry.

\hypertarget{back-to-data}{%
\section{Back to data}\label{back-to-data}}

The location and scale parameters \(\mu\) and \(\sigma\) (e.g., mean and standard deviation) we often calculate, and have so since middle school, are not the mean and standard deviation of data \(x\). They are not properties of the physical representation of events called \textbf{observed data}. These measures use counts of events, measures of, perhaps physical properties like heat, size, time, but in the end they are abstractions and summarizations about the events.

These parameters do carry information about the probability distribution of the representation of physical reality in data. To say \(\mu\) is the mean of the data is to ascribe a mind's eye idea to the physical reality, to invest in physical, objective reality, a property that only exists in the mind, but is in fact \textbf{unobserved data}.

This is an example of the \emph{mind projection} or \emph{reification} fallacy much in vogue in the circles of \emph{fake}, or better yet, \emph{chop logic}. In the same way, probabilities only exist in our minds: there is no physical reality that is a probability, just a mental construct that helps us think through potential outcomes. Is information just such a fiction?

\hypertarget{exercising-our-grip-on-reality}{%
\section{Exercising our grip on reality}\label{exercising-our-grip-on-reality}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Suppose there are 5 voters of unknown affiliation. What is the probability that 60\% are red affiliates?
\item
  So one of 5 moved out and we are back to 4 voters. What is the most plausible proportion of blue voters if we sample the district and find 2 blue and 3 red voters?
\end{enumerate}

\hypertarget{probability-for-real-people}{%
\chapter{Probability for Real People}\label{probability-for-real-people}}

\hypertarget{can-we-rationally-reason}{%
\section{Can we rationally reason?}\label{can-we-rationally-reason}}

Some might wonder if this section header even makes sense! A decision maker, such as as CFO, looks at it and knows everyone has a reason, some good and some not so good. But sometimes our reasons are not founded in any data that can be observed either by ourselves or by others. Our reasoning can be founded on falsified data, delusions, unfounded opinions, beliefs with no authoritative grounds. Her questions move from the various relationships in production planning in the previous part of this project to the forecasting of what might happen at all. What would be the rationale for a forecast? So what does our CFO decision maker do? She begins her thought experiment with the weather in her hometown of Albany, New York.

Rationality in the context we have been discussing at least means that we, as decision makers, would tend to act based on the consistency of observed reality with imagined models of the real world and through ideas about the world in which data are collected. We attempt to infer claims about the world based on our beliefs about the world. When confronting ideas, imbued with beliefs, with observed reality we might find ourselves in the position to \emph{update our beliefs}, even those, and sometimes especially those, we so dearly hold.

In our thinking about anything we would venture candidate hypotheses \(h\) about the world, say the world of meeting demand, providing services, marshalling resources in specific markets. Of course the whole point is that we do not know which hypothesis \(h\) is more plausible, or not. We then collect some data \(d\). When we perform this task, we move from the mental realm of the possibiity of hypotheses, theories, surmises, and model to the realm of observed reality. We may well have to revise our original beliefs about the data.

To implement our maintained hypothesis of rationality, we begin our search for \textbf{potential consistencies of the collected data with our hypotheses} that are fed by the data. In our quest we might find that some one of the hypotheses has more ways of being consistent with the data than others. When the data is consistent with a hypothesis, that is, when the hypothesis is reasonable logically, then our belief in that hypothesis strengthens,\footnote{The core idea of \emph{strengthen} is to take us from a more vulnerable to a less vulnerable place or state. Synonyms for strength include \emph{confirm} and \emph{validate}.} and becomes more plausible. If the data is less consistent with the hypothesis, our belief in that hypothesis weakens. So far we have performed this set of tasks with conjectures about virus testing and voter alliance in zip codes. Let's switch up our program and consider the following very simplified question about the weather.

\hypertarget{before-we-get-to-the-shovels}{%
\section{Before we get to the shovels!}\label{before-we-get-to-the-shovels}}

Let's apply a probability analysis to understanding the plausibility of the demand for pies at one of the smaller restaurants the vegan pie maker Make-A-Pie LLC vends to. Here we query a day by day count of the number of pies sold (the \(pies\) variable) and the weather (clear coded as \(C\) or rainy coded as \(R\)). The original data only has the date, count, and the weather. The rest is analysis based on this data.

For our purposes we would like to know the conditional probability of selling more than 10 pies when the weather is clear. We can answer this query by making a 2x2 contingency table of the conditional data counts of pie demand. This panel displays the counts of pies sold.

\begin{longtable}[]{@{}cccc@{}}
\toprule
pies ~weather & rain & clear & total\tabularnewline
\midrule
\endhead
\(\geq 10\) & 2 & 8 & 10\tabularnewline
\(\leq 10\) & 3 & 6 & 9\tabularnewline
total & 5 & 14 & 19\tabularnewline
\bottomrule
\end{longtable}

How likely is it to sell more than 10 pies when the weather is clear? When the weather is clear either we sell more than 10 pies or we don't. So we look at the clear weather column of counts of pies sold for our answer: \(Pr( >10 \mid clear ) = 8/14\). We can also see in this column that there are 14 pies sold. So we can also find our that the likelihood of clear weather is \(Pr( clear )=14/19\). Also the \textgreater10 row gives us the likelihood of selling more than 10 pies, rain or shine, that is \(Pr( >10 )=10/19\). But it does not make much sense to ask if we sell more than 10 pies (or not for that matter) that it will rain. Unless of course the weather somehow depends on how many pies we sell. Rule 3: use common sense!.

The probability of selling more than 10 pies when the weather is clear is about \(8/14 \times 100 = 57\)\%. We sold 8 pies greater than the threshold and 6 pies less than the threshold when the weather was clear. The ratio of \(8:6 = 1.33\) reports the odds of selling greater than 10 pies. We interpret this number with the phrase: it is 1.33x more likely to sell greater than 10 pies, than not, when the weather is clear.\footnote{Some folks might want to aggrandize this statement by saying that is is 33\% more likely to sell more than 10 pies when clear. We might caution ourselves as we might be falling into a distortion by magnifying an apparent difference. The probability of selling more than 10 pies when clear is \(8/14 = 4/7\). on the other hand the complementary probability of selling up to 10 pies when clear is \(6/14 = 3/7\). We get to sell only 1 pie in crossing the 10 pie threshold given this data set. An inflated way of expressing the results of this analysis? Isocrates would agree.}

\hypertarget{shovels-in-albany-ny}{%
\section{Shovels in Albany NY}\label{shovels-in-albany-ny}}

The CFO's ask of the analysts is this question about winter consumer behavior in the Albany-area market:

\begin{quote}
\emph{We see people carrying snow shovels. Will it snow?}
\end{quote}

What is the data \(d\)? We have recorded a simple observation about the state of the weather so that single piece of data (\(d = \text{We see people carrying snow shovels}\)). Here is where our beliefs enter. We have two \textbf{\emph{hypotheses}}, \(h\): either it snows today or it does not.

Let's figure out how to solve this problem? We have three \emph{desiderata}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We should include our experiences with snow in our analysis.
\item
  We should collect data about carrying snow shovels in January as well.
\item
  We prefer more consistency of data with hypotheses to less consistency.
\end{enumerate}

Here we go, let's strap ourselves in.

\hypertarget{priors-what-we-think-might-happen}{%
\subsection{Priors: what we think might happen}\label{priors-what-we-think-might-happen}}

Our observation is about the weather: clouds, wind, cold. But we want to know about the snow! That is our objective and we have definite ideas about whether (don't pardon the pun!) it will snow or not. We will identify our beliefs, ever before we make our observations, about snow. The analytical profession and custom is to label these beliefs as \emph{a priori},\footnote{The \emph{a priori} elements of any argument include just about everything you and I know, including the kitchen sink! We can't help but to have these antecedent thoughts, experiences, shared and not-so-shared histories. They tend to persist in most humans, including us. At least that is what we will maintain. Thus it is a \emph{necessity} to include these beliefs in our discussion. Without their consideration we most plausibly will introduce unsaid and denied bias, let blindspots have the same focus as clearly understood experiences, and produce time and resource consuming blind alleys. But we should hang on here: even blind alleys and blind spots are extremely important bits of knowledge that help us understand what does not work, an \emph{inverse insight} as exposed by Bernard \citet{Lonergan_1970}.} and thus the ellipsis \emph{prior}, contentions we hold when we walk into the data story we create with the question of \emph{will it snow?}

Our prior contentions are just the plausibilities of each hypothesis whatever data we eventually collect. After all we have to admit to everyone what we believe to be true as the antecedent to the consequent of observations and the plausibility of snow. This move allows us to learn, to revise, to update our dearly held beliefs. We thus can grow and develop. This is in a phrase a \emph{sine qua non}, a \emph{categorial imperative}, a \emph{virtually unconditioned} requirement for change.

What might we believe about whether it will snow (today)? If you come from Malone, New York, north of the Adirondack mountains, you will have a different belief than if you come from Daytona, Florida, on the matter of how many ways snow might happen in a given month. So let's take as our benchmark Albany, the capital of the state of New York.

We will refer to some data to form hypotheses and their plausibility.using this \href{https://www.currentresults.com/Weather/New-York/snowfall-january.php}{weather statistics site}. The site reports the average number of days of snowfall in January, when there is at least a 0.25 cm accumulation in a day. It is 10.3 days. These are the number of ways (days) in January, in Albany, NY, that it is true, on average and thus some notion of expected, or believed to be, that it snows. The total number of ways snow could possibly fall in any January (defined by calendar standards) is 31. While the formation of the hypotheses \emph{snowy} and \emph{nice} days is informed by data, we are asking a question about snow because we have yet to observe if will snow. We cannot observe something that has not yet happened. We can thus characterize hypotheses and conjectures as \textbf{unobserved data}.

Thus we might conclude that we believe that is it plausible (probable) that snow \emph{can} fall \(10.3 / 31 = 30\%\) of the different ways snow can fall. Note very well we will talk about \emph{priors} as \emph{potentials} and \emph{conjectures} and \emph{hypotheticals}, and thus used the modal verbs \emph{can} or \emph{might}. Thus we believe it might not snow, because it is possible, with plausibility \(1-0.30 = 0.70\), or, multiplying by 100, 70\%, according to the law of total probability of all supposed (hypothesized) events. We only have two such events: \emph{snow} and \emph{not snow}. Probabilities must, by definition, add up to 1 and must, again by definition be a number between 0 and 1.

\begin{table}
\centering
\caption{\label{tab:prior}Priors by hypotheses}
\centering
\begin{tabular}[t]{c|c}
\hline
hypotheses & priors\\
\hline
snowy day & 0.3\\
\hline
nice day & 0.7\\
\hline
\end{tabular}
\end{table}

Nice ideas, nice beliefs, are our as yet to be observed, but projected notions of a snowy day. But how real, how plausible, how rational, that is, how consistent are they with any \emph{observed data}? Is there any \emph{observed data} we can use to help us project which of our unobserved data, our hypotheses, is more or less reasonable?

\hypertarget{likelihoods-thinking-about-the-data}{%
\subsection{Likelihoods: thinking about the data}\label{likelihoods-thinking-about-the-data}}

Life in the Northeast United States in January much revolves around the number of snow days, also known as days off from school. A prediction of snow meets with overtime for snow plow drivers, school shut downs, kids at home when they normally are in school. On some snowy days we see people carrying snow shovels, on others we don't. On some nice days we see people with snow shovels, on others we don't. Confusing? Confounding? A bit.

Now we link our observations of shovels with our unobserved, but through about and hypothesized, prediction of snow. We then suppose we observe that people carry snow shovels about 7 of the 10 snowy days in January or about 70\%. On nice days we observe that people carry shovels at most 2 days in the 21 nice days or about 10\%.

This table records our thinking using data we observe in Januaries about weather conditions.

\begin{table}
\centering
\caption{\label{tab:unnamed-chunk-6}data meets hypotheses}
\centering
\begin{tabular}[t]{c|c|c}
\hline
hypotheses & shovels & hands\\
\hline
snow day & 0.7 & 0.3\\
\hline
nice day & 0.1 & 0.9\\
\hline
\end{tabular}
\end{table}

First of all these probabilities register yet another set of beliefs, this time about whether we see shovels or not, \emph{given}, \emph{conditioned by}, the truth of each hypothesis \(h\). We write the \emph{\emph{conditional probability}} \(\operatorname{Pr}(d \mid h)\), which you can read as ``the probability of \(d\) given \(h\)''. Also here we will follow the convention that this set of results of our assessment of the relationship of shovels to snowy days as a \emph{\emph{likelihood}} .\footnote{For Pierre Simon \citet{Laplace_1902}, likelihood also has the idea of \(\operatorname{Pr}(h \mid d)\). Let's stick to our knitting, and tolerance for ambiguity, with using the rows of this table as our entries for likelihood.}

\hypertarget{altogether-now}{%
\subsection{Altogether now}\label{altogether-now}}

Do we have everything to fulfill our \emph{desiderata}? Let's check where we are now.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We should include our experiences with snow in our analysis.
\end{enumerate}

\begin{quote}
Yes! We put our best beliefs forward. We even (sometimes this is a courageous analytical step) quantified teh ways in which snow and not snow would occur, we believe, in Albany NY in an average January.\footnote{we really need to think further about our notions of an average or centrally located anything. This means more consideration later, including deviations from these locations measured by scale.}
\end{quote}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  We should collect data about carrying snow shovels in January as well.
\end{enumerate}

\begin{quote}
Yes we did! Again we elicited yet another opinion, belief, whatever we want to colloguially call it. That belief if what we register and document based on observation of shovels and just hands in the presence of snowy and nice days in a January.
\end{quote}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  We prefer more consistency of data with hypotheses to less consistency.
\end{enumerate}

\begin{quote}
Not yet! We will impose our definition of rationality here.
\end{quote}

Let's start out with one of the rules of probability theory. The rule in question is the one that talks about the probability that \emph{two} things are true. In our example, we will calculate the probability that today is snowy (i.e., hypothesis \(h\) is true) \emph{and} people carry shovels (i.e., data \(d\) is observed). The \textbf{\emph{joint probability}} of the hypothesis and the data is written \(\operatorname{Pr}(d,h)-\operatorname{Pr}(d \wedge h\), and you can calculate it by multiplying the prior \(\operatorname{Pr}(h)\) by the likelihood \(\operatorname{Pr}(d \mid h)\). The conjunction is a \emph{both-and} statement. We express conjunctions using the wedge \(\wedge\) symbol. Logically, when the statement that both \(d\) \emph{\emph{and}} \(h\) is true, then the plausibility, now grown into probability is:

\[
\operatorname{Pr}(d \wedge h) = \operatorname{Pr}(d \mid h) \operatorname{Pr}(h)
\]

When we divide both sides by \(\operatorname{Pr}(h)\) we get the definition, some say derivation, of \emph{\emph{condition}} probability. If we count \(\#()\) the ways \(d \wedge h\) are true and the ways that \(h\) are true then

\[
\#(d \mid h) = \frac{\#(d \wedge h)}{\#(h)}
\]

Then the number of ways the data \(d\) are true, \emph{\emph{given}} \(h\) is true, equals the total number of ways that \(d\) \emph{\emph{and}} \(h\) per each way that \(h\) is true. We have thus normed our approach to understanding a conditional statement like \emph{\emph{if \(h\), then \(d\)}}. Even more so, when we combine the law of conditional probability with the law of total probability we get \emph{\emph{Bayes Theorem}}. This allows us to recognize the dialectical principle that, yes, we recognize \(h = snowy\), but we also know that every cloud has its silver lining and that there is a non-snowy day and thus a

\[
not\,\,h = \lnot h = nice
\]

lurking in our analysis.

Here it in in all its glory.

\begin{align}
\operatorname{Pr}(h\mid d) &= \frac{\operatorname{Pr}(d\mid h)\,\operatorname{Pr}(h)}{\operatorname{Pr}(d\mid h)\,\operatorname{Pr}(h)+\operatorname{Pr}(d\mid \lnot h)\,\operatorname{Pr}(\lnot h)} \\
&= = \frac{\operatorname{Pr}(d \wedge h)}{\operatorname{Pr}(d\mid h)\,\operatorname{Pr}(h)+\operatorname{Pr}(d\mid \lnot h)\,\operatorname{Pr}(\lnot h)}
\end{align}

The numerator is the same as the conjunction \emph{\emph{both \(d\) and \$h}}. The denominator is the probability that either \emph{\emph{both \(d\) and \(h\)}} or \emph{\emph{both \(d\) and \(h\)}} are true. While the build up to this point is both instructive, and thus may at first be \emph{confusing}, it is useful as it will highlight the roles these probabilities perform in the drama that is our analysis.

We had better get back to the data or get lost in the weeds of the maths. So, what is the probability it is true that today is a snowy day \emph{and} we observed people to bring a shovel?

Let's see what we already have. Our prior tells us that the probability of a snowy day in any January is about 30\%. Thus \(\operatorname{Pr}(h) = 0.30\). The probability that we observe people carrying shovels is true given it is a snowy day is 70\%. So the probability that both of these things are true is calculated by multiplying the two to get 0.21. We can make this

\begin{align}{l}
\operatorname{Pr}(snowy,\,shovels) & = & \operatorname{Pr}(shovels \, | \, snowy) \times \operatorname{Pr}( snowy ) \\
& = & 0.70 \times 0.30 \\
& = & 0.21
\end{align}

This is an interesting result, something odds makers intuitively know when punters put skin in the game. There will be a 21\% chance of a snowy day when we see shovels in people's hands. However, there are of course \emph{four} possible pairings of hypotheses and data that could happen. We then repeatthis calculation for all four possibilities. We then have the following table.

\begin{table}
\centering
\caption{\label{tab:unnamed-chunk-7}Both data and hypotheses}
\centering
\begin{tabular}[t]{c|c|c|c}
\hline
hypotheses & shovels & hands & sum\\
\hline
snow day & 0.21 & 0.09 & 0.3\\
\hline
nice day & 0.07 & 0.63 & 0.7\\
\hline
sum & 0.28 & 0.72 & 1.0\\
\hline
\end{tabular}
\end{table}

Just to put this into perspective, we have for the 31 days in a January this table.

\begin{table}
\centering
\caption{\label{tab:unnamed-chunk-8}both data and hypotheses in days in January}
\centering
\begin{tabular}[t]{c|c|c|c}
\hline
hypotheses & shovels & hands & sum\\
\hline
snowy day & 6.5 & 2.8 & 9.3\\
\hline
nice day & 2.2 & 19.5 & 21.7\\
\hline
sum & 8.7 & 22.3 & 31.0\\
\hline
\end{tabular}
\end{table}

We have four \emph{\emph{logical}} possibilities for the interaction of observed data and unobserved hypotheses. We arrange these possibilities in two stacked rows. We recall that visualizatiton is everything, even in tables! Here is the first row.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Snowy and shovels
\end{enumerate}

\begin{align}{l}
\operatorname{Pr}(snowy,\,shovels) & = & \operatorname{Pr}(shovels \, | \, snowy) \times \operatorname{Pr}( snowy ) \\
& = & 0.70 \times 0.30 \\
& = & 0.21
\end{align}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Snowy and just hands
\end{enumerate}

\begin{align}{l}
\operatorname{Pr}(snowy,\,hands) & = & \operatorname{Pr}(hands \, | \, snowy) \times \operatorname{Pr}( snowy ) \\
& = & 0.30 \times 0.30 \\
& = & 0.09
\end{align}

In this row the prior probability about snow is 0.30.

Here is the second row with its separate calculations.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Nice and shovels
\end{enumerate}

\begin{align}{l}
\operatorname{Pr}(nice,\,shovels) & = & \operatorname{Pr}(shovels \, | \, nice) \times \operatorname{Pr}( nice ) \\
& = & 0.10 \times 0.70 \\
& = & 0.07
\end{align}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Nice and just hands
\end{enumerate}

\begin{align}{l}
\operatorname{Pr}(nice,\,hands) & = & \operatorname{Pr}(hands \mid nice) \times \operatorname{Pr}( nice ) \\
& = & 0.90 \times 0.70 \\
& = & 0.63
\end{align}

In this row the prior probability about nice days is 0.70.

An insightful exercise is to carry these calculations from the number of ways snow with and without shovels occurs given we think we know something about snow. The same with the number of ways a nice day might occur with and without shovels, given what we think about nice days.

Let's put one calculation together with a not so surprising requirement. When we conjoin snow with shovels, how many possible ways can these logical statements occur? It is just the 31 days.

We now have all of the derived information to carry our investigation further. We also total the rows and, of course, the columns. We will see why very soon.

The row sums just tell us as a check that we got all of the ways in which snow occurs in 31 days. What is brand new are the column sums. They add up the ways that data occurs across the two ways we hypothesize that data can occur: snow, no snow (nice day). They tell us the probability of carrying a shovel or not, across the two hypotheses. Another way of thinking about the \(p(d)\) column sums is that they are the expectation of finding snow or hands in the data. The consistency of all of these calculations is that column sums equal row sums, 100\%. All regular, all present and correct, probability-wise.

\hypertarget{updating-beliefs}{%
\subsection{Updating beliefs}\label{updating-beliefs}}

The table lays out each of the four logically possible combinations of data and hypotheses. So what happens to our beliefs when they confront data? In the problem, we are told that we really see shovels, just like the picture from Albany, NY at the turn of the 20th century. Is surprising? Not necessarily in Albany and in January, so you might expect this behavior out of habit during a rough Winter. The point is that whatever our beliefs have been about shovel behavior, we should still subject them to the possibility of accomodating the fact of seeing shovels in hands in Albany in January, a winter month in the Northern Hemisphere.

We should recall this formula about the probability of seeing both an hypothesis and data:

\[
\operatorname{Pr}(h \mid d) = \frac{\operatorname{Pr}(d \wedge h)}{\operatorname{Pr}(d)}=\frac{\operatorname{Pr}(d \mid h) \operatorname{Pr}(h)}{\operatorname{Pr}(d)}
\]

Now we can trawl through about our intuitions and some arithmetic. We worked out that the joint probability of \emph{both \textbf{snowy day} and \textbf{shovel}} is 21\%, a rate reasonable given the circumstances. In our formula, this is the product of the likelihood \(\operatorname{Pr}(d=shovels \mid h=snow)=0.70\) and the prior probability we registered that snow might occur \(\operatorname{Pr}(h=snow)=0.30\).

Relative to the product of the likelihood of shovels given a nice day and the chance that snow might occur is the the joint probability of \emph{both \textbf{nice day} and \textbf{shovel}} at 10\%, or \(\operatorname{Pr}(d=shovels \mid h = nice)\operatorname{Pr}(h=nice)=0.10\times 0.70=0.07\), again a reasonable idea, since we plausibly wouldn't see much shovel handling on that nice day in January..

Both of these estimates are consistent with actually seeing shovels in people's hands. But what are the chances of just seeing shovels at all? This is an \emph{\textbf{either or}} question. We see shovels 21\% of the time on snowy days or we see shovels 7\% of the total days in January on nice days. We then add them up to get 28\% of the time we see shovels in all of January, whether it snows or not.

So back to the question: if we do see shovels in the hands of those folk, will it snow? The hypothesis is \(h=snow\) and the data is \(d=shovels\). The joint probability of both snow and shovels is \(\operatorname{Pr}(d, h)=0.21\). But just focusing on the data we just observed, namely that we see shovels, we now know that the chances of seeing shovels on any day in January in Albany, NY is \(\operatorname{Pr}(d)=0.27\). Out of all of the ways that shovels can be seen in January then we would anticipate that the probability of snow, upon seeing shovels, must be \(\operatorname{Pr}(h \mid d)=\operatorname{Pr}(d,h)/\operatorname{Pr}(d)=0.21/0.28=0.75\).

What is the chance of a nice day given we see shovels? It would be again likelihood times prior or \(0.10\times0.7=0.07\) divided by the probability of seeing shovels any day in January 28\%. We then calculate \(0.07/0.28=0.25\). We now have the posterior distribution of the two hypotheses, snow or nice, in the face of data, shovels. So what are the odds in favor of snow when we see shovels?

\begin{align}
OR(h \mid d) &=\frac{\operatorname{Pr}(h=snow \mid d=shovels)}{\operatorname{Pr}(h=nice \mid d=shovels)} \\
&=\frac{0.75}{0.25} \\
&=3
\end{align}

We can read this as: when we see people with shovels in January in Albany, NY, then it is 3 times more plausible to have a snowy day than a nice day. The ratio of two posteriors gives us some notion of the plausible divergence in likely outcomes of snowy versus nice days. Again we must append the circumstances of time and place: in a January and in Albany, NY.

Here is table that summarizes all of our work to date.

\begin{table}
\centering
\caption{\label{tab:unnamed-chunk-9}Unobserved belief tempered by observed data = posteriors.}
\centering
\begin{tabular}[t]{c|c|c|c|c|c}
\hline
hypotheses & shovels & hands & priors & posterior shovels & posterior hands\\
\hline
snow day & 0.7 & 0.3 & 0.3 & 0.75 & 0.13\\
\hline
nice day & 0.1 & 0.9 & 0.7 & 0.25 & 0.88\\
\hline
sum & 0.8 & 0.2 & 1.0 & 1.00 & 1.00\\
\hline
\end{tabular}
\end{table}

\hypertarget{what-did-we-accomplish}{%
\section{What did we accomplish?}\label{what-did-we-accomplish}}

We have travelled through the complete model of probabilistic reasoning.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We started with a question. The question at least bifurcates into the dialectical \emph{\textbf{is it?}} or \emph{\textbf{is it not?}}.
\item
  We then began to think about beliefs inherent in the question for each of the hypotheses buried in the question.
\item
  We then collected data that is relevant to attempting an answer to the question relative to each hypothesis.
\item
  Then we conditioned the data with the hypotheses inside the question. It is always about the question!
\item
  Finally we derived plausible answers to the question.
\item
  We then illustrated the example with data collected, threshold set, table constructed, odds computed.
\end{enumerate}

What is next? We continue to use this recurring scheme of heuristic thinking, sometimes using algorithms to count more efficiently, applied to questions of ever greater complexity. In the end our goal will be to learn, and learning is inference.

\hypertarget{some-exercise-is-in-order}{%
\section{Some exercise is in order}\label{some-exercise-is-in-order}}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-10}{}\label{exr:unnamed-chunk-10}Start with a question for analysis using a indicative-interrogative statement format, for example ``We observe X. Will Y occur?'' Based on this statement identify the unobserved data of the hypothesis and the observed data. Use binary hypotheses and observations.
\end{exercise}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-11}{}\label{exr:unnamed-chunk-11}Rework the Albany NY example using your hometown or city. Develop initial distribution of hypotheses, distributions of data given a hypothesis, joint distributions of hypotheses and data. Find the probability that a particular hypothesis might occur given a specific piece of data.
\end{exercise}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-12}{}\label{exr:unnamed-chunk-12}Rework the example using various thresholds. Use the Data Table sensitivity analysis to calculate results. Plot the Data Table. Lead a discussion with some skeptical people about your findings.
\end{exercise}

\hypertarget{foundations}{%
\chapter{Foundations}\label{foundations}}

Here is an architectonic approach ready to drop straight into any of our analytics design and product.

\begin{itemize}
\tightlist
\item
  Objections (with references for further reading)\\
\item
  On the Contrary (with classical anchors)\\
\item
  I Respond That (your affirmative position, fully argued)\\
\item
  Replies to objections\\
\item
  Further reading list and considerations.
\end{itemize}

A \emph{quaestio} is a consideration, in question form, for discussion, debate, formation, development of principles for analytics design.

\hypertarget{quaestio-i.-whether-the-whole-is-greater-than-the-sum-of-the-parts}{%
\section{QUAESTIO I. Whether the Whole Is Greater Than the Sum of the Parts}\label{quaestio-i.-whether-the-whole-is-greater-than-the-sum-of-the-parts}}

\hypertarget{objections}{%
\subsubsection{Objections}\label{objections}}

\hypertarget{objection-1-baconian-empiricism}{%
\paragraph{Objection 1 --- Baconian Empiricism}\label{objection-1-baconian-empiricism}}

One reading of Francis Bacon holds that knowledge arises from the accumulation of discrete observations, and that wholes are nothing but aggregates of parts. Emergent form is dismissed as a projection of the mind.

Reference: \citet{bacon_novum}, \emph{Novum Organum}, Book I.

\hypertarget{objection-2-scientistic-reductionism}{%
\paragraph{Objection 2 --- Scientistic Reductionism}\label{objection-2-scientistic-reductionism}}

Contemporary scientism asserts that all phenomena are reducible to physical components and their interactions. Consciousness, meaning, and value are epiphenomena of chemistry.

Reference: Alex \citet{rosenberg_atheist}, \emph{The Atheist's Guide to Reality}.

\hypertarget{objection-3-neuroscientific-materialism}{%
\paragraph{Objection 3 --- Neuroscientific Materialism}\label{objection-3-neuroscientific-materialism}}

Some neuroscientists claim that thought is nothing but neural firing patterns, and thus any ``whole'' is merely shorthand for chemical processes.

Reference: Patricia \citet{churchland_neurophilosophy}, \emph{Neurophilosophy}.

\hypertarget{objection-4-mechanistic-physics}{%
\paragraph{Objection 4 --- Mechanistic Physics}\label{objection-4-mechanistic-physics}}

If physics reduces systems to fundamental particles and forces, then any ``whole'' is simply the sum of microphysical states.

Reference: Steven \citet{weinberg_final_theory}, \emph{Dreams of a Final Theory}

\hypertarget{on-the-contrary}{%
\subsection{On the Contrary}\label{on-the-contrary}}

Aristotle teaches that a whole possesses a \emph{form} that organizes the parts and gives them their intelligibility.

Reference: \citet{aristotle_metaphysics}, \emph{Metaphysics} VIII.6.

Aquinas affirms that form is ``that by which a thing is what it is,'' and that wholes possess properties irreducible to their components.

Reference: \citet{aquinas_st}, \emph{Summa Theologiae} I, q.76, a.8.

Lonergan argues that intelligibility is grasped in insight, not reducible to data alone.

Reference: Bernard \citet{lonergan_insight}, \emph{Insight}, ch.~1--3.

\hypertarget{i-respond-that}{%
\subsection{I Respond That}\label{i-respond-that}}

I answer that the whole is indeed greater than the sum of the parts, not by addition but by emergent intelligibility. A whole is constituted by:

\begin{itemize}
\tightlist
\item
  Form (Aristotle, Aquinas)\\
\item
  Higher-order relations (Lonergan's ``schemes of recurrence'')\\
\item
  Contextual unity (Polanyi's ``tacit integration'')\\
\item
  Functional organization (modern complexity theory)
\end{itemize}

The parts do not explain the whole; rather, the whole explains the parts.

Reductionism fails because:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It cannot account for emergence (life, consciousness, culture).\\
\item
  It cannot account for meaning (semantics is not chemistry).\\
\item
  It cannot account for value (ethics is not physics).\\
\item
  It cannot account for intelligibility (insight is not reducible to data).
\end{enumerate}

Thus, the whole is ontologically and epistemically prior to the parts. This is the foundation for any humane analytics: data points are not reality; they are fragments awaiting intelligible integration.

\hypertarget{replies-to-objections}{%
\subsection{Replies to Objections}\label{replies-to-objections}}

\hypertarget{reply-to-objection-1-bacon}{%
\subsubsection{Reply to Objection 1 (Bacon)}\label{reply-to-objection-1-bacon}}

Bacon's empiricism mistakes accumulation for understanding. Insight is not a sum of observations but a grasp of form.

\hypertarget{reply-to-objection-2-scientism}{%
\subsubsection{Reply to Objection 2 (Scientism)}\label{reply-to-objection-2-scientism}}

Scientism is self-refuting: the claim ``only physical explanations are valid'' is not itself a physical explanation.

\hypertarget{reply-to-objection-3-neuroscience}{%
\subsubsection{Reply to Objection 3 (Neuroscience)}\label{reply-to-objection-3-neuroscience}}

Neural correlates do not exhaust mental acts. Correlation is not identity. Thought transcends its material substrate.

\hypertarget{reply-to-objection-4-physics}{%
\subsubsection{Reply to Objection 4 (Physics)}\label{reply-to-objection-4-physics}}

Physics describes interactions, not intelligibility. Meaning, purpose, and organization are not derivable from microphysical laws.

\hypertarget{quaestio-ii.-whether-deduction-produces-new-knowledge}{%
\section{QUAESTIO II. Whether Deduction Produces New Knowledge}\label{quaestio-ii.-whether-deduction-produces-new-knowledge}}

\hypertarget{objections-1}{%
\subsubsection{Objections}\label{objections-1}}

\hypertarget{objection-1-humean-skepticism}{%
\paragraph{Objection 1 --- Humean Skepticism}\label{objection-1-humean-skepticism}}

Hume argues that deduction is analytic and tautological, adding nothing to knowledge.

Reference: David \citet{hume_enquiry}, \emph{An Enquiry Concerning Human Understanding}, Section IV.

\hypertarget{objection-2-lockes-passivity-of-mind}{%
\paragraph{Objection 2 --- Locke's Passivity of Mind}\label{objection-2-lockes-passivity-of-mind}}

Locke holds that the mind merely rearranges simple ideas; deduction is recombination, not discovery.

Reference: John \citet{locke_essay}, \emph{Essay Concerning Human Understanding}, Book II.

\hypertarget{objection-3-kantian-formalism}{%
\paragraph{Objection 3 --- Kantian Formalism}\label{objection-3-kantian-formalism}}

Kant claims deduction structures appearances but does not give knowledge of things-in-themselves.

Reference: Immanuel \citet{kant_cpr}, \emph{Critique of Pure Reason}, A6--7.

\hypertarget{objection-4-nihilistic-power-epistemologies}{%
\paragraph{Objection 4 --- Nihilistic Power-Epistemologies}\label{objection-4-nihilistic-power-epistemologies}}

Callicles, an early Nietzsche, and Sartre reduce knowledge to power, rendering deduction a tool of domination rather than discovery.

References: \citet{plato_gorgias}, \emph{Gorgias}; Friedrich \citet{nietzsche_genealogy}, \emph{Genealogy of Morals}; Jean-Paul \citet{sartre_bn}, \emph{Being and Nothingness}.

\hypertarget{on-the-contrary-1}{%
\subsection{On the Contrary}\label{on-the-contrary-1}}

Lonergan teaches that understanding is a dynamic, self-transcending process: insight â†’ formulation â†’ verification. Deduction unfolds the intelligibility grasped in insight.

Reference: Bernard \citet{lonergan_insight}, \emph{Insight}, ch.~2--4.

Aquinas affirms that reasoning extends knowledge by drawing out implications latent in principles.

Reference: \citet{aquinas_st}, \emph{Summa Theologiae} I, q.79, a.8.

\hypertarget{i-respond-that-1}{%
\subsection{I Respond That}\label{i-respond-that-1}}

I answer that deduction does indeed produce new knowledge, because:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Insight grasps a form, but deduction articulates its implications.\\
\item
  Judgment affirms reality, and deduction extends that affirmation.\\
\item
  Understanding is developmental, not static.\\
\item
  Implications are not present to consciousness until reason unfolds them.
\end{enumerate}

Thus, deduction is not mechanical manipulation but the expansion of intelligibility. It reveals what was implicit but not yet known.

This is essential for analytics: models, regressions, and forecasts are not mere computations --- they are the unfolding of intelligibility grasped in data.

\hypertarget{replies-to-objections-1}{%
\subsection{Replies to Objections}\label{replies-to-objections-1}}

\hypertarget{reply-to-objection-1-hume}{%
\subsubsection{Reply to Objection 1 (Hume)}\label{reply-to-objection-1-hume}}

Hume's empiricism cannot account for mathematics, science, or even his own argument. Deduction is not tautology; it is the unfolding of insight.

\hypertarget{reply-to-objection-2-locke}{%
\subsubsection{Reply to Objection 2 (Locke)}\label{reply-to-objection-2-locke}}

The mind is not passive. Insight is an active grasp of form, and deduction extends that grasp.

\hypertarget{reply-to-objection-3-kant}{%
\subsubsection{Reply to Objection 3 (Kant)}\label{reply-to-objection-3-kant}}

Kant's restriction of knowledge to appearances is a metaphysical decision, not a discovery. Deduction reveals real intelligibility.

\hypertarget{reply-to-objection-4-nihilists}{%
\subsubsection{Reply to Objection 4 (Nihilists)}\label{reply-to-objection-4-nihilists}}

If knowledge is only power, then the claim ``knowledge is power'' is itself a power-play and not knowledge. This is self-referentially incoherent.

The third and next \emph{quaestio} is the keystone --- the one that makes the first two \emph{operative} in a Stats/Analytics classroom.

\hypertarget{quaestio-iii.-whether-philosophical-and-theological-considerations-should-guide-an-empirically-oriented-management-analytics-course}{%
\section{QUAESTIO III. Whether Philosophical and Theological Considerations Should Guide an Empirically Oriented Management Analytics Course}\label{quaestio-iii.-whether-philosophical-and-theological-considerations-should-guide-an-empirically-oriented-management-analytics-course}}

\hypertarget{objections-2}{%
\subsection{Objections}\label{objections-2}}

\hypertarget{objection-1-milton-friedmans-value-neutral-economics}{%
\subsubsection{Objection 1 --- Milton Friedman's Value-Neutral Economics}\label{objection-1-milton-friedmans-value-neutral-economics}}

Friedman argues that the sole responsibility of business is to increase profits within the rules of the game. Ethics, theology, and philosophy are ``externalities'' irrelevant to managerial decision-making.

Reference: Friedman, ``The Social Responsibility of Business Is to Increase Its Profits,'' \emph{NYT Magazine} (1970).

\hypertarget{objection-2-neoclassical-economics-and-methodological-individualism}{%
\subsubsection{Objection 2 --- Neoclassical Economics and Methodological Individualism}\label{objection-2-neoclassical-economics-and-methodological-individualism}}

Mainstream economics treats agents as utility-maximizing individuals and assumes that aggregate welfare emerges automatically from self-interest. Normative considerations are dismissed as unscientific.

Reference: Hal \citet{varian_micro}, \emph{Microeconomic Analysis}; \citet{samuelson_nordhaus}, \emph{Economics}.

\hypertarget{objection-3-cartesian-mechanism}{%
\subsubsection{Objection 3 --- Cartesian Mechanism}\label{objection-3-cartesian-mechanism}}

Descartes reduces living beings to automata and treats nature as a machine. By analogy, management becomes the manipulation of mechanical parts (employees, processes, markets) rather than the stewardship of persons.

Reference: Ren'e \citet{descartes_treatise}, \emph{Treatise on Man}.

\hypertarget{objection-4-turings-computationalism}{%
\subsubsection{Objection 4 --- Turing's Computationalism}\label{objection-4-turings-computationalism}}

If thinking is reducible to algorithmic computation, then analytics is simply the execution of procedures. Philosophy and theology add nothing to the correctness of outputs.

Reference: Alan \citet{turing_imitation_game}, ``Computing Machinery and Intelligence.''

\hypertarget{objection-5-positivist-managerialism}{%
\subsubsection{Objection 5 --- Positivist Managerialism}\label{objection-5-positivist-managerialism}}

The positivist tradition holds that only empirical, measurable data count as knowledge. Values, meaning, and purpose are subjective and therefore inadmissible in a scientific curriculum.

Reference: Auguste \citet{comte_positive_philosophy}, \emph{Course of Positive Philosophy}.

\hypertarget{objection-6-pragmatic-technocracy}{%
\subsubsection{Objection 6 --- Pragmatic Technocracy}\label{objection-6-pragmatic-technocracy}}

Some argue that managers need tools, not metaphysics. Philosophy slows decision-making; theology is sectarian; analytics should be efficient, not contemplative.

Reference: Herbert \citet{simon_admin_behavior} , \emph{Administrative Behavior} (instrumental rationality).

\hypertarget{on-the-contrary-2}{%
\subsection{On the Contrary}\label{on-the-contrary-2}}

Aquinas teaches that every human act is ordered toward an end, and that ends are discerned through reason informed by virtue.

Reference: St.~Thomas \citet{aquinas_st}, \emph{Summa Theologiae} I--II, q.1, a.1.

Lonergan argues that authentic inquiry requires attentiveness, intelligence, reasonableness, and responsibility --- norms that are philosophical and theological before they are empirical.

Reference: Bernard \citet{lonergan_insight}, \emph{Insight}, ch.~18--20.

Catholic social teaching affirms that economic and managerial decisions must serve the dignity of the human person and the common good.

Reference: \citet{vat2_gaudium_et_spes}, \emph{Gaudium et Spes}; \citet{francis_laudato}, \emph{Laudato Si'}; \citet{benedict_caritas}, \emph{Caritas in Veritate}.

\hypertarget{i-respond-that-2}{%
\subsection{I Respond That}\label{i-respond-that-2}}

I answer that philosophical and theological considerations must guide an empirically oriented management analytics course, because analytics is never value-neutral. Every model presupposes:

\begin{itemize}
\tightlist
\item
  a view of the human person,\\
\item
  a theory of value,\\
\item
  a conception of the good,\\
\item
  and a telos toward which decisions are directed.
\end{itemize}

Empirical methods alone cannot supply these. They require a prior anthropology, ethics, and metaphysics.

Furthermore:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data are fragments; philosophy provides the intelligible whole.\\
\item
  Models encode assumptions about rationality, agency, and value; theology and ethics test these assumptions.\\
\item
  Analytics affects real people, especially the vulnerable; stewardship requires moral discernment.\\
\item
  Management is a moral practice, not merely a technical one.\\
\item
  Without philosophical grounding, analytics becomes technocratic power, not service.
\end{enumerate}

Thus, philosophy and theology do not constrain analytics --- they liberate it to serve the common good, to avoid decline, and to participate in the progress Lonergan describes: the expansion of the good of order for every neighbor without exception.

\hypertarget{replies-to-objections-2}{%
\subsection{Replies to Objections}\label{replies-to-objections-2}}

\hypertarget{reply-to-objection-1-friedman}{%
\subsubsection{Reply to Objection 1 (Friedman)}\label{reply-to-objection-1-friedman}}

Friedman's claim that business is value-neutral is itself a value-laden philosophical position. It cannot justify itself empirically and collapses into moral minimalism.

\hypertarget{reply-to-objection-2-neoclassical-economics}{%
\subsubsection{Reply to Objection 2 (Neoclassical Economics)}\label{reply-to-objection-2-neoclassical-economics}}

Methodological individualism is a metaphysical choice, not a scientific discovery. It ignores relationality, solidarity, and the common good.

\hypertarget{reply-to-objection-3-descartes}{%
\subsubsection{Reply to Objection 3 (Descartes)}\label{reply-to-objection-3-descartes}}

Human beings are not automata. Treating them as such leads to managerial dehumanization and organizational decline.

\hypertarget{reply-to-objection-4-turing}{%
\subsubsection{Reply to Objection 4 (Turing)}\label{reply-to-objection-4-turing}}

Even if computation can simulate reasoning, it cannot supply meaning, purpose, or value. Analytics requires judgment, not just algorithms.

\hypertarget{reply-to-objection-5-positivism}{%
\subsubsection{Reply to Objection 5 (Positivism)}\label{reply-to-objection-5-positivism}}

Positivism is self-refuting: the claim ``only empirical statements are meaningful'' is not itself empirical.

\hypertarget{reply-to-objection-6-technocracy}{%
\subsubsection{Reply to Objection 6 (Technocracy)}\label{reply-to-objection-6-technocracy}}

Efficiency without wisdom is dangerous. Analytics without ethics becomes manipulation; analytics with philosophy becomes stewardship.

\hypertarget{quaestio-iv.-whether-the-problem-of-evil-is-intelligible-as-a-failure-of-consciousness-and-a-distortion-of-the-good-of-order}{%
\section{QUAESTIO IV. Whether the Problem of Evil Is Intelligible as a Failure of Consciousness and a Distortion of the Good of Order}\label{quaestio-iv.-whether-the-problem-of-evil-is-intelligible-as-a-failure-of-consciousness-and-a-distortion-of-the-good-of-order}}

\hypertarget{objections-3}{%
\subsubsection{Objections}\label{objections-3}}

\hypertarget{objection-1-classical-theodicy-leibniz-clarke}{%
\paragraph{Objection 1 --- Classical Theodicy (Leibniz, Clarke)}\label{objection-1-classical-theodicy-leibniz-clarke}}

S
ome argue that evil is a metaphysical necessity in the ``best of all possible worlds,'' and thus cannot be reduced to human failure or historical distortion.

Reference: \citet{leibniz_theodicy}, \emph{Theodicy}.

\hypertarget{objection-2-augustinian-privation-alone}{%
\paragraph{Objection 2 --- Augustinian Privation Alone}\label{objection-2-augustinian-privation-alone}}

Others hold that evil is simply a privation of good, lacking positive intelligibility. If evil is only absence, it cannot be analyzed as a structured failure of consciousness.

Reference: \citet{augustine_enchiridion}, \emph{Enchiridion}.

\hypertarget{objection-3-marxist-structuralism}{%
\paragraph{Objection 3 --- Marxist Structuralism}\label{objection-3-marxist-structuralism}}

Marx locates evil in material conditions and class structures, not in consciousness. Thus, decline is economic, not intellectual or moral.

Reference: \citet{marx_manuscripts}, \emph{Economic and Philosophic Manuscripts}.

\hypertarget{objection-4-nietzschean-genealogy}{%
\paragraph{Objection 4 --- Nietzschean Genealogy}\label{objection-4-nietzschean-genealogy}}

Nietzsche claims that ``evil'' is a construct of ressentiment, not an intelligible failure. Decline is the triumph of the weak, not a distortion of the good.

Reference: Friedrich \citet{nietzsche_genealogy}, \emph{Genealogy of Morals}.

\hypertarget{objection-5-secular-technocracy}{%
\paragraph{Objection 5 --- Secular Technocracy}\label{objection-5-secular-technocracy}}

Modern managerialism holds that problems are technical, not moral or intellectual. Evil is inefficiency; decline is mismanagement; consciousness is irrelevant.

Reference: Herbert \citet{simon_admin_behavior}, \emph{Administrative Behavior}.

\hypertarget{on-the-contrary-3}{%
\subsection{On the contrary}\label{on-the-contrary-3}}

Lonergan writes in \emph{Insight} that ``the root of evil is the flight from understanding,'' and that decline is ``the cumulative effect of unauthenticity in the operations of consciousness.''\\
Reference: \emph{Insight}, ch.~7--8.

In \emph{De Redemptione}, he reads Marx and Nietzsche as diagnosing real distortions of the human good, but lacking the horizon of redemption that makes intelligibility whole.\\
Reference: Lonergan, \emph{De Redemptione} (Gregorian lectures).

In his \emph{Political Economy} essays and \emph{Economic Dynamics}, he argues that economic breakdowns are not merely technical failures but the result of ``bias, short-sightedness, and the refusal of responsibility.''\\
Reference: Lonergan, \emph{For a New Political Economy}; \emph{Macroeconomic Dynamics}.

\hypertarget{i-respond-that-3}{%
\subsection{I Respond That}\label{i-respond-that-3}}

I answer that the problem of evil is indeed intelligible as a failure of consciousness and a distortion of the good of order.

Lonergan's unified account proceeds in four movements:

\hypertarget{evil-begins-in-the-subject}{%
\subsubsection{1. Evil begins in the subject}\label{evil-begins-in-the-subject}}

Evil is not first a metaphysical puzzle or a cosmic defect.\\
It begins in the refusal of attentiveness, intelligence, reasonableness, and responsibility --- the transcendental precepts that constitute authentic subjectivity.

This is the ``flight from understanding.''

\hypertarget{evil-becomes-historical-through-bias}{%
\subsubsection{2. Evil becomes historical through bias}\label{evil-becomes-historical-through-bias}}

Individual refusals accumulate into:

\begin{itemize}
\tightlist
\item
  dramatic bias (self-deception),\\
\item
  individual bias (egoism),\\
\item
  group bias (exclusion, domination),\\
\item
  general bias (anti-intellectualism, technocracy).
\end{itemize}

These biases distort the good of order, the patterned cooperation that sustains human flourishing.

\hypertarget{decline-becomes-systemic}{%
\subsubsection{3. Decline becomes systemic}\label{decline-becomes-systemic}}

Lonergan's Political Economy of the 1940s and his Economic Dynamics of the 1980s show that:

\begin{itemize}
\tightlist
\item
  economic breakdowns,\\
\item
  colonial exploitation,\\
\item
  global inequality,\\
\item
  ecological devastation,\\
\item
  technological hypertrophy (e.g., nuclear capability)
\end{itemize}

are not merely technical failures. They are cumulative consequences of inauthentic consciousness.

\hypertarget{redemption-is-the-reversal-of-decline}{%
\subsubsection{4. Redemption is the reversal of decline}\label{redemption-is-the-reversal-of-decline}}

In \emph{De Redemptione}, Lonergan integrates Marx and Nietzsche into a theological horizon:

\begin{itemize}
\tightlist
\item
  Marx diagnoses structural distortion.\\
\item
  Nietzsche diagnoses cultural and moral decay.\\
\item
  Lonergan diagnoses the root: the refusal of transcendence.
\end{itemize}

Redemption is not magic; it is the healing of consciousness, the restoration of authenticity, and the reconstitution of the good of order.

Thus, evil is intelligible --- not as a metaphysical necessity, but as a historical, cumulative, and reversible failure of consciousness.

\hypertarget{replies-to-objections-3}{%
\subsection{Replies to Objections}\label{replies-to-objections-3}}

\hypertarget{reply-to-objection-1-leibniz}{%
\subsubsection{Reply to Objection 1 (Leibniz)}\label{reply-to-objection-1-leibniz}}

Metaphysical optimism ignores the historical, cumulative character of decline. Lonergan's account is not metaphysical but existential and historical.

\hypertarget{reply-to-objection-2-augustine}{%
\subsubsection{Reply to Objection 2 (Augustine)}\label{reply-to-objection-2-augustine}}

Privation is correct metaphysically, but insufficient historically. Decline has structure, pattern, and intelligibility.

\hypertarget{reply-to-objection-3-marx}{%
\subsubsection{Reply to Objection 3 (Marx)}\label{reply-to-objection-3-marx}}

Marx sees the symptoms but not the root. Structures distort because consciousness distorts.

\hypertarget{reply-to-objection-4-nietzsche}{%
\subsubsection{Reply to Objection 4 (Nietzsche)}\label{reply-to-objection-4-nietzsche}}

Nietzsche's genealogy unmasks moral decay but cannot account for authentic moral transcendence.

\hypertarget{reply-to-objection-5-technocracy}{%
\subsubsection{Reply to Objection 5 (Technocracy)}\label{reply-to-objection-5-technocracy}}

Technocracy is itself a form of general bias --- the refusal to acknowledge the moral and intellectual roots of decline.

If Q4 establishes the \emph{inverse insight} that the intelligibility of evil is a failure of consciousness, Q5 asks whether the reversal of that evil --- structural, global, cumulative --- is possible within human capability alone.

Your framing is exactly Lonergan's:\\
- decline is intelligible,\\
- decline is cumulative,\\
- decline is self-reinforcing,\\
- and decline eventually exceeds the power of unaided human capability to reverse.

This is where the contrast with Sen and Nussbaum becomes illuminating. They see the yearning (\emph{oregesthai}) but not the horizon toward which the yearning is ordered.

\hypertarget{quaestio-v.-whether-the-reversal-of-structural-decline-and-global-evil-is-possible-within-human-capability-alone}{%
\section{QUAESTIO V. Whether the Reversal of Structural Decline and Global Evil Is Possible Within Human Capability Alone}\label{quaestio-v.-whether-the-reversal-of-structural-decline-and-global-evil-is-possible-within-human-capability-alone}}

\hypertarget{objections-4}{%
\subsection{Objections}\label{objections-4}}

\hypertarget{objection-1-sens-capability-approach}{%
\subsubsection{Objection 1 --- Sen's Capability Approach}\label{objection-1-sens-capability-approach}}

Sen argues that human flourishing depends on expanding capabilities --- freedoms to do and to be. If structural evil is a deprivation of capability, then its reversal lies in expanding human agency.

Reference: Amartya \citet{sen_development_as_freedom}, \emph{Development as Freedom}.

\hypertarget{objection-2-nussbaums-virtue-capability-synthesis}{%
\subsubsection{Objection 2 --- Nussbaum's Virtue-Capability Synthesis}\label{objection-2-nussbaums-virtue-capability-synthesis}}

Nussbaum grounds capabilities in a neo-Aristotelian virtue ethics. Human beings ``reach for'' (\emph{oregesthai}) their flourishing through cultivated virtues. Thus, moral and political reform is possible through human development alone.

Reference: Martha \citet{nussbaum_creating_capabilities}, \emph{Creating Capabilities}.

\hypertarget{objection-3-secular-humanism}{%
\subsubsection{Objection 3 --- Secular Humanism}\label{objection-3-secular-humanism}}

Human progress has historically overcome disease, poverty, and injustice. Structural evil is a technical and political challenge, not a metaphysical one.

Reference: Steven \citet{pinker_enlightenment_now}, \emph{Enlightenment Now}.

\hypertarget{objection-4-marxist-praxis}{%
\subsubsection{Objection 4 --- Marxist Praxis}\label{objection-4-marxist-praxis}}

Structural evil is rooted in material conditions. Transforming those conditions through collective action is sufficient for reversing decline.

Reference: Karl \citet{marx_manuscripts}, \emph{Theses on Feuerbach}, 1844.

\hypertarget{objection-5-nietzschean-self-overcoming}{%
\subsubsection{Objection 5 --- Nietzschean Self-Overcoming}\label{objection-5-nietzschean-self-overcoming}}

Human beings can overcome decadence through the will-to-power and the creation of new values. No transcendent horizon is needed.

Reference: Friedrich \citet{nietzsche_zarathustra} \emph{Thus spoke Zarathustra}, 1885.

\hypertarget{on-the-contrary-4}{%
\subsection{On the contrary}\label{on-the-contrary-4}}

Lonergan writes that ``the cumulative product of unauthenticity is decline,'' and that decline becomes ``a surd, a nonsense, a horror'' that exceeds the power of unaided human intelligence and will.

Reference: Bernard \citet{lonergan_insight}\emph{Insight}, ch.~7--8.

In \emph{De Redemptione}, he argues that the healing of history requires a transformation ``beyond the resources of human nature,'' grounded in the divine self-communication
.\\
Reference: Bernard \citet{lonergan_de_redemptione}, \emph{De Redemptione} (Gregorian lectures).

Catholic social teaching affirms that structural sin requires conversion, grace, and a horizon of transcendence.

Reference: \citet{johnpaul_sollicitudo_rei_socialis} \emph{Sollicitudo Rei Socialis}; \citet{vat2_gaudium_et_spes} \emph{Gaudium et Spes}.

\hypertarget{i-respond-that-4}{%
\subsection{I respond that}\label{i-respond-that-4}}

I answer that the reversal of structural decline and global evil is not possible within human capability alone.

\hypertarget{structural-evil-is-cumulative-and-self-reinforcing}{%
\subsubsection{1. Structural evil is cumulative and self-reinforcing}\label{structural-evil-is-cumulative-and-self-reinforcing}}

Lonergan's analysis shows that:

\begin{itemize}
\tightlist
\item
  individual bias\\
\item
  group bias\\
\item
  general bias\\
\item
  and dramatic bias
\end{itemize}

all compound into structural sin --- patterns of cooperation that systematically harm the vulnerable and distort the good of order.

Once entrenched, these structures exceed the power of individual or collective goodwill.

\hypertarget{human-capability-is-necessary-but-insufficient}{%
\subsubsection{2. Human capability is necessary but insufficient}\label{human-capability-is-necessary-but-insufficient}}

Sen and Nussbaum correctly identify the human yearning (\emph{orexis}) for flourishing. But they lack the horizon of vertical transcendence:

\begin{itemize}
\tightlist
\item
  the yearning is real,\\
\item
  but the fulfillment lies beyond human self-assertion,\\
\item
  because the wound, teh vulnerability, is deeper than human capability.
\end{itemize}

\hypertarget{sin-is-not-merely-personal-but-historical}{%
\subsubsection{3. Sin is not merely personal but historical}\label{sin-is-not-merely-personal-but-historical}}

Lonergan's reading of history shows that:

\begin{itemize}
\tightlist
\item
  decline becomes global (and greater than the sum of the nations, regions, tribes, families),\\
\item
  decline becomes cultural (norms, values, stories, binding ),\\
\item
  decline becomes economic (the material well-being of the good of order),\\
\item
  decline becomes ecological (all connected, bio-, psycho-, socio-, anthropo-logical),\\
\item
  decline becomes technological (the works of our hands, nuclear capability, AI misuse, extractive capitalism).
\end{itemize}

This is original sin writ communally, not metaphorically but structurally.

\hypertarget{the-solution-requires-a-surplus-of-meaning-value-and-love}{%
\subsubsection{4. The solution requires a surplus of meaning, value, and love}\label{the-solution-requires-a-surplus-of-meaning-value-and-love}}

\begin{itemize}
\tightlist
\item
  Human capability can diagnose decline.\\
\item
  Human capability can mitigate decline.\\
\item
  But human capability cannot reverse decline.
\end{itemize}

Reversal requires:

\begin{itemize}
\tightlist
\item
  conversion,\\
\item
  grace, an unforced generosity,\\
\item
  a new horizon of meaning,\\
\item
  the divine self-communication Lonergan calls ``God's love flooding our hearts.''
\end{itemize}

This is not pious sentiment.It is the only adequate response to a problem that has exceeded and will necessarily exceed human scope and scale.

\hypertarget{the-yearning-oregesthai-points-beyond-itself}{%
\subsubsection{\texorpdfstring{5. The yearning (\emph{oregesthai}) points beyond itself}{5. The yearning (oregesthai) points beyond itself}}\label{the-yearning-oregesthai-points-beyond-itself}}

\citet{nussbaum_fragility_of_goodness} uses the medio-passive \emph{oregesthai},and it is telling:

\begin{itemize}
\tightlist
\item
  ``to reach for,
\item
  ``to stretch toward,''\\
\item
  ``to long for.''
\end{itemize}

But what is the object of this yearning?

Lonergan's answer: \emph{the unrestricted desire to know, to love, to be --- the desire for God.}

Thus, the very structure, the design itself, of human capability points beyond itself.

\hypertarget{replies-to-objections-4}{%
\subsection{Replies to Objections}\label{replies-to-objections-4}}

\hypertarget{reply-to-objection-1-sen}{%
\subsubsection{Reply to Objection 1 (Sen)}\label{reply-to-objection-1-sen}}

Capabilities expand agency but cannot heal the root of decline: bias and unauthenticity.

\hypertarget{reply-to-objection-2-nussbaum}{%
\subsubsection{Reply to Objection 2 (Nussbaum)}\label{reply-to-objection-2-nussbaum}}

Virtue is necessary, but without transcendence it becomes Pelagian --- insufficient for structural evil.

\hypertarget{reply-to-objection-3-secular-humanism}{%
\subsubsection{Reply to Objection 3 (Secular Humanism)}\label{reply-to-objection-3-secular-humanism}}

Technological progress does not reverse moral or structural decline; it often amplifies it.

\hypertarget{reply-to-objection-4-marx}{%
\subsubsection{Reply to Objection 4 (Marx)}\label{reply-to-objection-4-marx}}

Material transformation without conversion reproduces the same distortions under new forms.

\hypertarget{reply-to-objection-5-nietzsche}{%
\subsubsection{Reply to Objection 5 (Nietzsche)}\label{reply-to-objection-5-nietzsche}}

Self-overcoming cannot overcome the self-enclosure that constitutes decline.

This last \emph{quaestio} is extraordinary in the sense it is the whole that pulls together the parts, yet is subservient to the principles of the whole exceeding the parts. It begins tp completes the arc:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The whole exceeds the parts.\\
\item
  Deduction yields new knowledge.\\
\item
  Analytics requires philosophy and theology.\\
\item
  Evil is intelligible as a failure of consciousness.
\end{enumerate}

Reversal requires transcendence beyond human capability.

\hypertarget{the-good-of-progress}{%
\chapter{The Good of Progress}\label{the-good-of-progress}}

Here are some pressure points for the entire Western intellectual tradition. Let's take them in turn and then braid them back into Lonergan's arc of progress/decline.

\hypertarget{is-the-whole-greater-than-the-sum-of-the-parts}{%
\section{Is the whole greater than the sum of the parts?}\label{is-the-whole-greater-than-the-sum-of-the-parts}}

We're right to plant our flag firmly in the \emph{affirmative} and not just rhetorically --- metaphysically, epistemologically, ethically.

\hypertarget{why-the-objectors-fail}{%
\subsection{Why the objectors fail}\label{why-the-objectors-fail}}

\begin{itemize}
\tightlist
\item
  \textbf{Bacon} reduces intelligibility to empirical fragments, so of course he can't see emergent form.\\
\item
  \textbf{Scientism} collapses all explanation into mechanism, so it literally cannot \emph{see} wholes.\\
\item
  \textbf{Reductionist neuroscience} treats consciousness as epiphenomenal chemistry, which is like explaining Hamlet by analyzing the ink.\\
\item
  And yes --- if one is going to reduce thought to chemistry, why stop there? Why not reduce chemistry to QED amplitudes? Why not reduce \emph{those} to the vacuum state? \href{https://en.wikipedia.org/wiki/Reductionism}{Reductionism} always eats itself. It refutes itself in a classical \emph{retorquendo} move.
\end{itemize}

\hypertarget{why-the-affirmative-is-the-only-coherent-position}{%
\subsection{Why the affirmative is the only coherent position}\label{why-the-affirmative-is-the-only-coherent-position}}

A whole is not a pile of stuff. A whole is a \emph{form}, a \emph{pattern}, an \emph{intelligibility} that:

\begin{itemize}
\tightlist
\item
  organizes the parts,\\
\item
  conditions their relations,\\
\item
  and generates emergent properties irreducible to the components.
\end{itemize}

This is Aristotle â†’ Aquinas â†’ Lonergan â†’ modern complexity theory.

We're defending the very possibility of \textbf{intelligibility}, \textbf{meaning}, and \textbf{value}.

\hypertarget{is-deduction-new-knowledge}{%
\section{Is deduction new knowledge?}\label{is-deduction-new-knowledge}}

Again, your affirmative stance is exactly right --- and Lonergan gives you the conceptual artillery.

\hypertarget{why-the-skeptics-deny-it}{%
\subsection{Why the skeptics deny it}\label{why-the-skeptics-deny-it}}

\begin{itemize}
\tightlist
\item
  \textbf{Hume}: deduction is analytic tautology.\\
\item
  \textbf{Locke}: the mind is a passive container; deduction rearranges ideas but adds nothing.\\
\item
  \textbf{Kant}: synthetic a priori is possible, but deduction itself is not knowledge of being --- it is only the structuring of appearances.\\
\item
  \textbf{Callicles, Sartre, Nietzsche (in his early phase)}: knowledge is power-play, not insight.
\end{itemize}

All of these positions share a single root: \textbf{they deny that understanding is a real grasp of intelligibility in being.}

If knowing is not \emph{being-in-contact-with-reality}, then deduction is just linguistic shuffling.

\hypertarget{lonergans-counter-position}{%
\subsection{Lonergan's counter-position}\label{lonergans-counter-position}}

Lonergan's breakthrough is that \textbf{insight transforms data}, and \textbf{judgment transforms insight}. Deduction is not mechanical manipulation --- it is:

\begin{itemize}
\tightlist
\item
  the unfolding of the intelligibility grasped in insight,\\
\item
  the extension of understanding into new domains,\\
\item
  the discovery of implications that were \emph{not} present to consciousness before.
\end{itemize}

Deduction is new knowledge because \textbf{the mind is dynamically self-transcending}.

\hypertarget{progress-and-decline-why-these-two-questions-matter}{%
\section{Progress and Decline: Why These Two Questions Matter}\label{progress-and-decline-why-these-two-questions-matter}}

We connect these metaphysical questions to Lonergan's moral-historical cycle.

\hypertarget{progress}{%
\subsection{Progress}\label{progress}}

Progress occurs when:
- the mind affirms that wholes exist (intelligibility is real),\\
- the mind trusts its own capacity for insight and judgment,\\
- the community orients itself toward the good of the neighbor,\\
- and the good of order expands to include the vulnerable.

This is the \emph{yes} to being, to intelligibility, to value, to God.

\hypertarget{decline}{%
\subsection{Decline}\label{decline}}

Decline is not random. It is the predictable result of:
- reductionism (no wholes),\\
- skepticism (no new knowledge),\\
- individualism (no neighbor),\\
- and self-enclosure (no transcendence).

Decline is the \emph{no} to being, the \emph{no} to intelligibility, the \emph{no} to the neighbor, the \emph{no} to God.

It is privation, deflection, and self-protection masquerading as realism.

\hypertarget{our-position-stated-cleanly}{%
\section{Our Position, Stated Cleanly}\label{our-position-stated-cleanly}}

We're defending a metaphysics and epistemology that make:

\begin{itemize}
\tightlist
\item
  \textbf{emergence possible},\\
\item
  \textbf{insight real},\\
\item
  \textbf{deduction fruitful},\\
\item
  \textbf{progress intelligible},\\
\item
  \textbf{decline diagnosable},\\
\item
  \textbf{and the good of the neighbor non-negotiable}.
\end{itemize}

This is not ``same old - same old.'' This is the beating heart of a humane, theologically grounded, intellectually rigorous worldview.

\hypertarget{then-theres-uncertainty}{%
\section{Then there's uncertainty}\label{then-theres-uncertainty}}

We're reaching for exactly the right \emph{nerve center}: the moment in Nicholas Cusanus, \textbf{\emph{De Docta Ignorantia} I.3--4} where \citet{cusanus_ddi_english} articulates (a) the \textbf{impossibility theorem}---finite reason cannot ground or explain itself---and (b) the \textbf{coincidentia oppositorum} as the only horizon in which inquiry can proceed at all.

Below is the passage most directly expressing the ``impossibility theorem'' and the ``coincidence of opposites'' as a condition of possibility for wonder, inquiry, and analogy.

\hypertarget{de-docta-ignorantia-herein-ddi-i.34}{%
\subsection{\texorpdfstring{De Docta Ignorantia (herein, \emph{DDI}) I.3--4}{De Docta Ignorantia (herein, DDI) I.3--4}}\label{de-docta-ignorantia-herein-ddi-i.34}}

\hypertarget{on-the-impossibility-of-science-grounding-itself}{%
\subsubsection{On the impossibility of science grounding itself}\label{on-the-impossibility-of-science-grounding-itself}}

\emph{Cum igitur omne cognoscibile mensurari oporteat mensura propria, mensura autem infinita finitam mensuram excedat, impossibile est scientiam de infinito secundum propriam rationem haberi.}
(\emph{DDI} I.3)

``Since everything knowable must be measured by its proper measure, and since the infinite exceeds every finite measure, it is impossible to have a science of the infinite according to its own proper rational method.''

This is the core of Cusa's ``impossibility theorem'': \textbf{science cannot explain or ground itself}, because the very act of knowing presupposes a measure that finite reason cannot supply.

\hypertarget{on-the-coincidence-of-opposites-as-the-horizon-of-wonder-and-understanding}{%
\subsubsection{On the coincidence of opposites as the horizon of wonder and understanding}\label{on-the-coincidence-of-opposites-as-the-horizon-of-wonder-and-understanding}}

\emph{In infinito autem, ubi omnium oppositorum coincidentia est, nostra mens, quae per differentias intelligit, deficit; et in hoc deficit, ut mirari discat.}
( \emph{DDI} I.4)

``But in the Infinite, where the coincidence of all opposites is found, our mind---which understands by means of differences---fails; and in this very failing it learns to wonder.''

Here we stand.

\begin{itemize}
\tightlist
\item
  finite reason fails,\\
\item
  but the failure is \textbf{fruitful},\\
\item
  because it opens onto wonder,\\
\item
  which is the condition for analogy (Przywara),\\
\item
  and the border-logic Wittgenstein intuits (a boundary presupposes two sides).
\end{itemize}

\hypertarget{on-the-necessity-of-coincidence-for-any-knowledge-at-all}{%
\subsection{On the necessity of coincidence for any knowledge at all}\label{on-the-necessity-of-coincidence-for-any-knowledge-at-all}}

\emph{Quoniam igitur in Deo, qui est omnium mensura, opposita coincidunt, mens nostra, quae in oppositis haeret, ad veritatem nisi in docta ignorantia non ascendit.}
( \emph{DDI} I.4)

``Since in God, who is the measure of all things, opposites coincide, our mind---which clings to opposites---cannot ascend to truth except through learned ignorance.''

This is the metaphysical ground of Przywara's \emph{analogia entis}:

\begin{itemize}
\tightlist
\item
  God is the coincidence of opposites,\\
\item
  creatures are the ``ever greater dissimilarity,''\\
\item
  analogy is the oscillation between similarity and dissimilarity,\\
\item
  wonder is the epistemic posture proper to this oscillation.
\end{itemize}

\}

\hypertarget{wittgensteins-borderlogic}{%
\subsubsection{Wittgenstein's borderâ€‘logic}\label{wittgensteins-borderlogic}}

The key line is \textbf{Tractatus 5.6}:

\begin{quote}
\emph{``The limits of my language mean the limits of my world.''}\\
(\emph{Tractatus} 5.6)
\end{quote}

This is the passage that resonates with Cusa's \emph{docta ignorantia}:

\begin{itemize}
\tightlist
\item
  A \textbf{limit} is not a wall but a \textbf{border}.\\
\item
  A border implies \textbf{two sides}.\\
\item
  What lies beyond the limit is not nonsense but the \textbf{condition} for meaningful sense.\\
\item
  Language cannot contain the whole, but its very limit points to the whole.
\end{itemize}

This is why the \emph{Tractatus} ends with the famous 7th note:

\begin{quote}
\emph{``Whereof one cannot speak, thereof one must be silent.''}\\
(\emph{Tractatus} 7)
\end{quote}

Silence here is not negation --- it is \textbf{reverence}, the philosophical analogue of Cusa's wonder.

How might this connect Cusa-to-Wittgenstein-to-Przywara?

\begin{itemize}
\tightlist
\item
  \textbf{Cusa}: The mind fails at the infinite, and in failing learns to wonder.\\
\item
  \textbf{Wittgenstein}: The limit of language is the border where meaning opens beyond itself.\\
\item
  \textbf{Przywara}: The creature exists in a rhythmic tension of similarity and everâ€‘greater dissimilarity to God --- the \emph{analogia entis}.
\end{itemize}

All three converge on a single insight:

\textbf{The limit is not the end of understanding but the beginning of wisdom.}\footnote{Nicolaus Cusanus articulates the foundational limit of finite reason in \emph{De Docta Ignorantia} I.3--4 (\citet{cusanus_ddi_english}), where he argues that the infinite exceeds every finite measure and therefore cannot be grasped by any science that presupposes its own rational method: ``\emph{Cum igitur omne cognoscibile mensurari oporteat mensura propria, mensura autem infinita finitam mensuram excedat, impossibile est scientiam de infinito secundum propriam rationem haberi}.'' He next develops the epistemic consequence: ``\emph{In infinito autem, ubi omnium oppositorum coincidentia est, nostra mens, quae per differentias intelligit, deficit; et in hoc deficit, ut mirari discat}.'' The mind in a finite universe with a finite body (brain at least) falters right at the boundary of its own powers, yet the falling down is fruitful, for it reaches out with wonder. This borderâ€‘logic anticipates Wittgenstein's insight that a boundary presupposes two regions --- one on either side --- and that what lies beyond the limit of language is not nonsense but the very condition of meaningful speech, and thus of meaningful communication of analytical results to consumers of the analysis. \citet{przywara_analogia} develops the Cusanus structure into the rhythmic, cyclic ``ever greater dissimilarity'' of the \emph{analogia entis,} where creaturely, finite being is constituted by the tension between similarity and dissimilarity to the divine, the ever more relative to the creature. Thus, Cusa's \emph{coincidentia oppositorum} is not a collapse of distinctions but the metaphysical horizon within which analogy, wonder, and inquiry become possible.}

\hypertarget{great-expectations}{%
\chapter{Great Expectations}\label{great-expectations}}

\hypertarget{the-maths-the-maths}{%
\section{The maths! The maths!}\label{the-maths-the-maths}}

We can fuss about all we want about the maths, but they are impervious to our feelings. They remain. We can stay, or go. If we stay, and spend the time in active pursuit (just like a waiting time, waiting for insight, or not, waiting for Godot, who never shows up), we might achieve a learning apogee. We suppose that we will stay awhile, for the time being. Now let us dig into our model of waiting times. Our first stop is a set of tools we will need for the excavation.

In what follows we use \(Y\) as the wage, the metric we want to generate from its mean and standard deviation. We conjecture that \(Y\) depends on \(X\), the level of educational attainment through the conditional mean of \(Y \mid X\), just like we did with vaccination waiting times.

\hypertarget{what-did-we-all-expect}{%
\section{What did we all expect?}\label{what-did-we-all-expect}}

We define expectations as aggregations of two kinds of information. One is the information provided by an array of outcomes \(Y_i\) for \(i=1 \ldots N\), where \(i\) indexes \(N\) outcomes. The other is the array of probabilities assigned to each outcome \(\pi_i\). The frequentist will assign \(\pi = f_i/N\), where \(f_i\) is the long-run frequency of occurrence of outcome \(i\). Instead, with \citet{Jeffreys1966} and \citet{Jaynes2004} we will assign \(\pi\) as a normalized index of the logical plausibility of an outcome where all \(\pi\)s add up to one and each is somewhere between 0 and 1. After all we can't wait for the long-run, we have to go to school!

This allows us to interpret probability as an extension of logic, where probability quantifies the reasonable expectation that everyone (even a \emph{robot} or \emph{golem} ) who shares the same knowledge ( experience, understanding, \emph{and} judgment) should share in accordance with the rules of conditional probability.\footnote{Cox's theorem}(\url{https://en.wikipedia.org/wiki/Cox\%27s_theorem}) provides a logical underpinning to this statement: the rules of probability theory need not be derived from a definition of probabilities as relative frequencies (frequentist approach). He goes further to show that the properties of probability as logic but also follow from certain properties one might desire of any system of plausible reasoning about uncertainty. \href{https://reader.elsevier.com/reader/sd/pii/S0888613X03000513?token=7D6FBDB41F8831DD4DE22162DFA2FAA3A8786A9B0CAB6231C759CF725E203785164FE0BAE53EFCB46143DF016439BCD6\&originRegion=us-east-1\&originCreation=20211123164351}{Van Horn} is a tutorial on Cox's approach. Plausible reasoning may be illustrated with this example of the distinction between gradual degrees of possible outcomes, that is, uncertainty, and what is or is not, that is, truth. As an example, one's confidence in the statement \emph{Daniel is well over six feet tall}, after seeing Daniel, legs splayed out, sitting at a desk, is a degree of plausibility. In contrast, the statement \emph{Daniel is tall} may be somewhat true (if Daniel measures five feet eleven inches and the definition of tall is greater than or equal to six feet) or entirely true (if Daniel measures seven feet one inch).{]} All of this ensures we have a complete picture of all of the probability contributions, as weights, of each outcome consistent with a systematic, principled way to reason about uncertainty, as we mash together data and hypotheses about the data.

The aggregation we propose is then this expression for the expectation \(E\) of outcomes \(Y\).

\begin{align}
\operatorname{E}Y = \sum_{i}^{N} \pi_i Y_i
\end{align}

In this way we can say that \(\operatorname{E}\) operates on \(Y\) where \textbf{to operate} means \textbf{to aggregate} several outcomes \(X_i\) into one number (or possibly function) by multiplying probability weights times outcomes and then summing the products. That's really two operations combined into the expectation operation. And so goes the maths!

Using this idea of an operator \(\operatorname{E}Y\) means we define the aggregation as this expression.

\begin{align}
\operatorname{E} = \sum_{i}^{N} \pi_i \times
\end{align}

Here are some of the algebraic rules of the road when we use this highly condensed short-hand notation.

\begin{align}
Y &= \alpha\,X \\
\operatorname{E}Y &= \operatorname{E}[\alpha\,X] \\
                  &= \sum_{i}^{N}[\pi_i\,(\alpha\,X_i) ] \\
                  &= \pi_1\,\alpha\,X_1 + \ldots \pi_N\,\alpha\,X_N \\
                  &= \alpha \, (\pi_1\,X_1 + \ldots \pi_N\,X_N) \\
                  &= \alpha\,\sum_{i}^{N}[\pi_i\,(X_i) ] \\
                  &= \alpha\,\operatorname{E}X
\end{align}

This means that we can take the constant \(\alpha\) outside of the expectation operator. All we did, step by step on the logical staircase, is to use the definition of the operator and then manipulate it algebraicly to deduce an equivalent expression.

If \(X_1=1, \ldots, X_N=1\), and the sum of probabilities \(\sum_{i}^N \, \pi_i = 1\), then we can deduce this expression.

\begin{align}
Y &= \alpha\,X \\
\operatorname{E}Y &= \operatorname{E}[\alpha\,X] \\
                  &= \sum_{i}^{N}[\pi_i\,\dot (\alpha\,X_i) ] \\
                  &= \pi_1\,\alpha\,(1) + \ldots \pi_N\,\alpha\,(1) \\
                  &= \alpha \, (\pi_1\,(1) + \ldots \pi_N\,(1) \\
                  &= \alpha\,\sum_{i}^{N}[\pi_i (1)] \\
                  &= \alpha\,\operatorname{E}1 \\
                  &= \alpha
\end{align}

This may have been immediately clear to some of us before the 7 step deduction, but we might find it reassuring that the deduction verifies, and perhaps validates, our initial conjecture. We also discover another relationship.

\[
\operatorname{E}1 = 1
\]

In algebra we call this the identity operator. For any number or variable, or even another expectation, \(\alpha\), then this is true.

\begin{align}
\alpha \, \operatorname{E}1 &= \alpha\, 1 \\
                            &= \alpha
\end{align}

Yes, this is identity under a multiplication. Is there a zero? Yes, \(\operatorname{E}0 = 0\), the identity operator under addition. Anything added to \(\operatorname{E}0=0\) just returns itself.

What is the expectation of a sum of variables \(X\) and \(Y\)?

\begin{align}
Z &= X+Y \\
\operatorname{E}Z &= \operatorname{E}[X + Y] \\
                  &= \sum_{i}^{N}[\pi_i\,(\,X_i + Y_i) ] \\
                  &= \pi_1\,\,(X_1 + Y_1) + \ldots \pi_N\,(X_N+Y_N) \\
                  &= (\pi_1\,X_1 + \ldots \pi_1\,X_N) + (\pi_N\,Y_N + \ldots \pi_N\,Y_N)  \\
                  &= \sum_{i}^{N}[\pi_i\,(X_i) ] + \sum_{i}^{N}[\pi_i\,(Y_i) ] \\
                  &= \operatorname{E}X + \operatorname{E}Y
\end{align}

The expectation of a sum of outcome variables is the sum of the expectations of each variable.

We just examined a sum of two variables, so it behooves us to look at the product of two variables.

\begin{align}
Z &= XY \\
\operatorname{E}Z &= \operatorname{E}[XY] \\
                  &= \sum_{i}^{N}[\pi_i\,(\,X_i\,Y_i) ] \\
                  &= \pi_1\,X_1 \, Y_1 + \ldots \pi_N\,X_N\,Y_N) \\
                  &= \operatorname{E}XY
\end{align}

Alas, we have reduced this operation to its simplest expression already. If \(Y=X\), going through the same steps as above we find this out.

\begin{align}
if\,\,Z &= XY \\
and \\
Y &= X \\
then \\
Z&= XX\\
\operatorname{E}Z &= \operatorname{E}[XX] \\
                  &= \sum_{i}^{N}[\pi_i\,(\,X_i\,X_i) ] \\
                  &= \pi_1\,X_1 \, X_1 + \ldots \pi_N\,X_N\,X_N) \\
                  &= \pi_1\,X_1^2 + \ldots \pi_N\,X_N^2 \\
                  &= \operatorname{E}X^2
\end{align}

It turns out that we can take an expression like this, \(Y=\alpha + \beta\,X\), multiply it by \(X\) and, then operate on it with \(\operatorname{E} = \sum_{i}^{N} \pi_i \times\) with the tools we now possess.

\begin{align}
Y &=\alpha + \beta\,X \\
XY &= \alpha\,X + \beta\,XX \\
XY &= \alpha\,X + \beta\,X^2 \\
\operatorname{E}XY &= \operatorname{E}[\alpha\,X + \beta\,X^2] \\
                   &= \operatorname{E}[\alpha\,X] + \operatorname{E}[\beta\,X^2] \\
                   &= \alpha\,\operatorname{E}[X] + \beta\,\operatorname{E}[X^2]
\end{align}

This will be very useful indeed. We usually will call \(\operatorname{E}X = \mu_X\) in honor of the \textbf{mean} of the population of all possible realizations of \(X\). We already know this as the weighted average of \(X\) outcomes, where the weights are probabilities, all of which add up to 1. What about \(\operatorname{E}X^2\)? To ponder this we consider the calculation of another very familiar metric, the square of the standard deviation, which has been dubbed the \textbf{variance}. We start with the definition and use all of the new tricks up our sleeves. We define variance as the probability weighted average of squared deviations of outcomes from the expected outcome.

We will need the remembrance of things in our algebraic past that look like this.

\begin{align}
(a + b)^2 &= (a + b)(a + b) \\
          &= a^2 + 2ab + b^2
\end{align}

In what follows \(a = X\) and \(b = -\operatorname{E}X\). We will also need to remember that \(-2b^2 + b^2 = -b^2\).

\begin{align}
define \\
\sigma_X^2 &= Var(X) \\
then \\
Var(X) &= \operatorname{E}(X - \operatorname{E}X)^2 \\
       &= \operatorname{E}(X^2 - 2X\operatorname{E}X + \operatorname{E}X^2) \\
       &= \operatorname{E}X^2 - \operatorname{E}[2X\operatorname{E}X] + \operatorname{E}[\operatorname{E}X^2] \\
       &= \operatorname{E}X^2 - 2(\operatorname{E}X)^2 + (\operatorname{E}X)^2 \\
       &= \operatorname{E}X^2 - (\operatorname{E}X)^2 \\
       &= \operatorname{E}X^2 - \mu_X^2 \\
       thus \\
\sigma_{X}^2 &= \operatorname{E}X^2 - \mu_X^2 \\
rearranging \\
\operatorname{E}X^2 &= \sigma_{X}^2 + \mu_X^2
\end{align}

Yes, we can breathe a collective sigh of relief having accomplished these algebraic acrobatics! But we must now move on to the \emph{piece de resistance} , \(\operatorname{E}XY\). We start with the definition of covariance, for this is where an \(XY\) product resides.

\begin{align}
define \\
\sigma_{XY} &= Cov(X, Y) \\
then \\
Cov(X, Y) &= \operatorname{E}(X - \operatorname{E}X)(Y - \operatorname{E}Y) \\
          &= \operatorname{E}(XY - X\operatorname{E}Y - Y\operatorname{E}X + \operatorname{E}X\,\operatorname{E}Y) \\
          &= \operatorname{E}(XY - \operatorname{E}X\,\operatorname{E}Y  - \operatorname{E}Y\,\operatorname{E}X + \operatorname{E}X\,\operatorname{E}Y) \\
          &= \operatorname{E}XY - 2\operatorname{E}X\,\operatorname{E}Y + \operatorname{E}\,X[\operatorname{E}Y \\
          &= \operatorname{E}XY - \operatorname{E}X\,\operatorname{E}Y \\
          thus \\
\sigma_{XY} &= \operatorname{E}XY - \mu_X\mu_Y \\
rearranging \\
\operatorname{E}XY &= \sigma_{XY} + \mu_X\mu_Y
\end{align}

Now we can go to work on our model with one more stop: solving a simultaneous equation. This tool too will come in handy. We suppose we have the following two equations in \(a\) and \(b\). We will use the row-column convention of subscripts. Thus coefficient \(c_{12}\) will be in row 1, column 2 of a matrix. First the two equations.

\begin{align}
c_{11}a + c_{12}b &= d_1 \\
c_{21}a + c_{22}b &= d_2
\end{align}

In matrix form this is a very tidy arrangement like this.

\begin{align}
\begin{bmatrix}
c_{11} & c_{12}  \\
c_{21} & c_{22}
\end{bmatrix}
\begin{bmatrix}
a  \\
b
\end{bmatrix}
&=
\begin{bmatrix}
d_1  \\
d_2
\end{bmatrix} \\
\mathrm{C}\mathrm{a} &= \mathrm{d}
\end{align}

Very tidy indeed! We might remember that a unique solution exists only if (or is it if and only if?) the determinant of the matrix \(\mathrm{C}\) is not zero. If it is, then the solution \(\mathrm{a}=\mathrm{C}^{-1}d\) does not exist and the model is singular. In what we will do below we will compose our coefficients of means, standard deviations and correlations. Some combinations of these aggregations, constants, will prove to yield a zero determinant, and a singular model results.

The determinant \(\det{\mathrm{C}}\) is

\[
\det{\mathrm{C}} = c_{11}c_{22}-c_{12}c_{21}
\]

The solution proceeds in two sweeps, one for each of \(a\) and \(b\). In the first sweep we replace the first, the \(a\) column, in \(\mathrm{C}\) with the column vector \(d\). We find the determinant of this new \(\mathrm{C}_a\) matrix and divide by \(\det{\mathrm{C}}\). Here we go.

\begin{align}
original \, \, &\mathrm{C} \\
\begin{bmatrix}
c_{11} & c_{12}  \\
c_{21} & c_{22}
\end{bmatrix} \\
swap\,\, out\,\, &first\,\, column \\
\mathrm{C}_a &=
\begin{bmatrix}
d_{1} & c_{12}  \\
d_{2} & c_{22}
\end{bmatrix} \\
then \\
a &= \frac{\det{\mathrm{C_a}}}{\det{\mathrm{C}}} \\
  &= \frac{d_1c_{22}-d_2c_{12}}{c_{11}c_{22}-c_{12}c_{21}}
\end{align}

Now the second sweep in all its glory.

\begin{align}
original \, \, &\mathrm{C} \\
\begin{bmatrix}
c_{11} & c_{12}  \\
c_{21} & c_{22}
\end{bmatrix} \\
swap\,\, out\,\, &first\,\, column \\
\mathrm{C}_b &=
\begin{bmatrix}
c_{11} & d_{1}  \\
c_{21} & d_2
\end{bmatrix} \\
then \\
b &= \frac{\det{\mathrm{C_b}}}{\det{\mathrm{C}}} \\
  &= \frac{c_{11}d_2-c_{21}d_1}{c_{11}c_{22}-c_{12}c_{21}}
\end{align}

Very much a formula for the ages.

\hypertarget{walking-the-straight-line}{%
\section{Walking the straight line}\label{walking-the-straight-line}}

Here is our model where both \(Y\) and \(X\) have some distribution with \(\pi\) probabilities for each. Here we use \(\pi\) as the Greek letter for \(p\), not as the \(\pi\) of circle fame. Both \(Y\) and \(X\) are what we will very loosely call \textbf{random variables}, because they have outcomes with associated probabilities of occurrence.

\[
Y = \alpha + \beta\, X
\]

We now ask the question, what is \(\operatorname{E(Y \mid X=x)=\mu_{Y \mid X}}\)? What, on weighted average, can we expect \(Y\) to be? First of all, this must be true.

\begin{align}
if \\
\operatorname{E}(Y \mid X=x) &= \mu_{Y \mid X} \\
then \\
\mu_{Y \mid X} &= \operatorname{E}(\alpha + \beta\, X) \\
                &= \operatorname{E}\alpha (1) + \operatorname{E}(\beta\,X) \\
                &= \alpha\,\operatorname{E}1 + \beta\,\operatorname{E}X \\
                &= \alpha\,(1) + \beta\,\mu_X \\
                &= \alpha + \beta\,\mu_X
\end{align}

Result one is in hand, \(\mu_{Y \mid X}= \alpha + \beta\,\mu_X\) is a true statement according to our many deductions. By the way the statement \(\mu_{Y \mid X} = \alpha\,\operatorname{E}1 + \beta\,\operatorname{E}X\) is an example of the distributive property of multiplication over addition.

Now for our second trick we multiply \(Y\) by \(X\) to get a second result and a second true statement. We will condense \(Y \mid X = Y\) to save what's left of our eyesight. We remember all of our hard work above, especially this inventory of results.

\begin{align}
\operatorname{E}Y &= \mu_{Y} \\
\operatorname{E}X &= \mu_{X} \\
\operatorname{E}X^2 &= \sigma_{X}^2 + \mu_X^2 \\
\operatorname{E}XY &= \sigma_{XY} + \mu_X\mu_Y
\end{align}

Using this inventory more than a few times we get these results.

\begin{align}
Y &= \alpha + \beta\, X \\
then \\
XY &= \alpha\,X + \beta\, XX \\
   &= \alpha\,X + \beta\, X^2 \\
so\,\,that \\
\operatorname{E}XY &= \operatorname{E}(\alpha\,X + \beta\, X^2) \\
                &= \operatorname{E}\alpha\,X + \operatorname{E}\beta\,X^2 \\
                &= \alpha\,\operatorname{E}X + \beta\,\operatorname{E}X^2 \\
                &= \alpha\,\mu_X + \beta\,(\sigma_X^2 + \mu_X^2) \\
but\,\,we\,\,know\,\,that \\
\operatorname{E}XY &= \sigma_{XY} + \mu_X\mu_Y \\
thus,\,\, again \\
\sigma_{XY} + \mu_X\mu_Y &= \alpha\,\mu_X + \beta\,(\sigma_X^2 + \mu_X^2)
\end{align}

We now have two equations in two, as yet to be determined, unknowns. They are unobserved data, \(\alpha\) and \(\beta\). Both equations are true, and true jointly. This means we can stack one on top of the other as a simultaneous equation system and, we hope this time, solve them for unique values of \(\alpha\) and \(\beta\). Yes, we demand a formula!

Here are the two equations with \(\alpha\) and \(\beta\) terms on the left-hand side and constant terms, the expectations are all constant aggregations, on the right-hand side of the equation. We also commutes the terms so that our unknowns are pre-multiplied by coefficients.

\begin{align}
\alpha + \mu_X\,\beta &= \mu_Y \\
\mu_X\,\alpha + (\sigma_X^2 + \mu_X^2)\,\beta &= \sigma_{XY} + \mu_X\mu_Y
\end{align}

The matrix representation will help us easily match coefficients with our simultaneous equation model, way above as we replicate below.

\begin{align}
\begin{bmatrix}
c_{11} & c_{12}  \\
c_{21} & c_{22}
\end{bmatrix}
\begin{bmatrix}
a  \\
b
\end{bmatrix}
&=
\begin{bmatrix}
d_1  \\
d_2
\end{bmatrix} \\
\mathrm{C}\mathrm{a} &= \mathrm{d}
\end{align}

Our simultaneous equations of expected values for the linear model \(Y=\alpha+\beta X\) yields this structure.

\begin{align}
\alpha + \mu_X\,\beta &= \mu_Y \\
\mu_X\,\alpha + (\sigma_X^2 + \mu_X^2)\,\beta &= \sigma_{XY} + \mu_X\mu_Y \\
becomes \\
\begin{bmatrix}
1     & \mu_X  \\
\mu_X & \sigma_X^2 + \mu_X^2
\end{bmatrix}
\begin{bmatrix}
\alpha  \\
\beta
\end{bmatrix}
&=
\begin{bmatrix}
\mu_Y  \\
\sigma_{XY} + \mu_X\mu_Y
\end{bmatrix} \\
\mathrm{C}\mathrm{a} &= \mathrm{d}
\end{align}

We can solve for the unobserved, unknown, and otherwise conjectured (we might smell a hypothesis brewing here) \(\alpha\) and \(\beta\) using our trusty determinant solutions.

\begin{align}
\alpha &= \frac{\mu_Y(\sigma_x^2 + \mu_X^2) - \mu_X(\sigma_{XY} + \mu_X\mu_Y)}{\sigma_X^2 + \mu_X^2 - \mu_x^2} \\
       &= \frac{\mu_Y\sigma_X^2 -  \mu_X\sigma_{XY}}{\sigma_X^2} \\
       &= \mu_Y - \mu_X\frac{\sigma_{XY}}{\sigma_X^2}\\
       and\,\, then \\
\beta &= \frac{\det{\mathrm{C_{\beta}}}}{\det{\mathrm{C}}} \\
  &= \frac{c_{11}d_2-c_{21}d_1}{c_{11}c_{22}-c_{12}c_{21}} \\
  &= \frac{\sigma_{XY} + \mu_X\mu_Y - \mu_X\mu_Y}{\sigma_X^2 + \mu_X^2 - \mu_x^2} \\
  &= \frac{\sigma_{XY}}{\sigma_X^2}
\end{align}

Yeow! All that work to get at this simplification all due to the wonderful result that \(\alpha\) has \(\beta = \sigma_{XY}/\sigma_X^2\) in it.

\begin{align}
\operatorname{E}(Y \mid X) &= \alpha + \beta\,X \\
\operatorname{E}(Y \mid X) &= \left(\mu_Y - \mu_X\frac{\sigma_{XY}}{\sigma_X^2}\right) + \frac{\sigma_{XY}}{\sigma_X^2}\,X \\
rearranging\,\,terms\\
\operatorname{E}(Y \mid X) &= \mu_Y + \frac{\sigma_{XY}}{\sigma_X^2}(X - \mu_X)
\end{align}

The second formulation is also the basis for the vaunted Capital Asset Pricing Model in finance, where \(Y\) is the return on a security (stock, bond, etc.) and \(X\) is the return on a market index (e.g., S\&P 500).

We have, yes, one more stop, before we drop. The definition of correlation is here.

\[
\rho = \frac{\sigma_{XY}}{\sigma_X\,\sigma_Y}
\]

We can use this definition to rearrange the deck chairs on this Titanic of a beast of gnarly maths (all algebra! and nary a faint odor of calculus?).

\begin{align}
if \\
\rho &= \frac{\sigma_{XY}}{\sigma_X\,\sigma_Y} \\
then \\
\sigma_{XY} &= \rho\,\sigma_X\,\sigma_Y\\
thus \\
\beta &= \frac{\sigma_{XY}}{\sigma_X^2} \\
      &= \frac{\rho\,\sigma_X\,\sigma_Y}{\sigma_X^2} \\
      &= \frac{\rho\,\sigma_Y}{\sigma_X}
\end{align}

We need numbers, \emph{stat(im)}! But we should hold on. One more calculation to make. After we have the mean, but what about the conditional standard deviation?

\hypertarget{a-short-variance-diversion}{%
\section{A short variance diversion}\label{a-short-variance-diversion}}

Here we take the standard deviation as given, perhaps at our peril. The variance of waiting times is

\[
Var(Y \mid X) = (1-\rho^2)\sigma_Y^2
\]

How do we get this? A lot easier than the preceding. There are no simultaneous equations to worry about. Here's the algebra for the stout-hearted.

\begin{align}
Var(Y \mid X) &= \operatorname{E}[Y - \operatorname{E}(Y \mid X)]^2 \\
              &= \operatorname{E}[Y - (\mu_Y + \frac{\sigma_{XY}}{\sigma_X^2}(X - \mu_X))]^2 \\
              &= \operatorname{E}[(Y- \mu_Y) + \frac{\sigma_{XY}}{\sigma_X^2}(X - \mu_X)]^2 \\
              &= \operatorname{E}[(Y- \mu_Y)^2 +         \frac{\rho_{XY}^2\sigma_Y^2}{\sigma_X^2}(X - \mu_X)^2 - 2\frac{\rho_{XY}\sigma_Y}{\sigma_X}(Y- \mu_Y)(X - \mu_X)] \\
              &= \operatorname{E}[(Y- \mu_Y)^2] +         \frac{\rho_{XY}^2\sigma_Y^2}{\sigma_X^2}\operatorname{E}[(X - \mu_X)^2] - 2\frac{\rho_{XY}\sigma_Y}{\sigma_X}\operatorname{E}[(Y- \mu_Y)(X - \mu_X)] \\
              &= \sigma_Y^2 + \frac{\rho_{XY}^2\sigma_Y^2}{\sigma_X^2} \sigma_X^2 - 2\frac{\rho_{XY}\sigma_Y}{\sigma_X}(\rho_{XY}\sigma_X\,\sigma_Y) \\
              &= \sigma_Y^2 - \rho_{XY}^2\sigma_Y^2 \\
              &= (1 - \rho_{XY}^2)\sigma_Y^2
\end{align}

Done! Yes, really.

If the joint distribution of \(X\) and \(Y\) is Gaussian, then we can generate \(Y \mid X \sim \operatorname{N}( \alpha + \beta X, (1 - \rho_{XY}^2)\sigma_Y^2)\). Now we can infer \(Y\) behavior.

\hypertarget{quaestio-whether-corporate-risk-disclosures-exhibit-logical-coherence}{%
\chapter{Quaestio: Whether Corporate Risk Disclosures Exhibit Logical Coherence}\label{quaestio-whether-corporate-risk-disclosures-exhibit-logical-coherence}}

This quaestio integrates:

\begin{itemize}
\tightlist
\item
  formal logic,\\
\item
  managerial ethics,\\
\item
  and the metaphysics of intelligibility
\end{itemize}

\hypertarget{objections-5}{%
\section{Objections}\label{objections-5}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  It seems that corporate MD\&A and Risk Factor language is inherently inconsistent, since firms routinely assert both vulnerability and resilience, both risk and opportunity, both exposure and mitigation. Therefore, no coherent logical structure can be extracted.
\item
  Further, the conditional forms used in disclosures (``only if,'' ``may,'' ``could,'' ``unless'') are too vague to admit propositional Formalize. Their ambiguity defeats the precision required for deductive analysis.
\item
  Moreover, the complexity of modern supply chains (e.g., NVIDIA's dependence on TSMC for fabrication and packaging) introduces causal entanglement, not discrete propositions. Thus, propositional logic is too coarse to capture the real structure of managerial claims.
\end{enumerate}

\hypertarget{on-the-contrary-5}{%
\section{On the contrary}\label{on-the-contrary-5}}

The \emph{Rules} section of this chapter teaches that even complex statements can be decomposed into binary propositions and tested for consistency using the algebra of binary.

And the Apostle says: \emph{``Let your yes be yes and your no be no.''}\\
Therefore, intelligibility is not only possible but required.

\hypertarget{i-respond-that-5}{%
\section{I respond that}\label{i-respond-that-5}}

Corporate disclosures do exhibit a latent logical structure, even when the prose is hedged, qualified, or strategically ambiguous.

Three insights guide the analysis:

\hypertarget{emergence-of-structure-beneath-ambiguity}{%
\subsection{1. Emergence of structure beneath ambiguity}\label{emergence-of-structure-beneath-ambiguity}}

Risk language often appears fuzzy, but its \emph{functional role} in MD\&A is to articulate conditional dependencies:
- If X occurs, Y may follow.\\
- Only if A is achieved, B will result.\\
- Unless C is mitigated, D could arise.

These are precisely the forms your binary conditional \(1 + x + xy\) is designed to encode.

\hypertarget{ethical-intelligibility}{%
\subsection{2. Ethical intelligibility}\label{ethical-intelligibility}}

Managers owe stakeholders clarity about:
- what conditions threaten liquidity,\\
- what dependencies constrain operations,\\
- what assumptions underlie forecasts.

Logical consistency is not merely a technical property; it is a moral one.\\
Incoherent disclosures obscure responsibility.

\hypertarget{pedagogical-emergence}{%
\subsection{3. Pedagogical emergence}\label{pedagogical-emergence}}

By beginning with synthetic passages, we learn to:
- identify propositions,\\
- translate English into logical form,\\
- encode implications in binary,\\
- test for consistency.

Then, by moving to actual MD\&Aâ€‘style passages, we encounter the complexity of actual managerial communication.

\hypertarget{application-synthetic-passages-option-a-content}{%
\section{Application: Synthetic Passages (Option A Content)}\label{application-synthetic-passages-option-a-content}}

Below are three synthetic but realistic NVIDIAâ€‘style passages, each followed by a propositional structure suitable for binary analysis.

\hypertarget{passage-1-supplychain-concentration}{%
\subsection{Passage 1 --- Supplyâ€‘Chain Concentration}\label{passage-1-supplychain-concentration}}

\begin{quote}
``We depend on a limited number of suppliers for wafer fabrication and advanced packaging. If these suppliers cannot meet our demand, our ability to deliver products will be adversely affected. Our suppliers will meet fabrication demand. Our suppliers may not meet packaging demand.''
\end{quote}

Let:
- \(F\): fabrication demand is met\\
- \(P\): packaging demand is met\\
- \(D\): we deliver products on schedule

Formalize:
- \(\lnot F \oplus \lnot P \rightarrow \lnot D\)\\
- \(F\)\\
- \(\lnot P\)

we test whether \(D\) must be false.

\hypertarget{passage-2-geopolitical-exposure}{%
\subsection{Passage 2 --- Geopolitical Exposure}\label{passage-2-geopolitical-exposure}}

\begin{quote}
``A disruption in Taiwan could materially affect our supply chain. Taiwan will not experience a major disruption. Our supply chain will be materially affected.''
\end{quote}

Let:
- \(T\): Taiwan disruption\\
- \(S\): supply chain materially affected

Formalize:
- \(T \rightarrow S\)\\
- \(\lnot T\)\\
- \(S\)

we test whether the set is consistent.

\hypertarget{passage-3-customer-concentration}{%
\subsection{Passage 3 --- Customer Concentration}\label{passage-3-customer-concentration}}

\begin{quote}
``A small number of customers account for a substantial portion of our revenue. If one of these customers reduces orders, our financial results may be adversely affected. No major customer will reduce orders. Our financial results will be adversely affected.''
\end{quote}

Let:
- \(C\): major customer reduces orders\\
- \(R\): financial results adversely affected

Formalize:
- \(C \rightarrow R\)\\
- \(\lnot C\)\\
- \(R\)

We test whether the set is consistent.

\hypertarget{replies-to-the-objections}{%
\section{Replies to the Objections}\label{replies-to-the-objections}}

\hypertarget{reply-to-objection-1}{%
\subsection{Reply to Objection 1}\label{reply-to-objection-1}}

While managerial language is often hedged, its underlying structure is conditional.\\
Once decomposed, many passages are perfectly consistent---or reveal subtle contradictions.

\hypertarget{reply-to-objection-2}{%
\subsection{Reply to Objection 2}\label{reply-to-objection-2}}

Ambiguity does not preclude formalization. The binary algebra handles ``only if,'' ``unless,'' and ``may'' through standard encodings in a principled framework.

\hypertarget{reply-to-objection-3}{%
\subsection{Reply to Objection 3}\label{reply-to-objection-3}}

Even complex supply chains (e.g., NVIDIA â†’ TSMC â†’ Foxconn â†’ Supermicro) can be represented as binary dependencies. Propositional logic does not capture all causal nuance, but it captures the core logical commitments managers make.

\hypertarget{buildinig-a-logicks-technology}{%
\section{Buildinig a logicks technology}\label{buildinig-a-logicks-technology}}

Below we'll treat each passage's propositions as variables in \(\{0,1\}\), with:

\begin{itemize}
\tightlist
\item
  Negation:\\
  \[
  \lnot x = 1 + x
  \]
\item
  Conjunction:\\
  \[
  x \land y = xy
  \]
\item
  Disjunction (exclusive ``or''):\\
  \[
  x \oplus y = x + y + xy
  \]
\item
  Implication \(x \rightarrow y\): encoded as the truth-value polynomial\\
  \[
  x \rightarrow y \equiv 1 + x + xy
  \]\\
  (This equals \(1\) in all truth-table rows except \(x=1, y=0\), where it equals \(0\).)
\end{itemize}

We clarify our approach with rules learnt from basic arithmetic. Addition is defined as addition of integers, but restricted to modulo 2, with only two possible values \(\{0,1\}\).\footnote{\citet{burris2021SEP-algebra-logic-tradition} develops a sweeping history of the progress, decline, and revitalization of algebraic logic. \citet{burris2013horn} augment Boole's algebraic logic with a Rule of 0 and 1 founded on Horn sentences. This is significant because of the hermeneutical anti-compositional principle of building up an understanding of reasoned arguments from its propositional elements and in turn from terms. We take into account the argument as a whole and the reciprocal interaction of the whole argument with its components.} These values are the terms and the propositions themselves, not to mention the possible outcomes of sequences of propositions we would call an argument. If we remember our basic maths and modulo arithmetic as a clock, the hands on this clock can sit at 0 or 1. If the hand is at 1 and we add 1 to this setting we end up at, yes, 0. A regular OR in many languages would often mean and/or, an inclusive OR, an either one or the other or both. The XOR excludes any possibility in common between two propositions. Propositions are mutually exclusive of one another. Here is a table to help us.

\[
\begin{array}{c | c c}
+ (XOR)   &  0    &  1 \\ \hline
0         &  0    &  1 \\
1         &  1    &  0 
\end{array}
\]

Addition with boolean values \(\{0,1\}\) is the same as that of the logical exclusive OR operations, XOR. Since each element equals its opposite, subtraction is thus the same operation as addition, so that \(0-0=0\), \(0-1=1\) (remembering the clock with just two settings!), \(1-0=1\), and \(1-1=0\), anyway, all equivalent to the \(+\) rules. In words, it is \textbf{one or the other, not both}.

The multiplication is multiplication modulo 2 (see the table below), and on boolean variables corresponds to the logical AND operation. Here is the \textbf{both} proposition with two terms.

\[
\begin{array}{c | c c}
\times (AND)   &  0    &  1 \\ \hline
0         &  0    &  0 \\
1         &  0    &  1 
\end{array}
\]

The NOT logic simply takes a 0 and flips it to 1, and if 1 flips it to zero.

\[
\begin{array}{c | c}
\neg (NOT)   &  1+x   \\ \hline
0            &  1      \\
1            &  0      
\end{array}
\]

With these three operations we can evaluate any logical statement. And we must remember that NOT is not the subtraction operation we are very familiar with from non-binary algebra. Some folks will write \(\overline x\) instead of \(\neg\).

Here is a table of several useful expressions based on AND, XOR, and NOT. For any two propositions \(x\) and \(y\) whose values can only take on a 0 or 1, so that when \(x=0\), we mean \(x\) is false, otherwise (that is, \$x=1) true.\footnote{Boole has the negation of x as 1-x. We follow \citet{carnielli2015method} 1+x, given that all subtractions end up being addition in mod 2 arithmetic.}

\[
\begin{array}{c c | c c c c c}
  &   & AND & XOR & NOT & y \mid x & SUB \\ \hline
x & y & xy & x+y & 1+x & 1+x+xy    & x-y=x+y\\ \hline
0 & 0 & 0 & 0 & 1 & 1 & 0 \\
0 & 1 & 0 & 1 & 1 & 1 & 1 \\
1 & 0 & 0 & 1 & 0 & 0 & 1 \\
1 & 1 & 1 & 0 & 0 & 1 & 0\\ \hline
x & y & x \land y & x \oplus y & \neg x,\,\overline x & x \rightarrow y \\ \hline
x & y &     -      & (x \lor y) \land \neg (x \land y) & \neg x \lor y & \neg (x \oplus (x \land y)) \\
\hline
\end{array}
\]

The last row describes the algebraic operations in term of propositional logic as applied to Boolean values of \(\{0, 1\}\). I can easily get lost in that row. I tend to prefer 9th grade algebraic expressions.

The last column uses the negative to SUBtract \(y\) from \(x\). We might be tempted to say that \(x-y= x+ (1+y) = 1+x+y=1+(x+y)\) so that the subtraction operator is ``NOT the ADD'' operator. Nice try! We must always go back to the most literal, most primitive operations, namely the addition and multiplication rules which govern these more derived expressions.

That next to the last column will deserves some important attention since it forms the foundation of conditional (reasonably expected) probability. But first some very helpful derived relationships will make our work going forward a lot smoother.

\[
\begin{array}{c c | c c c c c}
x & x & xx=x^2 & xxx=x^3 & x+x=2x & x+x+x=3x & x-x=x+x\\ \hline
0 & 0 & 0      & 0       & 0      & 0        & 0      \\
0 & 0 & 0      & 0       & 0      & 0        & 0      \\
1 & 1 & 1      & 1       & 0      & 1        & 0      \\
1 & 1 & 1      & 1       & 0      & 1        & 0      \\ \hline
  & = & x      & x       & 0      & x        & 0      \\ \hline
\end{array}
\]

What happens here is that there are no monomial terms of any higher degree than 1; no quadratic or cubic terms at all. Two propositions literally cancel each other, as the Germans might say an \emph{Aufhebung} event. But three return the single proposition. All of this is the result of the modulo 2 arithmetic to which we constrain ourselves.

We now study \(y|x=1+x+xy\) in three moves. The first move is to realize that, at least for binary data, Aristotle discovered four logical forms, two in the affirmative, two negative; two universal, two contingent. Medieval logicians called the two affirmative Forms A and I for the first two vowels in \emph{AffIrmo}, Latin for ``I affirm,'' and the two negative forms E and O from the Latin \emph{nEgO}, ``I deny.'' Together they form the \textbf{Square of Opposition}. Here S is the subject and P the predicate. Any subject S signifies what it is we are talking about, say, rain. Any predicate P signifies what the subject is about, say, falling to the ground.

Equations and identities do not have a subject or a predicate and themselves might be the subject or predicate. But all propositions do have a subject and a predicate, just like in 3rd grade when we learned to write and speak in complete sentences, that is, in propositions, which contain a subject (usually a noun) and a predicate (usually a verb). We assume that when we apply the forms to concrete examples of propositions, the content of the form, that is, the S and the P, exist. For Thomas Aquinas signs are physical manifestations that allow us to understand something beyond their immediate appearance, like a footprint manifesting someone's presence or smoke manifesting fire. This something with an immediate appearance we will assume without further bother, that it somehow exists. Perhaps we append the particle \emph{any} to S and P to get any rain and any falling (of rain).

We build a table of logical forms in this first move. In the table, \emph{decisions} is the subject S, what we are talking about, and \emph{are rational} is the predicate P, what the subject is about.

\[
\begin{array}{c | c | c | c }
Form  & Proposition & Sentence & Algebra \\
\hline
A     & \text{All S is P}        & \text{"All decisions are rational."}         & a(1+b) = 0      \\ 
E     & \text{No S is P}         & \text{"Not all decisions are rational."}     & ab = 0          \\
I     & \text{Some S is P}       & \text{"Some decisions are rational."}        & ab \neq 0       \\
O     & \text{Some S is not P}   & \text{"Some decisions are not rational."}    & a(1+b) \neq 0   \\
\hline
\end{array}
\]

The second move is to parse the Form A proposition ``All decisions (a) are rational (b).'' Logically \(a\) and not \(b\) is false (0) in Form A, that is, algebraicly, \(a(1+b) = 0\). ``Decisions'' and ``not-rational'' is false, that is, inconsistent according to Form A. The obverse must be true, that ``No-decisions are not-rational,'' as if we might be able to interpret this double negative. One more swipe at interpretation is called for. To say that ``all of anything is something else'' is, effectively to identify ``anything'' with ``something else.'' If this statement is true, as we are positing here in the form, then it cannot be true that both ``anything = \(a\)'' and ``not-something else = \((1+b)\)'' can coexist. Thus the logical Form A seems to admit a very basic principle we would hold alongside all other principles (Aristotle called this \emph{metaphysics}), namely, the \emph{Principle of Non-Contradiction}. Yes, we cannot, so far it seems, to be in two places at the same time.

In a third move, we use the NOT operation on Form A. We recall we are using ``+'' as exclusive OR, XOR with this algebraic rearrangement.

\[
\begin{array}{c| c l}
statement & reason \\ 
\hline
1 & a(1+b) = 0     & \text{Form A definition}  \\
2 & 1 + a(1+b) = 1 & \text{Symmetric property} \\
3 & 1 + a + ab = 1 & \text{Distribution of multiplication over addition property} \\
\hline
4 & a \rightarrow b = b \mid a & \text{a gives some information to b} \\ 
\hline
\end{array}
\]

Simply negating (using 1+) the Form A definition reveals a conditional relationship between \(a\) and \(b\). Saying that \(a\) AND \(not-b\) is possible when we negate the Form A requirement, means \(a\) does share something with \(b\). So-called ``implication'' means that a shares a's information with b. This is the primary meaning we ascribe to ``a conditions b'', \(b \mid a\). We now have all the ingredients to use this framework to discover a principled path to the expectations approach to probability, that is, counting the ways in which data are consistent with hypotheses. But before we enter that particular room, we need to argue a bit more, that is, discover valid and invalid logical arguments.

Each sentence in the passage becomes a polynomial equation of the form ``expression \(= 1\)'' (true) or ``variable \(= 0/1\)''.

\hypertarget{passage-1-supplychain-concentration-1}{%
\subsubsection{Passage 1: Supplyâ€‘chain concentration}\label{passage-1-supplychain-concentration-1}}

\begin{quote}
``We depend on a limited number of suppliers for wafer fabrication and advanced packaging. If these suppliers cannot meet our demand, our ability to deliver products will be adversely affected. Our suppliers will meet fabrication demand. Our suppliers may not meet packaging demand.''
\end{quote}

Let:

\begin{itemize}
\tightlist
\item
  \(F\): fabrication demand is met.
\item
  \(P\): packaging demand is met.
\item
  \(D\): we deliver products on schedule.
\end{itemize}

We formalized:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\lnot F \oplus \lnot P \rightarrow \lnot D\)\\
\item
  \(F\)\\
\item
  \(\lnot P\)
\end{enumerate}

Step 1: Encode each statement.

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{(\arabic{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    \(F\) is true:\\
    \[
    F = 1
    \]
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{(\arabic{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    \(\lnot P\) is true:\\
    \[
    \lnot P = 1 + P = 1 \quad \Rightarrow \quad P = 0
    \]
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{(\arabic{enumi})}
  \tightlist
  \item
    \(\lnot F \oplus \lnot P \rightarrow \lnot D\):
  \end{enumerate}

  First compute \(\lnot F\) and \(\lnot P\):

  \[
  \lnot F = 1 + F,\quad \lnot P = 1 + P
  \]

  Disjunction:

  \[
  \lnot F \oplus \lnot P = (1+F) \oplus (1+P)
  \]
  \[
  = (1+F) + (1+P) + (1+F)(1+P)
  \]
  Simplify:

  \[
  (1+F) + (1+P) = F + P
  \]
  \[
  (1+F)(1+P) = 1 + F + P + FP
  \]
  So

  \[
  \lnot F \oplus \lnot P = F + P + 1 + F + P + FP = 1 + FP
  \]

  So the antecedent is \(1 + FP\). The consequent is \(\lnot D = 1 + D\).

  The implication polynomial is:

  \[
  (1 + FP) \rightarrow (1 + D) = 1 + (1 + FP) + (1 + FP)(1 + D)
  \]

  Compute:

  \[
  1 + (1+FP) = FP
  \]
  \[
  (1+FP)(1+D) = 1 + D + FP + FPD
  \]

  So:

  \[
  1 + (1 + FP) + (1+FP)(1+D) = FP + 1 + D + FP + FPD = 1 + D + FPD
  \]

  Therefore (1) says:

  \[
  1 + D + FPD = 1 \quad \Rightarrow \quad D + FPD = 0
  \]
\end{itemize}

Step 2: Substitute known values.

We already have \(F = 1, P = 0\). Then:

\[
FPD = (1)(0)D = 0
\]

So the equation \(D + FPD = 0\) becomes:

\[
D + 0 = 0 \quad \Rightarrow \quad D = 0
\]

\textbf{Conclusion}

A satisfying assignment is:

\[
F = 1,\quad P = 0,\quad D = 0
\]

All three statements can be true simultaneously. The set is logically consistent, and the algebra forces the natural managerial reading: if packaging capacity fails, onâ€‘time delivery fails.

\hypertarget{passage-2-geopolitical-exposure-1}{%
\subsubsection{Passage 2: Geopolitical exposure}\label{passage-2-geopolitical-exposure-1}}

\begin{quote}
``A disruption in Taiwan could materially affect our supply chain. Taiwan will not experience a major disruption. Our supply chain will be materially affected.''
\end{quote}

Let:

\begin{itemize}
\tightlist
\item
  \(T\): Taiwan disruption.
\item
  \(S\): supply chain materially affected.
\end{itemize}

Formalize:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(T \rightarrow S\)\\
\item
  \(\lnot T\)\\
\item
  \(S\)
\end{enumerate}

Step 1: Encode each statement.

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{(\arabic{enumi})}
  \tightlist
  \item
    \(T \rightarrow S\):
  \end{enumerate}

  \[
  T \rightarrow S = 1 + T + TS = 1
  \]
  So:
  \[
  1 + T + TS = 1 \quad \Rightarrow \quad T + TS = 0
  \]
\item
  \begin{enumerate}
  \def\labelenumi{(\arabic{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    \(\lnot T\):
  \end{enumerate}

  \[
  1 + T = 1 \quad \Rightarrow \quad T = 0
  \]
\item
  \begin{enumerate}
  \def\labelenumi{(\arabic{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    \(S\):
  \end{enumerate}

  \[
  S = 1
  \]
\end{itemize}

Step 2: Substitute into (1).

With \(T = 0\):

\[
T + TS = 0 + 0\cdot S = 0
\]

So equation (1) becomes \(0 = 0\), automatically satisfied for any \(S\). We also impose \(S = 1\) from (3).

\textbf{Conclusion}

A satisfying assignment is:

\[
T = 0,\quad S = 1
\]

All three statements are jointly consistent. The algebra highlights a conceptual point for we: supplyâ€‘chain stress may have causes other than the Taiwan scenario flagged in (1).

\hypertarget{passage-3-customer-concentration-1}{%
\subsubsection{Passage 3: Customer concentration}\label{passage-3-customer-concentration-1}}

\begin{quote}
``A small number of customers account for a substantial portion of our revenue. If one of these customers reduces orders, our financial results may be adversely affected. No major customer will reduce orders. Our financial results will be adversely affected.''
\end{quote}

Let:

\begin{itemize}
\tightlist
\item
  \(C\): a major customer reduces orders.
\item
  \(R\): financial results adversely affected.
\end{itemize}

Formalize:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(C \rightarrow R\)\\
\item
  \(\lnot C\)\\
\item
  \(R\)
\end{enumerate}

Step 1: Encode each statement.

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{(\arabic{enumi})}
  \tightlist
  \item
    \(C \rightarrow R\):
  \end{enumerate}

  \[
  C \rightarrow R = 1 + C + CR = 1
  \]
  So:
  \[
  1 + C + CR = 1 \quad \Rightarrow \quad C + CR = 0
  \]
\item
  \begin{enumerate}
  \def\labelenumi{(\arabic{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    \(\lnot C\):
  \end{enumerate}

  \[
  1 + C = 1 \quad \Rightarrow \quad C = 0
  \]
\item
  \begin{enumerate}
  \def\labelenumi{(\arabic{enumi})}
  \setcounter{enumi}{2}
  \tightlist
  \item
    \(R\):
  \end{enumerate}

  \[
  R = 1
  \]
\end{itemize}

Step 2: Substitute into (1).

With \(C = 0\):

\[
C + CR = 0 + 0\cdot R = 0
\]

Equation (1) becomes \(0 = 0\), automatically satisfied for any \(R\). Together with \(R = 1\), we get the assignment:

\[
C = 0,\quad R = 1
\]

\textbf{Conclusion}

The set is consistent. Again, the algebra cleanly registers an interpretive point: adverse financial results can occur even without the particular customerâ€‘concentration scenario highlighted in (1).

\hypertarget{note-on-the-analytical-requirements-for-ethical-disclosure}{%
\subsection{Note on the analytical requirements for ethical disclosure}\label{note-on-the-analytical-requirements-for-ethical-disclosure}}

When we translate MD\&A and riskâ€‘factor prose into precise propositions, and then further into \$ \mathbb{F}\_2 \$ equations, they are not merely doing a technical exercise. They are learning what it means to speak truthfully under conditions of complexity.

A few points we might consider as we reflect on the work of our hands, brains together:

\begin{itemize}
\item
  \emph{Clarity as respect for the neighbor.} Investors, employees, suppliers, and communities all stand downstream of these disclosures. To speak in ways that are internally contradictory, or so vague as to defeat intelligibility, is to treat those neighbors as means rather than ends. Logical consistency is not sufficient for ethical communication, but it is necessary.
\item
  \emph{The humility of stating conditions.} A manager who says, ``If \(C\), then \(R\)'' is admitting contingency, dependence, vulnerability. Your algebra makes those dependencies concrete. To acknowledge them truthfully is a form of humility: our flourishing is not selfâ€‘generated but conditioned by others (TSMC, customers, regulators, macro conditions).
\item
  \emph{The danger of strategic obscurity.} Some disclosures are engineered to sound cautious while saying almost nothing determinate. When a student cannot even \emph{formulate} a proposition or a polynomial from a sentence, that is already an ethical signal: the language has been constructed to evade accountability. The discipline of logic becomes a training in discernment: where is the ``yes'' and where is the ``no''?
\item
  \emph{The vocation of the analyst.} An analyst on your project is not merely someone who manipulates symbols. She is someone called to seek intelligibility in the world: to surface the real dependencies, the actual risks, the genuine exposures, and to resist both cynicism (``it's all spin'') and naivetÃ© (``they must mean exactly what they say''). The algebra is a tool for that vocation.
\item
  \emph{Truthfulness as a shared project.} Finally, these exercises model something communal. We work together to check each other's translations, test each other's systems for consistency, notice where an assumption has been smuggled in. This mirrors what ought to happen in an organization: \emph{crossâ€‘functional dialogue that insists disclosures be both accurate and intelligible.}
\end{itemize}

We can close the section with a simple exhortation:

\begin{quote}
When you put a statement into symbols and test it, you are practicing a habit that matters far beyond this course. You are learning to ask of any claim---corporate, political, even personal---``Is this coherent? What else must be true if this is true?'' That habit, exercised in love of the truth and love of the neighbor, is part of what makes disclosure ethical rather than merely compliant.
\end{quote}

\hypertarget{paraphrased-nvidiastyle-risk-subsection}{%
\section{1. Paraphrased NVIDIAâ€‘style risk subsection}\label{paraphrased-nvidiastyle-risk-subsection}}

This is synthetic but grounded in NVIDIA's real risk profile: extreme dependence on TSMC in Taiwan, geopolitical exposure, and supplyâ€‘chain concentration.

\begin{quote}
Risk: Dependence on a single advanced foundry in a geopolitically exposed region

We rely on a limited number of thirdâ€‘party foundries to manufacture and package our most advanced GPUs. In particular, we depend on a single leadingâ€‘edge foundry located primarily in Taiwan to fabricate and provide advanced packaging for substantially all of our highestâ€‘margin data center products. If this foundry, or its facilities in Taiwan, become unable to operate at planned capacity due to geopolitical tensions, natural disasters, export controls, or other disruptions, our supply of advanced GPUs could be materially reduced or delayed. Any such disruption could materially and adversely affect our ability to fulfill customer demand, our revenue growth, and our operating results.\\
At the same time, we are investing in supplyâ€‘chain resiliency initiatives, including engaging additional foundry partners and diversifying advanced packaging capacity in other regions. These initiatives are designed to mitigate, but cannot fully eliminate, the risks associated with our current geographic and supplier concentration.
\end{quote}

Goal: Is this logically coherent? What must be true, if the company stands by these claims?

\hypertarget{propositional-decomposition}{%
\section{2. Propositional decomposition}\label{propositional-decomposition}}

Define propositional variables:

\begin{itemize}
\tightlist
\item
  \(F\): The single leadingâ€‘edge foundry in Taiwan operates at planned capacity.\\
\item
  \(S\): Our supply of advanced GPUs is materially sufficient (neither materially reduced nor delayed).\\
\item
  \(R\): Our ability to fulfill customer demand, revenue growth, and operating results are not materially and adversely affected.\\
\item
  \(I\): Our resiliency initiatives (additional foundries, diversified packaging) are effective in materially mitigating the supplyâ€‘chain risk.
\end{itemize}

We also need some ``disruption'' variable:

\begin{itemize}
\tightlist
\item
  \(D\): The foundry or its Taiwanese facilities are unable to operate at planned capacity due to geopolitical or other disruptions.
\end{itemize}

From the text:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  If the foundry in Taiwan becomes unable to operate at planned capacity, then our supply of advanced GPUs could be materially reduced or delayed.\\
  \[
  D \rightarrow \lnot S
  \]
\item
  If our supply of advanced GPUs is materially reduced or delayed, then our ability to fulfill demand, revenue growth, and operating results are materially and adversely affected.\\
  \[
  \lnot S \rightarrow \lnot R
  \]
\item
  We depend on this single foundry in Taiwan for substantially all of our highestâ€‘margin data center products. (Descriptive---bakes in the importance of \(F\); no direct conditional yet.)
\item
  Our resiliency initiatives are designed to mitigate, but not fully eliminate, the risks associated with geographic and supplier concentration.\\
  Intuitively:

  \begin{itemize}
  \tightlist
  \item
    If \(I\) is effective, then some disruptions that would otherwise have caused \(\lnot S\) might not do so.\\
  \item
    But ``not fully eliminate'' means: even if \(I\) is effective, there exist possible disruptions for which \(D\) still leads to \(\lnot S\).\\
    We can encode this in a minimal way:
  \item
    \(I \rightarrow M\), where \(M\) = ``some risk is mitigated.'' (We'll treat this as background, not central to the consistency test.)
  \item
    And crucially we \emph{do not} assert \(I \rightarrow \lnot(D \rightarrow \lnot S)\) or its negation; the text is careful.
  \end{itemize}
\item
  The company does not actually assert any of:

  \begin{itemize}
  \tightlist
  \item
    ``\(D\) will occur,''\\
  \item
    ``\(D\) will not occur,''\\
  \item
    ``\(S\),''\\
  \item
    ``\(\lnot S\),''\\
  \item
    ``\(R\),''\\
  \item
    ``\(\lnot R\).''\\
    It speaks in conditional and designed to mitigate language.
  \end{itemize}
\end{enumerate}

For the algebra, focus on the core conditionals:

\begin{itemize}
\item
  \begin{enumerate}
  \def\labelenumi{(\Alph{enumi})}
  \tightlist
  \item
    \(D \rightarrow \lnot S\)\\
  \end{enumerate}
\item
  \begin{enumerate}
  \def\labelenumi{(\Alph{enumi})}
  \setcounter{enumi}{1}
  \tightlist
  \item
    \(\lnot S \rightarrow \lnot R\)
  \end{enumerate}
\end{itemize}

Everything else is context.

\hypertarget{encoding-in-_2}{%
\section{\texorpdfstring{3. Encoding in \$ \mathbb{F}\_2 \$}{3. Encoding in \$ \_2 \$}}\label{encoding-in-_2}}

Let each proposition be in \(\{0,1\}\), with:

\begin{itemize}
\tightlist
\item
  \(\lnot x = 1 + x\)\\
\item
  \(x \land y = xy\)\\
\item
  Implication \(x \rightarrow y = 1 + x + xy\)
\end{itemize}

\hypertarget{statement-a-d-rightarrow-lnot-s}{%
\subsection{\texorpdfstring{Statement (A): \(D \rightarrow \lnot S\)}{Statement (A): D \textbackslash rightarrow \textbackslash lnot S}}\label{statement-a-d-rightarrow-lnot-s}}

\[
D \rightarrow \lnot S = 1 + D + D(1 + S) = 1 + D + D + DS = 1 + DS
\]

Truth of (A) means:

\[
1 + DS = 1 \quad \Rightarrow \quad DS = 0
\]

So (A) imposes the constraint:

\[
DS = 0
\]

\hypertarget{statement-b-lnot-s-rightarrow-lnot-r}{%
\subsection{\texorpdfstring{Statement (B): \(\lnot S \rightarrow \lnot R\)}{Statement (B): \textbackslash lnot S \textbackslash rightarrow \textbackslash lnot R}}\label{statement-b-lnot-s-rightarrow-lnot-r}}

First compute:

\[
\lnot S = 1 + S,\quad \lnot R = 1 + R
\]

So:

\[
\lnot S \rightarrow \lnot R = 1 + (1+S) + (1+S)(1+R)
\]

Compute step by step:

\[
1 + (1+S) = S
\]
\[
(1+S)(1+R) = 1 + R + S + SR
\]

So:

\[
1 + (1+S) + (1+S)(1+R) = S + 1 + R + S + SR = 1 + R + SR
\]

Truth of (B) means:

\[
1 + R + SR = 1 \quad \Rightarrow \quad R + SR = 0
\]

So (B) imposes:

\[
R + SR = 0
\]

\hypertarget{consistency-analysis-in-_2}{%
\section{\texorpdfstring{4. Consistency analysis in \$ \mathbb{F}\_2 \$}{4. Consistency analysis in \$ \_2 \$}}\label{consistency-analysis-in-_2}}

Our system is:

\begin{itemize}
\tightlist
\item
  \(DS = 0\)\\
\item
  \(R + SR = 0\)
\end{itemize}

There are no direct equations fixing the actual values of \(D, S, R\); only relationships.

We check for existence of at least one satisfying assignment.

\hypertarget{case-analysis}{%
\subsection{Case analysis}\label{case-analysis}}

\hypertarget{case-1-s-0}{%
\subsubsection{\texorpdfstring{Case 1: \(S = 0\)}{Case 1: S = 0}}\label{case-1-s-0}}

Then:

\begin{itemize}
\tightlist
\item
  \(DS = D \cdot 0 = 0\) --- automatically satisfies \(DS = 0\).\\
\item
  For the second equation:\\
  \[
  R + SR = R + 0\cdot R = R
  \]
  So we need:
  \[
  R = 0
  \]
\end{itemize}

So with \(S = 0\), any \(D \in \{0,1\}\) works, provided \(R = 0\).\\
Thus we have satisfying assignments like:
- \(D = 0, S = 0, R = 0\)\\
- \(D = 1, S = 0, R = 0\)

Both reflect the intended reading: if supply is materially reduced/delayed (\(S=0\)), then results must be adversely affected (\(R=0\)).

\hypertarget{case-2-s-1}{%
\subsubsection{\texorpdfstring{Case 2: \(S = 1\)}{Case 2: S = 1}}\label{case-2-s-1}}

Then:

\begin{itemize}
\item
  \(DS = D\cdot 1 = D\), so \(DS = 0\) implies:
  \[
  D = 0
  \]
\item
  For the second equation:
  \[
  R + SR = R + 1\cdot R = R + R = 0
  \]
\end{itemize}

So this is automatically satisfied for any \(R\).\\
Thus we have satisfying assignments like:
- \(D = 0, S = 1, R = 0\)\\
- \(D = 0, S = 1, R = 1\)

Here, if the foundry is \emph{not} disrupted (\(D=0\)), (A) places no further constraint on \(S\); if supply is fine (\(S=1\)), (B) places no constraint on \(R\).

\hypertarget{verdict}{%
\subsection{Verdict}\label{verdict}}

There are many satisfying assignments. The system is logically consistent.

Moreover, the structure is \emph{exactly} what you would hope for ethically:

\begin{itemize}
\tightlist
\item
  If disruption occurs and propagates through supply, results must suffer.\\
\item
  If there is no disruption, or if supply holds, results may or may not suffer for other reasons---not specified here.
\end{itemize}

No contradiction; no hidden guarantees.

\hypertarget{interpretive-and-ethical-commentary}{%
\section{5. Interpretive and ethical commentary}\label{interpretive-and-ethical-commentary}}

\hypertarget{logical-interpretation}{%
\subsection{Logical interpretation}\label{logical-interpretation}}

we can now see:

\begin{itemize}
\tightlist
\item
  The subsection is purely conditional: ``If \(D\), then \(\lnot S\); if \(\lnot S\), then \(\lnot R\).''\\
\item
  NVIDIAâ€‘style language does not assert that \(D\) will occur; it does not promise that \(S\) will hold.\\
\item
  The role of resiliency initiatives (\(I\)) is explicitly modest: ``designed to mitigate, but cannot fully eliminate.'' That means the firm refuses to encode a global implication like \(I \rightarrow S\), which would be logically fragile.
\end{itemize}

Formally: the passage asserts necessary relations, not actual outcomes.

\hypertarget{ethical-note-on-analytical-requirements-for-disclosure}{%
\subsection{Ethical note on analytical requirements for disclosure}\label{ethical-note-on-analytical-requirements-for-disclosure}}

There is a pedagogically powerful and pastoral point here:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Ethical disclosure requires that conditionals be honest about propagation.\\
  If executives know that a disruption at TSMC (\(D\)) would devastate supply (\(\lnot S\)) and results (\(\lnot R\)), it would be ethically suspect to describe the risk in vague, nonâ€‘committal language. The chain \(D \rightarrow \lnot S \rightarrow \lnot R\) is a \emph{moral obligation} to articulate, not merely a technical dependency.
\item
  But honesty also requires restraint.\\
  They do not (and should not) promise: ``Our resiliency initiatives guarantee that disruptions won't matter.'' An honest risk section resists both false assurance and performative alarmism. Your logic tools help we see where a sentence would secretly encode a guarantee or a contradiction.
\item
  Analytical work is part of justice.\\
  To know that \(D \rightarrow \lnot S\) and \(\lnot S \rightarrow \lnot R\) and then not disclose it is a failure of justice toward counterparties and the broader public. The algebraic exercise is a rehearsal in noticing hidden dependencies and insisting that they be made explicit.
\item
  Formation in prudence.\\
  When we work through the system and see that the risk language is consistent but sobering, they are being trained not only to detect deception but to practice prudence:

  \begin{itemize}
  \tightlist
  \item
    What is not said?\\
  \item
    What variables are left open?\\
  \item
    Are there plausible disruptions the firm has not modeled?
  \end{itemize}
\item
  Truthfulness as shared discernment.\\
  Analysts, auditors, boards, and regulators are all, in principle, coâ€‘discerners of the truth about a firm's condition. Your exercises invite we into that vocation: not to weaponize cleverness, but to serve intelligibility and the good of neighbors through disciplined reasoning.
\end{enumerate}

\hypertarget{logic-exercises}{%
\chapter{Logic Exercises}\label{logic-exercises}}

Our ability to model events, decisions, policies depends on the framing of arguments, definition of terms, posing relationships among those terms, and at the very least determining the logical coherence of the argument, result, implication. We will use these methods to build a skein of hypotheses against which we will sample data, count the ways in which the data are consistent with hypotheses, and ultimately infer results.

\hypertarget{liquidity-and-credit-facilities}{%
\subsection{1. Liquidity and credit facilities}\label{liquidity-and-credit-facilities}}

\begin{quote}
``We have sufficient liquidity to meet all of our obligations over the next twelve months without drawing on our revolving credit facility. However, if market conditions deteriorate significantly, we will need to draw on the facility to meet those obligations. Current market conditions have not deteriorated significantly.''
\end{quote}

\textbf{Propositional encoding:}

Let:

\begin{itemize}
\tightlist
\item
  \(L\): ``We have sufficient liquidity to meet all obligations over the next 12 months.''
\item
  \(D\): ``We need to draw on the revolving credit facility to meet those obligations.''
\item
  \(M\): ``Market conditions deteriorate significantly.''
\end{itemize}

From the passage, we can reasonably formalize:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\lnot D \rightarrow L\)\\
  (If we do not draw on the facility, then we have sufficient liquidity.)
\item
  \(M \rightarrow D\)\\
  (If markets deteriorate significantly, we will need to draw.)
\item
  \(\lnot M\)\\
  (Current markets have not deteriorated significantly.)
\item
  \(\lnot D\)\\
  (We will not draw on the facility --- this is implied by ``without drawing on'' in the first sentence.)
\end{enumerate}

\textbf{Questions}

\begin{itemize}
\tightlist
\item
  \textbf{Consistency:} Are propositions \(1\)--\(4\) logically consistent?
\item
  \textbf{Entailment:} From \(1\) and \(4\), does it follow that \(L\) must be true?
\item
  \textbf{Counterfactual exploration:} What can be said about the truth value of \(D\) if instead we assumed \(M\) is true?
\end{itemize}

\hypertarget{profit-growth-cost-inflation-and-guidance}{%
\subsection{2. Profit growth, cost inflation, and guidance}\label{profit-growth-cost-inflation-and-guidance}}

\begin{quote}
``We will achieve double-digit profit growth this year only if we keep annual cost inflation below three percent. We will not keep annual cost inflation below three percent. Nevertheless, we expect to achieve double-digit profit growth this year.''
\end{quote}

Let:

\begin{itemize}
\tightlist
\item
  \(G\): ``We achieve double-digit profit growth this year.''
\item
  \(C\): ``We keep annual cost inflation below three percent.''
\end{itemize}

A natural formalization is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(G \rightarrow C\)\\
  (``Only if'' = \(G\) implies \(C\).)
\item
  \(\lnot C\)\\
  (We will not keep cost inflation below 3\%.)
\item
  \(G\)\\
  (We expect to achieve double-digit profit growth.)
\end{enumerate}

\textbf{Questions}

\begin{itemize}
\tightlist
\item
  \textbf{Consistency:} Are \(1\), \(2\), and \(3\) jointly consistent?
\item
  \textbf{Derivation:} Show that from \(1\) and \(3\) we can derive \(C\), and then combine with \(2\) to derive a contradiction.
\item
  \textbf{Interpretive question:} How does this illustrate tension between ``risk factor'' and ``guidance'' language in corporate disclosures?
\end{itemize}

\hypertarget{market-share-pricing-strategy-and-competition}{%
\subsection{3. Market share, pricing strategy, and competition}\label{market-share-pricing-strategy-and-competition}}

\begin{quote}
``If we increase prices in our core segment, we will lose market share unless our main competitor also increases its prices. Our main competitor will not increase its prices. We will increase prices in our core segment. We will not lose market share.''
\end{quote}

Let:

\begin{itemize}
\tightlist
\item
  \(P\): ``We increase prices in our core segment.''
\item
  \(S\): ``We lose market share.''
\item
  \(C\): ``Our main competitor increases its prices.''
\end{itemize}

The structure of ``A unless B'' is naturally treated as ``if A and not B then S''. So:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \((P \land \lnot C) \rightarrow S\)\\
\item
  \(\lnot C\)\\
\item
  \(P\)\\
\item
  \(\lnot S\)
\end{enumerate}

\textbf{Questions}

\begin{itemize}
\tightlist
\item
  \textbf{Consistency:} Are \(1\)--\(4\) jointly consistent?
\item
  \textbf{Derivation:} Show that from \(2\) and \(3\) we get \(P \land \lnot C\), then use \(1\) to infer \(S\), and finally clash with \(4\).
\item
  \textbf{Extension:} Rewrite the passage to make it logically consistent, then re-symbolize anda chech for consistency and derivation.
\end{itemize}

\hypertarget{liquidity-buffers-asset-sales-and-regulatory-requirements}{%
\subsection{4. Liquidity buffers, asset sales, and regulatory requirements}\label{liquidity-buffers-asset-sales-and-regulatory-requirements}}

\begin{quote}
``If our liquidity buffer falls below the regulatory minimum, we will either sell liquid assets at a discount or raise short-term funding in the market. We will not sell liquid assets at a discount. Our liquidity buffer will fall below the regulatory minimum. We will not raise short-term funding in the market.''
\end{quote}

Let:

\begin{itemize}
\tightlist
\item
  \(B\): ``Our liquidity buffer falls below the regulatory minimum.''
\item
  \(A\): ``We sell liquid assets at a discount.''
\item
  \(F\): ``We raise short-term funding in the market.''
\end{itemize}

``Either \ldots{} or \ldots{}'' in risk language is typically inclusive unless specified. So:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(B \rightarrow (A \lor F)\)\\
\item
  \(\lnot A\)\\
\item
  \(B\)\\
\item
  \(\lnot F\)
\end{enumerate}

\textbf{Questions}

\begin{itemize}
\tightlist
\item
  \textbf{Consistency:} Are these four propositions consistent?
\item
  \textbf{Derivation:} From \(1\) and \(3\), derive \(A \lor F\). Show why \(2\) and \(4\) together with this lead to a contradiction.
\item
  \textbf{Conceptual twist:} How would the logic change if the phrase ``either \ldots{} or \ldots{} but not both'' were explicitly used? We will formalize this language as \(B \rightarrow (A \oplus F)\) where \(\oplus\) is the exclusive or \(XOR\).
\end{itemize}

\hypertarget{going-concern-refinancing-and-macro-conditions}{%
\subsection{5. Going concern, refinancing, and macro conditions}\label{going-concern-refinancing-and-macro-conditions}}

\begin{quote}
``We will remain a going concern over the next twelve months if, and only if, we successfully refinance our upcoming debt maturities and macroeconomic conditions do not deteriorate materially. Macroeconomic conditions will deteriorate materially. We will successfully refinance our upcoming debt maturities. We will remain a going concern over the next twelve months.''
\end{quote}

Let:

\begin{itemize}
\tightlist
\item
  \(G\): ``We remain a going concern over the next 12 months.''
\item
  \(R\): ``We successfully refinance our upcoming debt maturities.''
\item
  \(M\): ``Macroeconomic conditions deteriorate materially.''
\end{itemize}

``If, and only if'' results in a a biconditional statement.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(G \leftrightarrow (R \land \lnot M)\)\\
\item
  \(M\)\\
\item
  \(R\)\\
\item
  \(G\)
\end{enumerate}

\textbf{Questions}

\begin{itemize}
\tightlist
\item
  \textbf{Decomposition:} Split \(1\) into two conditionals to model the biconditional statement.

  \begin{itemize}
  \tightlist
  \item
    \(G \rightarrow (R \land \lnot M)\)\\
  \item
    \((R \land \lnot M) \rightarrow G\)
  \end{itemize}
\item
  \textbf{Consistency:} Are \(1\)--\(4\) jointly consistent?
\item
  \textbf{Derivation:} From \(1\) and \(4\), derive \(R \land \lnot M\); from that, derive \(R\) and \(\lnot M\). Compare \(\lnot M\) with \(2\) (i.e., \(M\)). Show the contradiction.
\end{itemize}

\hypertarget{topic-2-hypothetically-speaking}{%
\chapter*{Topic 2 -- Hypothetically Speaking}\label{topic-2-hypothetically-speaking}}
\addcontentsline{toc}{chapter}{Topic 2 -- Hypothetically Speaking}

\hypertarget{gausss-robots-credibly-go-rogue}{%
\chapter{Gauss's robots credibly go rogue}\label{gausss-robots-credibly-go-rogue}}

\hypertarget{spreadsheets-really}{%
\section{Spreadsheets? Really?}\label{spreadsheets-really}}

Yes, emphatically! George Gilder says we should waste transistors (that is chips).\footnote{See \href{images/09/https://gilderpress.com/2020/03/12/investors-should-ignore-materialistic-superstitions/}{Gilder's comments here}. He goes on to a further idea: build billions of 1-chip interconnected systems (our mobile phones that are really computers) and waste chips that way instead of manufacturing billion chip data centers. According to Moore's law we will eventually get to a near zero-cost chip.} Gilder makes the fairly obvious point that we must use transistors (lots of them in an integrated circuit) or go out of business. They are ubiquitous. And arrived everywhere in a very short amount of time to boot. If you do not use them you lose control of your cost structure. Anything you build will be too expensive, too heavy, too big, too slow, too lacking in quality.

The same idea goes with Michael Schragge builds on Gilder's ironic hyperbole about transistors and analogizes that we should ``waste simulations.''\footnote{Here is a \href{images/09/https://www.technologyreview.com/2004/07/01/40161/prepared-minds-favor-chance/}{taste of Schrage's points of view}. He compiled the \href{images/09/https://www.amazon.com/exec/obidos/ASIN/0875848141/softwgardeinc}{``wasting prototyping'' paradigm into this book a couple of decades ago}.} If we do not so-called waste prototyping, rapid development, simulating potential problems, solutions, we will also go out of business. We must simulate until we drop! The alternative is that we will miss the one opportunity to improve or the one error that eliminates us. Of course the point he makes is that it iss not a waste, rather we should never shy away from working the problem, simulating the art of the possible.

So what is the value added of a prototype, which is simply a working model? It is about information, and information is a surprise, a deviation from a trend. Schragge believes that testing a hypothesis just gets us to the point of saying we seem, in probability that is, to have a trend going on here. In the world of growth, opportunity, error and ignorance, having a trend is barely the beginning of our journey. It is the deviation from the trend that matters.

Are we still talking about spreadhseets? Schragge quotes Louis Pasteur: ``Chance favors the prepared mind.'' Here the prepared mind is a product of simulations, the rapidly developed prototypes, Fleming used agar and discovered penicillin -- completely unexpected! Dan Bricklin developed the spreadsheet IBMDOS/Apple IIe program Visicalc.\footnote{\href{images/09/https://en.wikipedia.org/wiki/Dan_Bricklin}{Here is a summary of his work.} His innovation with Visicalc was to transform 20 hours of work into 15 minutes, almost of play at the time. Visicalc first ran on the Apple IIe. Dan is working on a web-based WikiCalc these days.} As a complete surprise this product was able to be used by millions of people to rapidly simulate other products and services. Steve Jobs credited Visicalc with the success of the Apple IIe and the Macintosh in 1985. IBM credited it with the success of the PC. Now people had a reason to buy the PC.

Using Visicalc we were able 40 years ago to build practical, plottable, usable option pricing models which transparently allowed us to visualize the calculations directly. Financial analysts built interactive pro forma balance sheet, income statements, and cash flow statements fed from managers' expectations, scenarios, and expert knowledge of markets. These models literally paid for themselves in days, not years. The main criterion for innovation success has always been the customer's payback, not the investors. How long did it take for the customer to recoup her investment? That's the innovation criterion. The spreadsheet is a sophisticated scratchpad some have used to be a production ready system.

But what is the most important message? A working prototype should be a sandbox where everyone is willing to get in and play. It has at least to be durable enough to get to the first slate of useful comments and suggestions for further improvement. Development continues! Rick Lamers recently open sourced his \href{images/09/https://github.com/ricklamers/gridstudio?ref=hackernoon.com}{Grid Studio spreadsheet product with deep integration with Python.}

Yes, let's continue to play.

\hypertarget{an-auspicious-result-again}{%
\section{An auspicious result again?}\label{an-auspicious-result-again}}

Instead of nearly 3,000 observations about wages and educational attainment, suppose we only believed that the minimum and maximum wage rate and years of education are credible. This is about the least amount of information we could possible cull from our knowledge of labor markets. If we simulate samples from these markets a lot of times (10,000 let's say) will be get our Gaussian auspicious results again?

To play in this sandbox, let's think a bit about what having just maximum and minimum data means. Here is that dataset.

\includegraphics{images/09/wage-educ-data-minmax.jpg}

It really does not get simpler than this! Our thought experiment, as we waste Gilder transistors over this, is to consider wages and years of educational attainment to be equally likely within the maximum and minimum intervals in our truly sparse data set. This is an example of the \textbf{uniform distribution}. Here is a sketch of the distribution.

\includegraphics{images/09/wage-educ-uniform.jpg}

There are two parameters only: \(a\) is the minimum and \(b\) is the maximum of the outcomes \(x=X\) on the horizontal axis. The probability that any outcome occurs between \(a\) and \(b\) is just this,

\[
Pr(x \mid a, \, b) = \frac{1}{b-a}
\]

\hypertarget{the-most-uninformative-distribution}{%
\section{The most uninformative distribution}\label{the-most-uninformative-distribution}}

Suppose we have no idea about the shape of the distribution of our stock portfolio. We do know that the value of the portfolio can range from \$10,000 to \$15,000 in a 52 week period. What is mean and standard deviation you can expect in this situation?

We use the uniform distribution to model our diffuse beliefs. For this problem \(a = 2.14\), \(b = 97.5\). The population mean of the uniform distribution is

\[
\mu = \frac{(a+b)}{2} = \frac{2.14 + 97.5}{2} = 49.82
\]

The population standard deviation is

\[
\sigma = \frac{(b-a)}{\sqrt{12}} = \frac{(97.5 - 2.14)}{3.46} = 27.53
\]

Why? Press this button to flex your calculus-based analytical muscles!

show / hide

\leavevmode\hypertarget{uniform}{}%
We start with the probability distribution function of the uniform distribution.

\begin{equation}
f(x) =
\begin{cases}
\frac{1}{b-a}, & a \le x \le b \\
0, & \text{otherwise}
\end{cases}
\end{equation}

where \(x\) is a number between \(a\) and \(b\), \(a \le x \le b\). This equation states simply that in an experiment, say call some brokers, there is an \(f(x)\) probability of getting a value \(x\). The \(f(x\) are all the same and constant. This is about as uninformative a distribution as we can imagine.

What is the expected value? It is just the weighted average of \(x\) from \(x=a\) to \(x=b\) where the weights are all given by \(f(x)\). Since \(x\) is on the real line, we use the integral to calculate the weighted average. We should expect the answer to be the simple average of \(a\) and \(b\).

\begin{align}
  \mu &= \int_a^b xf(x)dx \\
      &= \int_a^b x \left(\frac{1}{b-a}\right) dx \\
      &= \left(\frac{1}{b-a}\right) \int_a^b x\, dx \\
      &= \left(\frac{1}{b-a}\right) \left. \frac{x^2}{2} \right|_{x=a}^{x=b} \\
      &= \left(\frac{1}{b-a}\right)\left( \frac{b^2 - a^2}{2}\right) \\
      &= \frac{a+b}{2}
\end{align}

We used the division of \(b^2-a^2\) by \(b-a\) to get \(b+a\), for \((b-a)(b+a)=b^2-a^2\). The result aligns with our intuition.

How about the variance and standard deviation. We use the result that the variance is

\[
E(x-\mu)^2 = E(x^2) - \mu^2
\]

Here the \(E(y)\) is the expectation of \(y\), that is the weighted average computed in the same way as we did for \(\mu\). Intuitively this measure should most likely include the range of the distribution, that is, \(b-a\). In fact we can show that the standard deviation is

\[
\sigma = \frac{b-a}{\sqrt{12}}
\]
So it is the range scaled by \(\sqrt{12}\). Where on earth did 12 come from?

\begin{align}
  E(x^2) &= \int_a^b x^2f(x)dx \\
      &= \int_a^b x^2 \left(\frac{1}{b-a}\right) dx \\
      &= \left(\frac{1}{b-a}\right) \int_a^b x^2\, dx \\
      &= \left(\frac{1}{b-a}\right) \left. \frac{x^3}{3} \right|_{x=a}^{x=b} \\
      &= \left(\frac{1}{b-a}\right)\left( \frac{b^3 - a^3}{3}\right) \\
\end{align}

We then combine this result with \(\mu^2\) use our cleverness in combining and simplifying polynomials.

\begin{align}
  Var(x) =\sigma^2 &= E(x^2) - [E(x)]^2 \\
      &= \left(\frac{1}{b-a}\right)\left( \frac{b^3 - a^3}{3}\right) - \frac{(b+a)^2}{4} \\
      &= \left(\frac{1}{b-a}\right)\left[\frac{4b^3 - 4a^3}{12} - \frac{3(b-a)(b+a)^2}{12}\right] \\
      &=\frac{(b-a)^2}{12}
\end{align}

\hypertarget{simulate-until-morale-improves}{%
\section{Simulate until morale improves!}\label{simulate-until-morale-improves}}

\includegraphics{images/09/wage-educ-simulation-minmax.jpg}
Do we notice a change from the previous model of last week. We compare column F for the two models. Indeed we made an error: we failed to model (and it's an easy model) the sample index from 1 to 12. We missed a sample. Last week there were only 11 samples taken 10,000 times. Well, this week we adjust the model and move on. An improvement here would be to calculate and simulate education means and standard deviations. This is a good exercise and all it takes is to put the cursor into cell L3 and insert two columns using the Home \textgreater{} Insert feature. Then do the same at cell Q3. We then fill in the relevant formulas and check the name manager to be sure the ranges are intact. Then we rerun the model

\hypertarget{is-it-true-that-gauss-is-in-the-house-again}{%
\section{Is it true that Gauss is in the house again?}\label{is-it-true-that-gauss-is-in-the-house-again}}

\begin{figure}
\centering
\includegraphics{images/09/wage-educ-mean-minmax.jpg}
\caption{Wages and education 10000 simulations - mean wages plot}
\end{figure}

It definitely appears to be so. A near 3.00 mesokurtotic relative frequency distribution with near zero skewness match with a mean that is almost exactly the median all point to a Gaussian distribution.

\hypertarget{and-again}{%
\section{And again?}\label{and-again}}

\includegraphics{images/09/wage-educ-sd-minmax.jpg}

This one's a little rugged do we think? Yes, and probably due to sampling error with square root calculationg. We recall that the standard deviation is the square root of the variance, that the variance is the average of the sum of squared deviations of wage from its arithmetic mean. But all in all it looks Gaussian for the same reasons as the mean.

\hypertarget{the-association}{%
\section{The Association}\label{the-association}}

\begin{figure}
\centering
\includegraphics{images/09/wage-educ-corr-minmax.jpg}
\caption{Wages and education 10000 simulations - correlation.}
\end{figure}

\begin{quote}
Correlation is not Causation
\end{quote}

True or False? In our simple model, \(E \rightarrow W\) where \(E\), education, is represented and measured by years of education starting with first grade, and \(W\), is the median USD per hour wage rate. This is causation. There is an argument of \textbf{antecedent probability} that a highly plausible correlation supports the conjecture (here again, theory, hypothesis) that education causes wages. But by the application of the mind projection fallacy, theory is not all of reality. It can be refuted (we remember, maybe with some trepidation the logical consistency of \textbf{modus tollens}). In any case, we might conjecture on the conjecture that correlation is least certain of all findings.

\hypertarget{a-tale-of-coir}{%
\section{A tale of coir}\label{a-tale-of-coir}}

Here is a round of work steps we can perform to repurpose our model with coir data to help solve a procurement question.

\hypertarget{business-situation}{%
\subsection{Business Situation}\label{business-situation}}

We imagine we work for an organization which manufactures activated carbon for filtration systems. We reduce coconut fiber (coir) to carbon using a chemical process to make activated carbon. We are about to contact 12 vendors for price quotes.

Here is a quickly rendered picture of the organization's supply chain from vendors \(V\) to customers \(C\).

\includegraphics{images/09/coir-chain.jpg}

The flow of tasks ranges from vendor \(V\) through procurement \(P\), manufacturing \(M\), distribution \(D\), all driven by the customer \(C\). Political and international relations, regulators, and the Board of Directors \(BoD\) govern the management of the processes and relationships. Infrastucture supports the processes.

\hypertarget{business-questions}{%
\subsection{Business Questions}\label{business-questions}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is a reasonable range of coir prices we might use to write a contract for coir delivery with a potential vendor?
\item
  What prices would be considered too high (potential price gouging) ot too low (potential evidence of poor quality coir)?
\end{enumerate}

Both of these questions bear on our organization's tolerance for risk. This tolerance is buried in the Delegation of Authority (DoA) policy understanding between the organization's governing board and the managers the board hires. The DoA helps to align manager actions with organizational preferences and requirements.

\hypertarget{data}{%
\subsection{Data}\label{data}}

{[}We visit the Coconut Community Trade organization site\footnote{Go to this page as of January 5, 2023: \url{https://coconutcommunity.org/page-statistics/market-review} and select a \emph{Coconut Fiber} Market Review month for a sample.} to get a range of possible coir prices. There are two series:. The price difference (basis) is due to insurance and freight charges in the distance between the two countries. Apparently the direction of trade is from west to east.

Here is a graph taken from the site. It only shows recent history through 2018 unfortunately. A task will be to get more recent data. But for now this will work for us.

\includegraphics{images/09/coir-prices-graph-iccorg.jpg}

Our manufacturing facility is in Southeast Asia, so we would be taking delivery of coir closer to Indonesia than to Sri Lanka, thus we choose the Indonesian FOB price series for analysis. We keep in mind the International Commercial Terms of trade and the {[}definition of FOB.\^{}{[}This article is updated frequently with appropriate references: \url{https://en.wikipedia.org/wiki/Incoterms} .)

Here are our findings.

\includegraphics{images/09/coir-data-map.jpg}
The map shows the direction of trade as west to east as we suspected (a prior emerges?). Indonesian prices are higher than Sri Lankan prices by a range of USD 130 to 170 per MT. This is called a physical price basis and is due to freight charges and the differentials between the countries and their supply and demand for coir. From an economics point of view we might interpret the differences, the basis, as differences in capital and labor productivity at one end of the supply chain, and at the other end as changes in preferences and technology demand for consumers of wholesale coir.\footnote{\citet{LeClair2006} describe several international trade externalities that we might suspect would influence physical basis: yet another set of considerations for prior probabilities.}

\hypertarget{analysis}{%
\subsection{Analysis}\label{analysis}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We will use the uniform distribution to model 12 vendor price quote responses. Why? Mainly because we are agnostic about the shape of the distribution of possible prices. The only thing we know is a maximum and minimum of prices.
\item
  Our tolerance for error is set by corporate policy at 5\%. This means that we are looking for the plausibility of prices that center around the most probable price plus or minus \(100 \times (1.00 - 0.05)/2 =47.5\)\%. We will call this the *high density probability interval**.
\item
  We simulate 12 vendor price responses many times to generate a distribution of mean prices across the 12 vendors. This move recalls Schrage's \textbf{waste simulations} which we will interpret as: we will not sign a contract until we exhaust the possibilities of price movements, or ourselves, whichever comes first.
\end{enumerate}

\hypertarget{the-exercise-finally-emerges}{%
\subsection{The exercise finally emerges}\label{the-exercise-finally-emerges}}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-14}{}\label{exr:unnamed-chunk-14}What is our level of credibility regarding the Indonesian range of coir FOB prices? If we were to change our tolerance, how would the range of coir prices change?
\end{exercise}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-15}{}\label{exr:unnamed-chunk-15}What low price range is consistent with this data? Why should we be wary?
\end{exercise}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-16}{}\label{exr:unnamed-chunk-16}What low price range is consistent with this data? Again, do we have any reason to be wary?
\end{exercise}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-17}{}\label{exr:unnamed-chunk-17}What if we were to model the difference (basis) between Indonesia and Sri Lanka? How different are the two prices? We might come back to this question in future rounds!
\end{exercise}

\hypertarget{credibility-or-confidence}{%
\chapter{Credibility or Confidence?}\label{credibility-or-confidence}}

That will be confidence for those who would pursue a career in Frequentist statistics.

\hypertarget{imagine-this}{%
\section{Imagine this\ldots{}}\label{imagine-this}}

Your team job is to handle the 579 current client billings in your team's book of business. You only can contact 10 clients in the short time between now and when you must estimate the range of billings for a revenue forecast for your team's managing director. Specifically,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is the expected level of billings?
\item
  What is the range of billings we might expect?
\end{enumerate}

\textbf{What is a team member to do?}

This time around we will try mightily to do these things.

\begin{itemize}
\item
  Understand the reason for estimating with confidence interval
\item
  Calculate credibility / probability intervals for population proportions and means
\item
  Interpret a probability interval
\item
  Know the meaning of margin of error and its use
\item
  Compute sample sizes for different experimental setting
\item
  Know when and how to use z- and t-scores and probability intervals to estimate the population mean
\item
  Compute sample sizes for estimating the population mean
\end{itemize}

Yes, it seems a very tall order. Actually, all of these items are in our tool box already. Now we use them, interpret the results, mold them to the problem at hand.

\hypertarget{try-this-on-for-size}{%
\section{Try this on for size}\label{try-this-on-for-size}}

What is a team member to do? Experiment! That's what.

\begin{itemize}
\item
  Suppose there are only 10 billings.
\item
  What if we take samples of 4 billings? There are \(_{10}C_4 = 210\) combinations of samples.
\item
  We might enumerate them all. Calculate means for each sample.
\item
  Calculate the mean of the sampled means.
\item
  Compare with the mean of the population of the five experimental billings.
\end{itemize}

Below is the distribution the random variable, sample means.

\includegraphics[width=0.75\linewidth]{_main_files/figure-latex/unnamed-chunk-20-1}

\begin{table}

\caption{\label{tab:unnamed-chunk-21}Billing data: 10 observations}
\centering
\begin{tabular}[t]{r|r|r|r|r}
\hline
mean & std\_dev & median & skewness & kurtosis\\
\hline
139 & 37.4 & 138 & 0.103 & 2.04\\
\hline
\end{tabular}
\end{table}

\begin{table}

\caption{\label{tab:unnamed-chunk-21}Sample Means: N =  10 , n =  4 , Samples =  210}
\centering
\begin{tabular}[t]{r|r|r|r|r}
\hline
mean & std\_dev & median & skewness & kurtosis\\
\hline
140 & 17.2 & 139 & 0.095 & 2.84\\
\hline
\end{tabular}
\end{table}

Some observations are likely in order. The sampled mean distribution is

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Approximately symmetric
\item
  Defined on + to - infinity
\item
  Almost mesokurtic
\item
  Mean of sample means is equal to the population mean
\item
  Standard deviation of sample means is about half of the population standard deviation
\end{enumerate}

That last observation is the square root of one-fourth. The others indicate a normal distribution is at work underneath all of this calculational machinery.
Are there any recommendations out there?

\hypertarget{what-about-the-sampled-standard-deviation}{%
\section{What about the sampled standard deviation?}\label{what-about-the-sampled-standard-deviation}}

We supposed we had a experimental population of 10 billings. We just pulled several samples of sample size 4 from this population. We just found out that the mean of the sample means is the same as the population mean. This the same as saying that the point estimate of the mean of the sample means is unbiased.

The samples are all pulled from a population with population mean \(\mu = 138\) and has a population standard deviation of \(\sigma = 35.33\).

All of this indicates that each and every draw of each of the 4 sampled billings comes from a population distributed with a \(\mu = 138\) and a population standard deviation of \(\sigma = 35.33\).

Let's get a little more precise using the logic of algebra. We know that each sample is a draw of 4 billings \(X = \{X_1, X_2, X_3, X_4\}\), where 1, 2, 3 and 4 are simply any four indiscriminate draws from the experimental population of 10 billings. Now we take just three samples each with 4 draws.

We calculate the mean of the sampled means of each sample here.

\[
\overline{X} = \frac{1}{3}(\overline{X}_1+\overline{X}_2+\overline{X}_3)
\]

We then calculate he variance (square of the standard deviation) of the sampled mean.

\[
\sigma_{\overline{X}}^2 = \left( \frac{1}{3} \right)^2 (\sigma_{\overline{X}_1}^2 + \sigma_{\overline{X}_2}^2 + \sigma_{\overline{X}_3}^2)
\]

We use a sleight of the algebraic hand to get this result. Here it is below in a separate section.

The variance (square of the standard deviation) of the independently drawn (no intersection!) \emph{sum} of the samples themselves is

\[
\sigma_{(\overline{X}_1+\overline{X}_2+\overline{X}_3)}^2 = \sigma_{\overline{X}_1}^2 + \sigma_{\overline{X}_2}^2 + \sigma_{\overline{X}_3}^2
\]

But

\[
\sigma_{\overline{X}_1}^2 + \sigma_{\overline{X}_2}^2 + \sigma_{\overline{X}_3}^2 = 3\sigma^2
\]

Which is three times the square of the population standard deviation. So that,

\[
\sigma_{\overline{X}}^2 = \left( \frac{1}{4} \right)^2 4\sigma^2 = \frac{\sigma^2}{4}
\]

and for any sample size \(n\), we have the standard deviation of the sampled means as

\[
\sigma_{\overline{X}} = \frac{\sigma}{\sqrt{n}}
\]

For our experiment all of this indicates that the distribution of the sample means is

` 1. Approximately normally distributed with
2. Mean = population mean \(\mu = 138\), and
3. Standard deviation:

\[
  \sigma_{\overline{X}} = \frac{35.33}{\sqrt{3}} = 20.398
  \]

This is very convenient result indeed, a veritable short-cut.

\hypertarget{heres-the-promised-derivation}{%
\subsection{Here's the promised derivation}\label{heres-the-promised-derivation}}

Let's derive the above formula. We define \textbf{variance: the expectation of the squared deviation of a random variable from its mean.} We denote variance by \sigma\^{}\{2\}, \(s^{2}\) or \(Var(X)\). From this definition of Variance, we can write the following equation.

\[
Var(X) = E[(X - E[X])^2] 
\]

Since we have to find the variance of the mean of samples, let's replace the random variable X in the above equation with its mean, \(\overline{X}\).

\[
Var(\overline{X}) = E[(\overline{X} - E[\overline{X}])^2] 
\]

We know that we can calculate the arithmetic mean this way.

\[\overline{X}= \frac{\sum_{i=1}^n{X_i}}{n}\]

we can expand the variance definition into this next expression.

\[
Var(\overline{X}) = E\left[\left(\frac{\sum_{i=1}^n{X_i}}{n} - E\left[\frac{\sum_{i=1}^n{X_i}}{n}\right]\right)^2\right] 
\]

We then factor the \(\frac{1}{n}\) from the above equation.

\begin{align}
Var(\overline{X}) &= E\left[\frac{1}{n^2}\left(\sum_{i=1}^n{X_i} - E[\sum_{i=1}^n{X_i}]\right)^2\right] \\ 
 &= \frac{1}{n^2}E\left[\left(\sum_{i=1}^n{X_i} - E[\sum_{i=1}^n{X_i}]\right)^2\right] 
\end{align}

We expand \(E[\sum_{i=1}^n{X_i}]\) into this expression.

\[
E[{X_1} + {X_2} + {X_3} + ...... + {X_n}] 
\]

Expectation is just an average so that the average of a sum is the sum of the averages.

\[
E[{X_1}] + E[{X_2}] + E[{X_3}] + ...... + E[{X_n}]
\]
Since all the samples in the distribution are random; also known as IID (Independent and Identically Distributed), the mean of each of them is the same. Therefore we can write the above equation as:

\[
E[\overline{X}_1] + E[\overline{X}_2] + \ldots + E[\overline{X}_n] = \sum_{i=1}^nE[\overline{X}_i] 
\]

We use this summation to get this result.

\begin{align}
Var(\overline{X}) &= \frac{1}{n^2}E[(\sum_{i=1}^n{X_i} - \sum_{i=1}^nE[\overline{X}])^2] \\ 
                  &= \frac{1}{n^2}\sum_{i=1}^nE[({X_i} - E[\overline{X}])^2] \\
                  &= \frac{1}{n^2}[E({X_1} - E[\overline{X}])^2 + E({X_2} - E[\overline{X}])^2 + ...... + E({X_n} - E[\overline{X}])^2] 
\end{align}

We used the idea that the expectation (the average after all) of an expectation (of an average) is just the average. Math!

The variance of the all the random variables\} \{X\_1\}, \{X\_2\}\ldots\{X\_n\} give us this progression.

\begin{align}
Var(\overline{X}) &=  \frac{1}{n^2}[nE({X} - E[\overline{X}])^2] \\ 
                  &= \frac{1}{n^2}(n\sigma^2) \\
                  &= \frac{\sigma^2}{n}
\end{align}

And finally we have this nugget.

\[
\sigma_{\overline{X}} = \sqrt{Var(\overline{X})} = \frac{\sigma}{\sqrt{n}}
\]

Now we return to our regularly sponsored program.

\hypertarget{probability-intervals-1-known-population-standard-deviation}{%
\section{Probability intervals 1: known population standard deviation}\label{probability-intervals-1-known-population-standard-deviation}}

Our first ``forecasting'' exercise is upon us. If we know the population standard deviation we use the Normal sample means distribution to help us think about "confidence. Out of all of the possible average billings, What is a range of expected billings such that the MD would still possibly believe that she has 95\% consistency with the data?

If the population standard deviation is known, then we can estimate expected billings such that \(\mu\) is somewhere between a lower bound \(\operatorname{L}\) and an upper bound \(\operatorname{U}\)

\[
\operatorname{L} \leq \mu \leq \operatorname{U}
\]

Our beliefs will be a probabilistic calculation of the lower and upper bounds. Suppose our required level of plausiblity is 95\%. We have two tails which add up to the maximum probability of error, which we will call the \(\alpha\) significance level. In turn \(alpha\) equals one minus the confidence level, which is \(1- \alpha = 0.95\). For the two tail interval, calculate \(1 - confidence = \alpha = 1- 0.95 = 0.05\), so that \((1-\alpha) / 2\) for the amount of alpha in each tail.

Here's what we can do next.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For \(1- \alpha = 95\%\) there is \(\alpha / 2 = 0.05/2 = 0.025\) in each tail.
\item
  The upper tail for the \(\alpha\) consistency level begins at \(1 - 0.025 = 0.975\) cumulative probability or \(97.5\%\).
\item
  The lower tail for the \(\alpha\) consistency level ends at \(0.025\) cumulative probability or \(2.5\%\).
\end{enumerate}

\includegraphics{_main_files/figure-latex/unnamed-chunk-22-1.pdf}

\includegraphics{_main_files/figure-latex/unnamed-chunk-23-1.pdf}

\hypertarget{our-first-procedure-emerges}{%
\section{Our first procedure emerges}\label{our-first-procedure-emerges}}

We may have a procedure we can follow.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We will base lower and upper bounds using the \(z\) score. Start with the \(z\) score and solve for the population mean \(\mu\) and remembering that \(z\) can take on plus and minus values:
\end{enumerate}

\[
z = \frac{\overline{X} - \mu}{\sigma / \sqrt{n}}
\]

\[
\mu = \bar X \pm z \sigma / \sqrt{n}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  If the population standard deviation \(\sigma\) is known then our belief about the size of the population mean \(\mu\) may be represented by the normal distribution of sample means. Suppose we desire a 95\% consistency interval of our conjectures with the data about the size of the population mean. Remember that in our experiment the sample size \(n = 3\).Then calculate
\end{enumerate}

\begin{itemize}
\tightlist
\item
  For the lower bound:
\end{itemize}

\[\operatorname{L} = \overline{X} - z_{0.025}\sigma / \sqrt{n}\,,
\],

where \(z_{0.025} =\) \texttt{NORM.INV(0.025,0,1)\ =\ -1.96}, so that

\begin{align}
\overline{X} - z_{0.025}\sigma/\sqrt{n} 
&= 138.13 + (-1.96)(35.33 / \sqrt{4}) \\ 
&= 138.13 - 34.6 \\
&= 104
\end{align}

\begin{itemize}
\tightlist
\item
  For the upper bound:
\end{itemize}

\[
\operatorname{U} = \overline{X} + z_{0.975}\sigma/\sqrt{n}\,,
\]

where \(z_{0.975} =\) \texttt{NORM.INV(0.975,0,1)\ =\ 1.96}, so that

\begin{align}
\overline{X} + z_{0.975}\sigma/\sqrt{n} &= 138.13 + (1.96)(35.33/ \sqrt{4}) \\
&= 138.13 + 34.6 \\
&= 173
\end{align}

Thus we have 95\% consistency that the expected billings \(\mu\) lie in the interval

\[
104 \leq \mu \leq 173
\]

For language and interpretation purposes, literally from day one of our investigations, we can also say that it is 95\% plausible, indeed credible, to believe that the population mean lies in this interval. And that is all we can say with this model.

\hypertarget{probability-intervals-2-on-to-the-unknown-standard-deviation}{%
\section{Probability intervals 2: on to the unknown standard deviation}\label{probability-intervals-2-on-to-the-unknown-standard-deviation}}

Let's now suppose we \emph{do not know} the population standard deviation. Instead of sampling from a known distribution like a uniform distribution, we sample from a population whose standard deviation we can only guess. Now the sampled standard deviation is also a random variable, like the sampled mean before it. In practice this is nearly always the case as we usually sample behavior in the raw, without help from friendly distributions. What do we do now? We have a can-opener. It is the Student's t-distribution. We use the Student's t distribution to correct for consistencies that are, well, not so consistent due to the increased uncertainty introduced by a thoroughly unknown, but sampled, standard deviation.

Here's a plot of the Student's t overlaid with the normal distribution.

\includegraphics{_main_files/figure-latex/unnamed-chunk-24-1.pdf}

What do we notice? The normal (red) distribution is more pinched in than the Student's t (kurtosis? right!). Student's t (blue) distribution has thicker tails than the normal distribution. Otherwise, both are symmetric.

Let's check tail thickness: in Excel we can use \texttt{=T.INV(2.5\%,3)} which returns \texttt{-4.30}, and where the degrees of freedom \(df\) of our 4 sample billings are \(df = n - k = 4 - 1 = 3\). We lose one degree of freedom when we calculate the sample mean. Thus for the t distribution it takes 4.30 standard deviations below the mean to hit the 2.5\% level of cumulative probability. It only took 1.96 standard deviations on the normal distribution. That it took fewer standard deviations for the normal than for the t distribution to hit the 2.5\% level of cumulative probability means that the t distribution is thicker tailed than the normal.

\includegraphics{_main_files/figure-latex/unnamed-chunk-25-1.pdf}

\hypertarget{by-the-way-who-is-student}{%
\subsection{By the way, who is Student?}\label{by-the-way-who-is-student}}

There is a brewery in Dublin, Ireland, whose slogan is ``Guiness is Good for You.'' W. S. Gosset (1876-1937) was a modest, well-liked Englishman who was a brewer and agricultural statistician for the famous Guinness brewing company in Dublin.

Guiness insisted that its employees keep their work secret, so he published the distribution under the pseudonym ``Student'' in 1908. This was one of the first results in modern small-sample statistics.

\hypertarget{our-second-procedure}{%
\section{Our second procedure}\label{our-second-procedure}}

Again a procedure that follows the known population \(\sigma\), but instead of using the z score and the normal (Gaussian of course) distribution, we use the thicker tailed Student's t-distribution.

We start by basing lower and upper bounds using the \(t\) score and solve for the population mean \(\mu\) and remembering that \(t\) can take on plus and minus values:

\[
t = \frac{\overline{X} - \mu}{\hat s / \sqrt{n}}
\]

\[
\mu = \overline{X} \pm t \hat s / \sqrt{n}
\]

If the population standard deviation \(\sigma\) is \emph{not} known then our belief about the size of the population mean \(\mu\) may be represented by the \emph{Student's t} distribution of sample means. Suppose we desire a 95\% level of plausibility, of consistency of the data with this observational model, the Student's t-distribution about the size of the population mean.

This means we have a \((1 - 0.95)/2 = 0.025\) \(\alpha\) probability of error in consistency of the data with this model in mind. Remember that in our experiment the sample size \(n = 4\). Instead of the population standard deviation \(\sigma\), we use the sample standard error \(\hat s\), where the hat over the variable emphasizes the sampled character of this whole computation. Suppose \(s = 37.23\). Given an 95\% consistency level and 3 degrees of freedom we have these computations.

\[
\ell = \overline{X} - |t_{0.025}| \hat s / \sqrt{n}
\]

where \(t_{0.025} =\) \texttt{T.INV(0.025,3)\ =\ -3.18}, and we take the absolute value of \(t_{0.025}\) since the \(\alpha\) significance rate 2.5\% is symmetrically positioned on the t distribution in each tail. We then have

\[
\operatorname{L} = 138.13 - (3.18)(35.33 / \sqrt{4}) = 82.11
\]
We have at the other end of the distribution

\[
\operatorname{U} = \overline{X} + t_{0.975}\hat s / sqrt{n}
\]

where \(t_{0.975} =\) \texttt{T.INV(0.975,2)\ =\ 3.18}, so that

\[
\overline{X} + t_{0.975}\sigma / \sqrt{n} = \bar X + (3.18)(35.33/ \sqrt{4}) = 194.55
\]

Thus we believe our deductions that the data are 95\% consistent with expected billings \(\mu\) that are somewhere in this interval:

\[
82.11 \leq \mu \leq 194.55
\]

We see a much wider an interval than if we knew the population standard deviation!

\hypertarget{exercises}{%
\section{Exercises}\label{exercises}}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-26}{}\label{exr:unnamed-chunk-26}The Hiatus retail outlet takes a random sample of 25 customers from a segment population of 1,000 with a mean average transaction size of \$80 normally distributed with a known population standard deviation of \$20 per transaction. Find
- The 90\% confidence interval for transaction size, and
- The 95\% confidence interval for transaction size, and
- The 99\% confidence interval for transaction size.

What do these results indicate for management?
\end{exercise}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-27}{}\label{exr:unnamed-chunk-27}A compensation analyst for an investment bank wants to estimate the mean hourly wages of several hundred employees in the first 5 pay bands plus or minus within plus or minus \$20. Management wants a 99\% confidence level for the analysis. Assume that the population standard deviation is known to be \$40 and that hourly wages are normally distributed. Find the minimum sample size required for this analysis.
\end{exercise}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-28}{}\label{exr:unnamed-chunk-28}Find the confidence intervals for 1 if the population is not known and the sample standard deviation is \$23 per transaction.
\end{exercise}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-29}{}\label{exr:unnamed-chunk-29}Find the minimum sample size required for 2 if the population is not known and a sample standard deviation is \$34.
\end{exercise}

\hypertarget{put-it-to-the-test}{%
\chapter{Put It to the Test}\label{put-it-to-the-test}}

\hypertarget{a-tale-of-two-hypotheses}{%
\section{A tale of two hypotheses}\label{a-tale-of-two-hypotheses}}

There are two hypotheses that we want to compare, a null hypothesis \(h_0\) and an alternative hypothesis \(h_1\). Prior to collecting the data that is relevant to the hypotheses we have some beliefs \(P(h)\) about which hypotheses are true. We then obtain data \(d\). Unlike frequentist statistics Bayesian statistics does allow us to talk about the probability that the null hypothesis is true, or that the alternative hypothesis is true, for that matter. Better yet, it allows us to calculate the \textbf{\emph{posterior probability of the null hypothesis}}, using Bayes' rule:

\begin{equation}
P(h_0 \mid d) = \frac{P(d \mid h_0) P(h_0)}{P(d)}
\end{equation}

Similarly we can, and have, calculated the \textbf{\emph{posterior probability of the alternative hypothesis}}, using Bayes' rule:

\begin{equation}
P(h_1 \mid d) = \frac{P(d \mid h_1) P(h_1)}{P(d)}
\end{equation}

We convert these two calculations into one in order to make the comparison.

\begin{equation}
\underbrace{\frac{P(h_1 \mid d)}{P(h_0 \mid d)}}_{\text{Posterior Odds}} = \underbrace{\frac{P(d \mid h_1)}{P(d \mid h_0)}}_{\text{Bayes Factor}} \times \underbrace{\frac{P(h_1)}{P(h_0)}}_{\text{Prior Odds}}
\end{equation}

There are three terms we can explore.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  On the left hand side are the deduced the \emph{\textbf{posterior odds}} which records the relative compatibility of the hypotheses with the data.
\item
  On the far right hand side are the \textbf{\emph{prior odds}}, which indicates what we believed about the two hypotheses \emph{before} applying the data.
\item
  On the right hand side of the equation and in the middle, is the \emph{\textbf{Bayes Factor}} (\emph{\textbf{BF}}), which the relative evidence, that is, the likelihood of data given each hypothesis. This factor quantifies the strength of evidence provided by the data.
\end{enumerate}

Often only the Bayes factor is reported. This is because different analysts might have different views and thus different prior odds surrounding the hypotheses.

There are two examples coming up to put these ideas to work.

\hypertarget{falsity-of-a-hypothesis}{%
\section{Falsity of a hypothesis}\label{falsity-of-a-hypothesis}}

Is \emph{falsity} even a word? TYpical this means some quality of being untrue, even incorrect. In the case of plausibility analysis of a conjecture we do not know if anything is virtually true or false. We do know if a conjecture is consistent with the data, observed, and possibly also supposed (as in priors). So to talk about hypothesis falsification (a much stronger word), or falsity is probably an incorrect way of characterizing the plausible state of a model, a hypothesis, an idea, a conjecture. The question of whether a hypothesis is acceptable as true or not seems to be binary: it's one way or the other. So we then seem to also need to invent some sort of superstructure that encompasses this question of hypothetical alternatives.

Be that as it may, inference is oftem framed in binary terms.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  An hypothesis is a binary outcome: either the theory is true or it is false.
\item
  We get a statistical hint, indicator, or cue about the probable falsity of the hypothesis.
\item
  Our goal is to deduce the impact of the hint or cue on the probable status of the hypothesis.
\end{enumerate}

If this frame makes any sense at all, and let's suppose that it does, we will use Bayes' theorem to deduce, that is, use a a principled and logical way, the impact of the cue, the indicator, on the status of the hypothesis. We move from purely iron-clad is or isn't to more or less plausable. We thus actively admit our potential for bias, ignorance, and the complemetary need for correction and updating.

We need all three steps to complete the analysis. We often, if not always, do not complete the last, that third, step.

\hypertarget{for-example}{%
\subsection{For example}\label{for-example}}

Our organization regularly conducts reviews of its research and development activities. Innovation is the key to the success of providing relevant services to our customers. When we observe that our innovations actually meet customer needs, our services reach an ability to service 95\% of the customer base. When we do not meet their needs we serve only 5\%. The rate at which innovations actually meet customer needs, as customers view them, is only 1\%. Yet we persevere! What is the probability of an innovation meeting customer needs given high service levels of 95\%?

Here we go.

\begin{itemize}
\item
  Suppose the probability of a positive finding \(positive\), that is, the customer service rate, when an hypothesis, that is when we meet customer needs, is true, is \(Pr(positive \mid true) = 0.95\). That's more often than not called the \emph{\textbf{power of the test}}.
\item
  Suppose that the probability of a positive finding \(positive\), when an hypothesis is false, is \(Pr(positive \mid false) = 0.05\). That's what researchers would call the \emph{\textbf{false-positive rate}} .
\item
  The so-called \textbf{base rate} is the probability of a true hypothesis, that is, an innovation meets a customer's needs on the customers own terms. We suppose that for 1 in every 100 hypotheses, that is, innovations in our example, turns out to be true. Then \(Pr(true) = 0.01\). Does anyone really know what this probability is? The history of science, innovation, and successful start-ups suggests it's small.
\item
  First, not all ideas, conjectures, innovations are reported, usually only the ones which survive. Even so, on about 15\% of new business start-ups survive more than 10 years in the US.\url{https://www.bls.gov/bdm/bdmage.htm}
\item
  Second, FDA drug trial rates of success of 13\% provide another indicator. Taking both factors jointly into account indicates that under 2\% of innovations might make it to market. Third, we might have it wrong as we mix pharmaceuticals with general business start-ups, but at least it is a beginning and so we posit a conservative 1\% base rate.
\end{itemize}

Let's compute the posterior.

\begin{align}
Pr(true \mid positive) &= \frac{Pr(positive \mid true) Pr(true)}{Pr(positive)} \\
             &= \frac{Pr(positive \mid true) Pr(true)}{Pr(positive \mid true) Pr(true) + Pr(positive \mid false) Pr(false)} \\
             &= \frac{Pr(positive \mid true) Pr(true)}{Pr(positive)} \\
             &= \frac{(0.95)(0.01)}{0.01(0.95) + 0.99(0.05)} \\
             &= \frac{0.0095}{0.059} \\
Pr(true \mid positive) &= 0.16         
\end{align}

The denominator always gets us. It is the weighted average, the expectation, that the likely answer is \(positive\), a positive finding. We substitute the relevant values to get this approximate \(Pr(true \mid positive) = 0.16\).

So a positive finding corresponds to a 16\% chance that the hypothesis is true. This is the same low base-rate phenomenon that applies in medical (and economic policy) testing. You can shrink the false-positive rate to 1\% and get this posterior probability up to 0.5, only as good as a coin flip. The most important thing to do is to improve the base rate, \(Pr(true)\), and that requires thinking, stewing over, reflection and many iterations of each, not testing.

Our nex stop is to compare two ways of performing the task of inferring a judgment about data. Both require hypotheses. But one does not go the distance to the 16\% result we just obtained.

\hypertarget{frequentist-vs.-bayesian-inference}{%
\section{Frequentist vs.~Bayesian Inference}\label{frequentist-vs.-bayesian-inference}}

\hypertarget{a-possibly-illuminating-example}{%
\subsection{A possibly illuminating example}\label{a-possibly-illuminating-example}}

We will solve a simple inference problem using both frequentist and Bayesian approaches. Then we will compare our results based on decisions made with the two methods, to see whether we get the same answer or not. If we do not, we will discuss why that might happen.

\begin{itemize}
\item
  Our organization has a contract to train workers in safe materials handling practices. The materials are highly toxic to people, animals, and plants, and spread toxins both in the air and through water media. Some executives claim that 10\% of worker-teams have enough experience not to require further training, while other executives believe that 20\% of the worker-teams have the requisite experience.
\item
  Our department must provide a convincing analysis supporting one or the other claim.
\end{itemize}

Let's assume these conditions.

\begin{itemize}
\item
  We are being asked to support a decision, and there are associated payoff/losses that we should consider. If we make the correct decision, your gives you a bonus. On the other hand, if we make the wrong decision, we might very well lose our jobs, the stakes are so high.
\item
  However, management knows about the highly likely uncertainty of results and will allow us to be wrong 5\% of the time equivalent to 1 wrong result in 20 samplings.
\item
  We have funds to draw a random sample of workers from the population. This is no easy task, since to sample workers means to observe their practices in person at their work sites. Accessing workers requires expensive travel and lodging expense into very remote regions where physical security may easily be compromised. A 5 person fact-finding team over a 15 day period will entail a consultant cost of \$200,000 plus another \$300,000 for guides, travel arrangement, licenses, insurance, and other fees for a total of \$\textbackslash500,000, just for this one sampling.
\end{itemize}

The cost of making a wrong decision is high financially and reputationally for the business, and physically demanding for the worker-teams, the data analytics team and the attention management must apply to this problem. At the same time, though, data collection is also very costly, with an unknown need for sample size.

\hypertarget{lets-be-a-frequentist}{%
\subsection{Let's be a frequentist}\label{lets-be-a-frequentist}}

We start with the textbook frequentist inference.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We first set out the two opposing hypotheses. One is the \textbf{null} hypothesis that only 10\% of worker teams are experienced enough in hazardous materials handling practices. The other is that there are more than 10\% of worker teams so experienced.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Hypothesis: \(H_0\) is 10\% experienced workers and \(H_1\) is \textgreater{} 10\% experienced workers.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  We then set a tolerance level for being wrong about selecting the null hypothesis as true, when in fact it turns out is is false. A commonly chosen level of signficance is a 1 in 20 time rate of observing that we are wrong about choosing the null hypothesis over the alternative.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Significance level: wrong about supporting the null hypothesis in only 1 in 20 samplings for an \(\alpha = 0.05\).
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  We sample the 5 different worker-teams and find 1 experienced team.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Observed data: \(x=1\) experienced worker team in \(n=5\) teams.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  We calculate the probability that the data associated with the alternative hypothesis, namely, that there are 10\% or more experienced worker-teams, given the null hypothesis. This is called the \(p-value\), \emph{not to be confused with \(p=\) the proportion of successes, also known as the probability of a single success}. With the null hypothesis the probability of no experienced worker-teams, in a single sample, is \(1-0.10 = 0.90\).
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \(p-value\): \(Pr(x \geq 1 \mid n=5, p=0.10) = 1 - Pr(x=0 \mid n=5, p=0.10) = 1 - 0.90^5 \approx 0.41\)
\end{itemize}

We note the clever use of the complement to calculate a bit more easily the 5 sample both-and sequence of no successes at all with each no success 90\% probable under the null hypothesis. The \(p-value\) is just the probability of observing more extreme outcomes given that the null hypothesis is true. We recall the many admonitions to resist the idea that this is the probability of accepting as true the null hypothesis, or, for that matter, that this is the probability of accepting as false the alternative hypothesis. In other words we will viscerally attempt to avoid the word probability in our report. Instead we will think of the \(p-\)value as a score.

\begin{itemize}
\item
  We find that the data fails to reject \(H_0\) and we seem to conclude that the data do (plural \emph{data}, singular \emph{datum}) not provide evidence that the proportion of experienced worker-teams is greater than 10\%.
\item
  This means that if we had to pick between 10\% and 20\% for the proportion of experienced worker-teamss, even though this hypothesis testing procedure does not actually in any way at all confirm the null hypothesis.
\item
  We would likely stick with 10\% since we couldn't find evidence that the proportion of experienced worker-teams is greater than 10\%. That this evidence is convincing will depend on whether this conclusion is a statement of the \emph{justified true belief} of whomever consumes the results of this analysis.
\end{itemize}

\hypertarget{now-we-are-bayesians}{%
\subsection{Now we are Bayesians}\label{now-we-are-bayesians}}

The Bayesian inference works differently from the frequentist approach as we illustrate below.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We construct a grid of alternative, mutually-exclusive hypotheses. For example, we can start with 5\% and work in increments of 5\% up to, for example, 50\% unobserved proportions of experienced worker-teams in the population. This example would have us list 10 equally spaced vertices (nodes) with 9 edges (intervals). For our immediate purposes we will just use two alternatives to build our comparison: 10\% and 20\%. This step differs from the null-alternative hypothesis step in that neither of these alternatives is the null, they are agnostically alternative, that's it.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Unobserved data: \(H_1\) is \(p_1=10\%\), and \(H_2\) is \(p_2=20\%\) worker-teams.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Initially, we do not have any preference, or expectations for that matter, about the plausibility of one hypothesis over the other. We thus let them be equally probable. This is something of a conservative acid test.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Prior experience: \(Pr(H_1) = Pr(H_2) = 0.50\)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  We sample the 5 different worker-teams and find 1 experienced team. This is exactly the same step as in the frequentist approach.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Observed data: \(x=1\) experienced worker team in \(n=5\) teams.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  We will also use the same binomial observational model as with the frequentist approach. We can justify the use of this model with the notion that we can only observe the binary outcomes of experienced or not experienced (either \(E\) or \(\neg{E}\) - \emph{not} \(E\) - is true) in the repeated sampling (n = 5 here) of worker-teams. In repeated independent samplings of binary data, the appropriate theoretical distribution of successes is the binomial model. Again \(n\) is the number of independent samples.
\end{enumerate}

\begin{align}
E &\sim \operatorname{Binomial}(n, p) \\
p &\sim \operatorname{Uniform}(0,\, 1)
\end{align}

We will simplify this further below to have \(\operatorname{Pr}(p) = 0.5\), the mean of the uniform distribution with 0 minimum and 1 maximum.

\begin{itemize}
\tightlist
\item
  The likelihood of observing \(x=1\) experienced worker-team in \(n=5\) samplings. Across hypotheses, likelihoods are mutually exclusive but do not add up to 1.
\end{itemize}

\begin{align}
\operatorname{Pr}(x=1 | H_1: p_1=0.10) &= {5 \choose 1} \, (0.10)^1 \, (0.90)^4 \approx 0.33 \\
\operatorname{Pr}(x=1 | H_2): p_2=0.20) &= {5 \choose 1} (0.20)^1 \, (0.80)^4 \approx 0.41
\end{align}

\begin{itemize}
\tightlist
\item
  Posterior \(\operatorname{Pr}(H_i: \,p_i \mid n, x)\) for each of \(i=1, 2\) to infer plausibility of each hypothesis. To force likelihoods into a distribution we would have to normalize them by adding up the likelihoods and compute the contribution of each likelihood to the overall distribution of probability mass across alternative hypotheses. Effectively this is the denominator of the Bayesian posterior distribution formula. We are, after all, solving for the probability that each hypothesis, given the sample data, is true.
\end{itemize}

\begin{align}
\operatorname{Pr}(H_1: p_1=0.10 \mid x=1) &= \frac{\operatorname{Pr}(H_1)\operatorname{Pr}(x=1 \mid H_1: p_1=0.10)}{\operatorname{Pr}(x=1)} \\
&= \frac{(0.50)(0.33)}{(0.50)(0.33) + (0.50) (0.41)} \\
&= 0.44
\end{align}

and, again using the clever complement trick, we have

\begin{align}
\operatorname{Pr}(H_2: p_2=0.20 \mid x=1) &= 1 - 0.45 \\
&= 0.56
\end{align}

The posterior probabilities of whether \(H_1\) or \(H_2\) is correct are close to each other. As a result, with equal priors and a low sample size, it is difficult to make a decision with an unwavering strength of analytical resolve. given the observed data. However, \(H_2\) does have a higher posterior probability than \(H_1\), so if we had to make a decision at this point, we are somewhat justified to choose \(H_2\), i.e., that the proportion of experienced worker-teams is \(p_2=20\%\) is true is more plausible than the alternative. This choice directly contradicts the decision based on the frequentist approach.

This table summarizes what the results would look like if we had chosen larger sample sizes. Under each of these scenarios, the frequentist method yields a higher \(p\)-value than our significance level, so we would fail to reject the null hypothesis with any of these samples. On the other hand, the Bayesian method always yields a higher posterior for the second model where \(p\) is equal to 0.20. So the decisions that we would make are contradictory to each other.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{options}\NormalTok{( }\DataTypeTok{digits =} \DecValTok{2}\NormalTok{, }\DataTypeTok{scipen =} \DecValTok{999999}\NormalTok{)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\CommentTok{# we first build two vector lists of observed data}
\NormalTok{n <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{ , }\DataTypeTok{length.out =} \DecValTok{2}\NormalTok{ )}
\NormalTok{x <-}\StringTok{ }\KeywordTok{c}\NormalTok{( }\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\CommentTok{# we specify the false negative rate}
\NormalTok{null <-}\StringTok{ }\FloatTok{0.1}
\CommentTok{# here is an example calculation of the p-value interpreted as a probability if in a Bayesian setting}
\NormalTok{pr_gt_null <-}\StringTok{ }\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{-}\NormalTok{null)}\OperatorTok{^}\NormalTok{n}
\CommentTok{# next the various conjectures as hypotheses about the proportion of successes}
\NormalTok{nodes <-}\StringTok{ }\DecValTok{2}
\NormalTok{p_grid <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(.}\DecValTok{1}\NormalTok{, }\FloatTok{.2}\NormalTok{, }\DataTypeTok{length.out =}\NormalTok{ nodes)}
\CommentTok{# equally plausible hypotheses, aka the prior}
\NormalTok{prior <-}\StringTok{ }\DecValTok{1}\OperatorTok{/}\NormalTok{nodes}
\CommentTok{# likelihood <- dbinom(1, n[1], p_grid) * prior}
\CommentTok{# posterior <- likelihood / sum(likelihood)}
\NormalTok{post_calc <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, n, p_grid)\{}
\NormalTok{  likelihood <-}\StringTok{ }\KeywordTok{dbinom}\NormalTok{(x, n, p_grid) }\OperatorTok{*}\StringTok{ }\NormalTok{prior}
\NormalTok{  posterior <-}\StringTok{ }\NormalTok{likelihood }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(likelihood)}
\NormalTok{  posterior }\CommentTok{# the last result is what the function returns}
\NormalTok{\}}
\CommentTok{# use tidyr function to make a grid of all combinations of x and n and p_grid}
\NormalTok{data_grid <-}\StringTok{ }\KeywordTok{expand_grid}\NormalTok{( x, n, p_grid )}
\CommentTok{# first, compose a key and calculate the likelihood}
\NormalTok{post_table <-}\StringTok{ }\NormalTok{data_grid }\OperatorTok{|}\ErrorTok{>}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{key =} \KeywordTok{paste0}\NormalTok{( }\StringTok{"x="}\NormalTok{, x, }\StringTok{", "}\NormalTok{, }\StringTok{"n="}\NormalTok{, n),}
    \DataTypeTok{likelihood =} \KeywordTok{post_calc}\NormalTok{( x, n, p_grid )}
\NormalTok{  )}
\CommentTok{# next by unique data scenario, the key, compute the posterior}
\NormalTok{post_table <-}\StringTok{ }\NormalTok{post_table }\OperatorTok{|}\ErrorTok{>}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{( key ) }\OperatorTok{|}\ErrorTok{>}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{posterior =}\NormalTok{ likelihood }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(likelihood)}
\NormalTok{  ) }\OperatorTok{|}\ErrorTok{>}
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{( key, p_grid, posterior ) }\OperatorTok{|}\ErrorTok{>}
\StringTok{  }\KeywordTok{pivot_wider}\NormalTok{( }\DataTypeTok{names_from =}\NormalTok{ key, }\DataTypeTok{values_from =}\NormalTok{ posterior ) }
\CommentTok{# finally append the null hypothesis row p-values}
\CommentTok{# finally append the null hypothesis row p-values}
\NormalTok{data_null <-}\StringTok{ }\KeywordTok{expand_grid}\NormalTok{( x, n)}
\NormalTok{null_table <-}\StringTok{ }\NormalTok{data_null }\OperatorTok{|}\ErrorTok{>}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{p_value =} \DecValTok{1}\OperatorTok{-}\KeywordTok{pbinom}\NormalTok{( x}\DecValTok{-1}\NormalTok{, n, null)}
\NormalTok{  )}
\NormalTok{post_table <-}\StringTok{ }\NormalTok{post_table }\OperatorTok{|}\ErrorTok{>}
\StringTok{  }\KeywordTok{rbind}\NormalTok{( }\KeywordTok{c}\NormalTok{( null, }\KeywordTok{t}\NormalTok{(null_table)[}\DecValTok{3}\NormalTok{,] ))}
\KeywordTok{colnames}\NormalTok{( post_table )[}\DecValTok{1}\NormalTok{] <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"H"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:post-table}Frequentist and Bayesian probabilities for various sample sizes}
\centering
\begin{tabular}[t]{lrrrrr}
\toprule
probability & H & x=1, n=5 & x=1, n=10 & x=3, n=5 & x=3, n=10\\
\midrule
Pr(10\% experienced | n, x) & 0.1 & 0.44 & 0.59 & 0.14 & 0.22\\
Pr(20\% experienced | n, x) & 0.2 & 0.56 & 0.41 & 0.86 & 0.78\\
Pr(x or more | 10\% experienced) & 0.1 & 0.41 & 0.65 & 0.01 & 0.07\\
\bottomrule
\end{tabular}
\end{table}

However, if we had set up our framework differently in the frequentist method and set our null hypothesis to be \(p = 0.20\) and our alternative to be \(p < 0.20\), we might obtain different results. What a wonderful exercise it would be to instantiate this claim!

We would have simply exposed the degree of sensitivity of the frequentist method to the null hypothesis. On the other hand, with the Bayesian method our results would be the same regardless of which order we evaluate our models.

\hypertarget{interpreting-parameter-estimates}{%
\section{Interpreting parameter estimates}\label{interpreting-parameter-estimates}}

A common error in interpretation of parameter estimates is to suppose that because one parameter is sufficiently far from a target level of an indicator (what we might call \emph{significant}) and another parameter estimate (what we might label \emph{not significant}) that the difference between the parameters is also significant. But this is not necessarily so. This isn't just an issue for non-Bayesian analysis: If you want to know the distribution of a difference, then you must compute that difference, a contrast. It isn't enough to just observe, for example, that a slope among males overlaps a lot with zero while the same slope among females is reliably above zero. We must compute the posterior distribution of the difference in slope between males and females.

For example, suppose we have posterior distributions for two parameters, \(\beta_f\) and \(\beta_m\).

\begin{itemize}
\item
  We find that \(\beta_f\)'s mean adjusted for 1 standard deviation is \(0.15 \pm 0.02\) indicating that values of \(\beta_f\) fall far from zero. The paramter can reach a 1 standard deviation upper bound of \(0.15+0.02=0.17\) and a 1 standard deviation lower bound of \(0.15-0.02=0.13\), certainly a bit far away from zero.
\item
  This is while \(\beta_m\)'s mean-adjusted for 1 standard deviation is \(0.02 \pm 0.10\), and has a 1 standard deviation upper bound of \(0.02+0.10=0.13\) and a 1 standard deviation lower bound of \(0.02-0.10=-0.08\), definitely crossing zero into negative territory.
\item
  Not only that, but the two 1 standard deviation intervals do not even touch one another.
\end{itemize}

So while \(\beta_f\) is reliably different from zero ( some say \emph{significant}, and we will say \emph{credibly so}) and \(\beta_m\) does not seem to be, the difference between the two (assuming they are uncorrelated) is another story. The mean of the difference is just \(0.15-0.02=0.13\). One standard deviation is the sum of the variances (squared standard deviations) raised to the half-power (some will call this the square root).

\[
(0.15 - 0.02) \pm \sqrt{0.02^2 + 0.10^2} = 0.13 \pm 0.10.
\]

The distribution of the difference does not overlap with zero unlike one of its components: it goes from an upper boudn of 0.23 to a low er bound of 0.03. This difference interval never touches zero. In other words, each taken separately it seems that \(\beta_f\) is credibly far from zero, while it also seems credible that \(\beta_m\) can be possibly zero and even negative. Their difference seems to wash away the possibility that \(\beta_m\) might wander into negative territory, changing sign. But, unless we disaggregate the two parameters, we would, wrongly, seem to be somewhat sure that the difference between \(\beta_f\) and \(\beta_m\) is far from zero.

In the context of non-Bayesian significance testing, this phenomenon arises from the fact that statistical significance is inferentially powerful in only one way: difference from the null, from the status quo. However, when \(\beta_m\) overlaps with zero, it may also overlap with values very far from zero, even negative. Thus its value, and sign of influence, is highly uncertain. So when we then compare \(\beta_m\) to \(\beta_f\) , that comparison is also uncertain, which begins to show up in the width of the posterior distribution of the difference \(\beta_f - \beta_m\), 0.20 wide.

Lurking underneath this example is a more fundamental mistake in interpreting statistical significance: the mistake of accepting the null hypothesis as true, at all. Whenever an article, academic, journalistic, book, or podcast says something like ``we found no difference'' or ``no effect,'' this usually means that some parameter was not significantly different from zero, and so the authors might be tempted to adopt zero as the estimate. This is both illogical and extremely common. In our case, one of the two parameters, ,\(\beta_f\) would have to achieve and overtake a lower bound of zero with about 7 \(\times\) a standard deviation of \(0.02\). Its confrere \(\beta_m\) gets there in 1 shot of a standard deviation.

For us, one of the biggest advantages to the Bayesian approach is that it answers the right questions, in fact, all of the relevant questions at that. Within the Bayesian framework, it is perfectly sensible and allowable to refer to \emph{the probability that a hypothesis (any including the status quo) is true.} We can even try to calculate this probability. Ultimately, isn't that what we \emph{want} our statistical tests to tell us? To a human decision maker, this would seem to be the whole point of doing statistics: to determine what is true and what isn't, probably, plausibly. Any time that we aren't exactly sure about what the truth is, we can use the language of complementary probabilities to make statements like \emph{there is an 80\% chance that Theory A is true, but a 20\% chance that Theory B is true instead.}

This seems so obvious to a \emph{commonsensical person,} yet it is explicitly forbidden within the conventional textbook framework. \emph{To a frequentist, such statements are nonsensical} because the judgment that \emph{``the theory is true''} is not a repeatable event. A theory is true or it is not, and no probabilistic statements can be linguistically even allowed, no matter how much we might want to make them. \emph{It is a one-time binomial outcome with no recourse.} We can not interpret the \(p\)-value as the probability that the null hypothesis is true. There's a reason why almost every textbook on statistics is forced to repeat that warning. It's because people desperately \emph{want} that probability statement to be the correct interpretation.

Frequentist dogma notwithstanding, decades of teaching and consulting suggests to me that most of us think that \emph{``the probability that the hypothesis is true''} is not only meaningful, it's the thing we care \emph{most} about. It's such an appealing idea that even trained statisticians fall prey to the mistake of trying to interpret a \(p\)-value this way. For example, here is a quote from
\href{https://www.bls.gov/news.release/empsit.tn.htm\#:~:text=BLS\%20analyses\%20are\%20generally\%20conducted\%20at\%20the\%2090-percent,by\%2050\%2C000\%20from\%20one\%20month\%20to\%20the\%20next.}{a technical note from the U.S. Bureau of Labor Statistics regarding confidence intervals.}

\begin{quote}
Statistics based on the household and establishment surveys are subject to both sampling and nonsampling error. When a sample, rather than the entire population,is surveyed, there is a chance that the sample estimates may differ from the true population values they represent. The component of this difference that occurs because samples differ by chance is known as sampling error, and its variability is measured by the standard error of the estimate. There is about a 90-percent chance, or level of confidence, that an estimate based on a sample will differ by no more than 1.6 standard errors from the true population value because of sampling error. BLS analyses are generally conducted at the 90-percent level of confidence.
\end{quote}

Also BLS provides this example.

\begin{quote}
For example, the confidence interval for the monthly change in total nonfarm employment from the establishment survey is on the order of plus or minus 110,000. Suppose the estimate of nonfarm employment increases by 50,000 from one month to the next. The 90-percent confidence interval on the monthly change would range from -60,000 to +160,000 (50,000 +/- 110,000). These figures do not mean that the sample results are off by these magnitudes, but rather that \emph{\textbf{there is about a 90-percent chance that the true over-the-month change lies within this interval.}} Since this range includes values of less than zero, we could not say with confidence that nonfarm employment had, in fact, increased that month. If, however, the reported nonfarm employment rise was 250,000, then all of the values within the 90-percent confidence interval would be greater than zero. In this case, \emph{\textbf{it is likely (at least a 90-percent chance) that nonfarm employment had, in fact, risen that month.}}
\end{quote}

The emphasis is mine in these quotes. This is \emph{not} what a 95\% confidence means to a frequentist statistician. The bolded section is wrong. Orthodox methods cannot tell you that ``there is a 95\% chance that a real change has occurred'', because this is not the kind of event to which frequentist probabilities may be assigned. To an ideologically bent frequentist, this sentence should be meaningless.

For hypothesis testing the use of a \(p\)-value is often misinterpreted as the probability that the null hypothesis is true. We assume that \(H_0\) is true when we calculate the \(p\)-value. Because of this assumption the \(p\)-value simply cannot provide information regarding whether \(H_0\) is in fact true.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  This argument also shows that first, \(p\) also cannot be the probability that the alternative hypothesis is true.
\item
  Second, the \(p\)-value is highly sensitive to the sample size.
\item
  Third, it is not true that the \(p\)-value is the probability that any observed difference is simply attributable to the chance selection of observations from the target population.
\end{enumerate}

The \(p-value\) is calculated based on an assumption that chance is the only reason for observing any difference. Thus it cannot provide evidence for the truth of any statement.

On the other hand, let's suppose we are Bayesians. Although the bolded passage is the wrong interpretation of a frequentist confidence interval, it's exactly what a Bayesian means when they say that the posterior probability of the finding a parameter value between an lower and an upper value is 89\%, suitably redubbed a probability interval. If the Bayesian posterior is actually thing you \emph{want} to report, and what the consumer of your analytical product requires, why are you even trying to use orthodox frequentist methods? If you want to make Bayesian claims, all you have to do is be a Bayesian and use Bayesian tools.

Speaking for myself, I found this to be a the most liberating thing about switching to the Bayesian view many years ago, not in academia, but in professional consultation with decision makers in complex organizations. Once you've made the jump, you no longer have to wrap your head around counterintuitive definitions of \(p\)-values or confidence intervals. You don't have to bother remembering why you can't say that you're 95\% confident that the true mean lies within some interval. All we have to do is be honest about what we believed to be true and experienced, including the objective statement -- we do not really know!, before we run the study, and then report what we learned from doing it. Sounds nice, doesn't it? To me, this is the big promise of the Bayesian approach: we do the analysis we really want to do, and express what we really believe the data are telling us, all in a principled logically consistent manner.

\hypertarget{evidentiary-standards-you-might-believe}{%
\section{Evidentiary standards you might believe?}\label{evidentiary-standards-you-might-believe}}

\begin{quote}
\emph{If {[}\(p\){]} is below .02 it is strongly indicated that the {[}null{]} hypothesis fails to account for the whole of the facts. We shall not often be astray if we draw a conventional line at .05 and consider that {[}smaller values of \(p\){]} indicate a real discrepancy.}\\
-- Sir Ronald \citet{Fisher1925}
\end{quote}

Consider the quote above by Sir Ronald Fisher, one of the founders of what has become the orthodox approach to statistics. If anyone has ever been entitled to express an opinion about the intended function of \(p\)-values, it's Fisher. In this passage, taken from his classic guide \emph{Statistical Methods for Research Workers}, he's pretty clear about what it means to reject a null hypothesis at \(p<.05\). In his opinion, if we take \(p<.05\) to mean there is ``a real effect'', then ``we shall not often be astray.'' This view is hardly unusual: in my experience, most practitioners express views very similar to Fisher's. In essence, the \(p<.05\) convention is assumed to represent a fairly stringent evidentiary standard.

How true is this a fairly stringent standard of evidence? One way to approach this question is to try to convert \(p\)-values to Bayes factors. The Bayes factor summarizes the consistency of any information provided by observed data with one hypothesis versus another, a binary decision problem. \citet{Jeffreys1961} suggested that we interpret the Bayes factor \(B\) (also known as the likelihood ratio or \(B^{-1}\) ) in half-units on the scale of \(log_{l0}\).\footnote{See the extensive implementation of Bayes factors at \url{https://richarddmorey.github.io/BayesFactor/}}

\citet{Jeffreys1961} uses 4 cut points of \((1/\sqrt{10})^\alpha\) for \(\alpha = 1,\ldots,4\) to discriminate high and low degrees of data compatibility with hypotheses. This is perhaps an apples to oranges comparison since a \(p\)-value since they simply do not measure the same thing at all derived in completely different frameworks.

However, there have been some attempts to work out the relationship between the two, and it's somewhat surprising. For example, \citet{Johnson2013} presents a compelling case that (for \(t\)-tests at least) the \(p<0.05\) threshold rejecting the null corresponds roughly to a Bayes factor of somewhere between 3:1 and 5:1 rejecting the null, and we are allowed to say also accepting the alternative hypothesis. If that's right, then Fisher's claim is a bit of a stretch.

Let's suppose that the null hypothesis is true about half the time (i.e., the prior probability of \(H_0\) is 0.5), and we use those numbers to work out the posterior probability of the null hypothesis given that it has been rejected at \(p<.05\). Using the data from \citet{Johnson2013}, we see that if you reject the null at \(p<.05\), you'll be correct about 80\% of the time. I don't know about you, but in my opinion an evidentiary standard that ensures you'll be wrong on 20\% of your decisions isn't good enough. The fact remains that, quite contrary to Fisher's claim, if you reject at \(p<0.05\) you shall quite often go astray. It's not a very stringent evidentiary threshold at all.

\hypertarget{exercising-our-wits}{%
\section{Exercising our wits}\label{exercising-our-wits}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Redo the entire frequentist-Bayesian exercise using the idea that results might be different with a frequentist null hypothesis of \(H_0: \, p_0 = 0.20\). How and why are our conclusions any different? {[}\textbf{Hint}: Which variable(s) would you alter?{]}
\item
  Let's try something different than data that uses the binomial distribution. We observe these monthly (successful!) launches of SpaceX rockets: 2, 5, 9. Again, as with the binomial example, we are initially completely agnostic about the existing distribution of potential launches. Instead of using the binomial distribution, we use a distribution appropriate for count data in mashing hypotheses with data. As frequestists, we want to test the belief that there are 3 launches on average with a significance level of 1 in 20 times that we are wrong about this hypothesis. As a Bayesian we test this belief again against an alternative hypothesis that launch patterns are drawn from a population with mean 6 launches per month. What are the odds that the alternative wins the launch control center's bet that they can beat 3 launches per month? {[}\textbf{Hint:} we might consider the Poisson distribution.{]}
\end{enumerate}

\hypertarget{hypothetically-speaking}{%
\chapter{Hypothetically Speaking}\label{hypothetically-speaking}}

\hypertarget{imagine-this-1}{%
\section{Imagine this\ldots{}}\label{imagine-this-1}}

Our team is about to contract for coir. We make activated carbon for filtration systems. We have a bet going on: half the team believes that coir prices are low, the other half high. Both sides have stated what they think is a low versus a high price based on procurement experience and trades in the market for coir, FOB Indonesia.

We formulate these two hypotheses.

\begin{align}
H_{high}:&\,\, \mu=b=330.\, with \, \operatorname{Pr}(H_{high}) = p \\
H_{low}:&\,\, \mu=a=230, \, with \, \operatorname{Pr}(H_{low}) = 1-p
\end{align}

Each hypothesized mean coir price can occur with the Jakob Bernoulli jump probabilities \(p\) and \(1-p\). In previous episodes, all we did was flip the up-down machine a number of times to get at the binomial probability. This time we know the magnitudes of the jump. The standard deviation \(\sigma=30\) betrays the riskiness of this market and the size of the jump as well.

What we are really trying to do? Suppose we observe a price of \(y=290\). We ask is this a high regime price or a low one.

Under \(H_{low}\), prices average USD 230/mt so that observed prices \(y=Y\) will be distributed as \(Y \mid H_{low}âˆ¼N(230, 30^2)\). If we believe this then we have this \(\operatorname{Pr}(data = y \mid hypothesis = H_{low})\).

\[
\operatorname{Pr}(y \mid H_{low})= \frac{1}{\sigma \sqrt{2\pi}}e^{-\left(\frac{(y-230)^2}{2\sigma^2}\right)}
\]

Yes, the Gaussian robot is back. We have our two observational models of coir prices. Which one is more plausible? Which one is more consistent with the observation of coir prices, really of one coir price USD 290/mt. Here is the second one model at stake when prices average a high level of \$330/mt.

\[
\operatorname{Pr}(y \mid H_{high})= \frac{1}{\sigma \sqrt{2\pi}}e^{-\left(\frac{(y-330)^2}{2\sigma^2}\right)}
\]

For both \(\sigma=30\). Will that \(\sqrt{2\pi}\) hang around? We will see soon enough.

For each hypothesized average coir price and probability of that hypothesis even occurring, it behooves us to pick the hypothesis that is most plausible. We have a both-and statement looming in the mist. We look at how plausible it is that both the event \(y=290\) occurs and the belief that \(H_{low}\) occur together. We know that this means we must multiply the two probabilities.

For the joint distribution of data \(y\) and \(H_{low}\) we have this relation.

\begin{align}
\operatorname{Pr}( (y=290) \wedge (H_{low}:\, \mu=230) ) &= \operatorname{Pr}(y \mid H_{low}) \operatorname{Pr}(H_{low}:\, \mu=230) \\
                               &= \left(\frac{1}{\sigma\sqrt{2\pi}}e^{-\left(\frac{(y-230)^2}{2\sigma^2}\right)}\right) \times (1-p)
\end{align}

Cutting to the chase we calculate this high regime version of the both-and probability.

\begin{align}
\operatorname{Pr}( (y=290) \wedge (H_{high}:\, \mu=330) ) &= \operatorname{Pr}(y \mid H_{high}) \operatorname{Pr}(H_{high}:\, \mu=230) \\
                               &= \left(\frac{1}{\sigma\sqrt{2\pi}}e^{-\left(\frac{(y-330)^2}{2\sigma^2}\right)}\right) \times p
\end{align}

As ominous as all of this looks all we want to find out is if \(H_{high}\) is more plausible than \(H_{low}\) then this simple relationship must be true.

\begin{align}
\operatorname{Pr}( (y=290) \wedge (H_{high}:\, \mu=330) ) &\geq \operatorname{Pr}( (y=290) \wedge (H_{low}:\, \mu=230) ) \\
\left(\frac{1}{\sigma\sqrt{2\pi}}e^{-\left(\frac{(y-330)^2}{2\sigma^2}\right)}\right) \times p &\geq \left(\frac{1}{\sigma\sqrt{2\pi}}e^{-\left(\frac{(y-230)^2}{2\sigma^2}\right)}\right) \times (1-p)
\end{align}

As bestly as the formulas look, we do know how to calculate then! In Excel each term in the large parenthesis is just NORM.DIST(y, \(\mu\), \(\sigma^2\), FALSE ), where FALSE calculates the probability mass. We get this result after a short bit of time inside Excel.

\begin{itemize}
\item
  \textbf{\(H_{high}\):}The left-hand side is \texttt{NORM.DIST(290,\ 330,\ \$30\^{}2\$,\ FALSE\ )*0.5\ =\ 0.0055\ \ \ \ *\ 0.5000\ =\ \ 0.0027}.
\item
  \textbf{\(H_{low}\):} The right-hand side is \texttt{NORM.DIST(290,\ 230,\ \$30\^{}2\$,\ FALSE\ )*0.5\ =\ 0.0018\ *\ 0.5000\ =\ 0.0009}
\end{itemize}

The high:low odds ratio is 0.0027:0009 = 3:1 in favor of the high price regime for coir prices observed at a level of \$290/mt.

What is the breakeven, that is even odds of 1:1, threshold price? It turns out to be in this simple case of agnostic, indifference to one or the other hypothesis, that it is the simple average of the two hypothesized means, \((330+230)/2=280\). Any observed price above this threshold favors a high price regime, and otherwise classifies the observed price as a low price.

In the next section we take our algebraic life into our hands and deduce, using our armory of albegraic tools and vast experience, to verify our results here. After that we implement the model into an Excel worksheet and continue our quest for more knowledge.

\hypertarget{for-those-who-really-want-to-or-even-need-to}{%
\subsection{For those who really want to, or even need to}\label{for-those-who-really-want-to-or-even-need-to}}

We can bow to the formulae and balance and reduce to a, perhaps, surprisingly simple result, algebraicly. We will also do all of this with a low price mean of \(a\) and a high price mean of \(b\). In this way we will have a general formula for any binary decision with Gaussian noise in it.

We start with the question we posed in the last section. What threshold favors a high price regime? The answer we propose is the hypothesis whose odds ratio is greater than 1, alternatively whose joint probability of observed data and hypothesis is the greater. We start right off with the big beasts of burden Gaussian inequalities.

\begin{align}
\left(\frac{1}{\sigma\sqrt{2\pi}}e^{-\left(\frac{(y-b)^2}{2\sigma^2}\right)}\right) \times p &\geq \left(\frac{1}{\sigma\sqrt{2\pi}}e^{-\left(\frac{(y-a)^2}{2\sigma^2}\right)}\right) \times (1-p) \\
exp{\left[-\left(\frac{(y-b)^2}{2\sigma^2}\right)\right]} &\geq exp{\left[-\left(\frac{(y-a)^2}{2\sigma^2}\right)\right]} \times \left(\frac{1-p}{p}\right) \\
exp{\left[-\left(\frac{(y-b)^2}{2\sigma^2}\right) + \left(\frac{(y-a)^2}{2\sigma^2}\right)\right]} &\geq \left(\frac{1-p}{p}\right) \\
\frac{(y-a)^2 - (y-b)^2}{2\sigma^2} &\geq log\left(\frac{1-p}{p}\right) 
\end{align}

As is the story with anything Gaussian we end up with a quadratic term. We used these algebraic moves to get the last inequality. We also save some notional angst with \(log_e() = log()\) throughout, sometimes \(ln()\) is used as well.

\begin{itemize}
\item
  Multiply both sides by \(\sigma\sqrt{2\pi}\) and anything multiplied by 1 is itself.
\item
  For base \(e\), or any base for that matter, \(e^{-x}/e^{-y}=e^{-x}e^{y}=e^{-x+y}\) and the fact that \(1/e^{z}= e^{-z}\).
\item
  Again for base \(log_e (e^x) = x\) and whatever we do to one side of the inequality we must do to the other (if there are no negative multiplications or divisions involved).
\end{itemize}

Let's stop there! On the right is the logarithm of the odds in favor of \(H_{low}:\, \mu = a\) versus \(H_{high}:\, \mu = b\) flipping the ratio on the left-hand side. Let's not be too misled by the use of the \(log()\) function. After all logs are the same function (although to base 10) that we measure decibels and wonder how loud the music can go before we lose our hearing! We have seen odds ratios before. Here again to overdo the point we use the logarithm to the base \(e=2.712\ldots\). The \(log()\) function just scores the ratios.

On the left-hand side we need to reduce the numerator to this polynomial.

\begin{align}
\frac{(y-a)^2 - (y-b)^2}{2\sigma^2} &= \frac{(y^2 - 2ay + a^2) - (y^2 - 2by + b^2)}{2\sigma^2} \\
                      &= \frac{(2b-2a)y + (a^2 - b^2)}{2\sigma^2} \\
                      &= \frac{2(b-a)y + (a-b)(a+b)}{2\sigma^2} \\
                      &= \frac{2(b-a)y - (b-a)(a+b)}{2\sigma^2} \\
                      &= \frac{2(b-a)\left(y - \frac{a+b}{2}\right)}{2\sigma^2} \\
                      &= \left(\frac{b-a}{\sigma^2}\right)\left(y - \frac{a+b}{2}\right)
\end{align}

The algebraic insight here was to realize that \((a^2-b^2)=(a-b)(a+b)=-(b-a)(a+b)\) a trick in much use by quadratic afficionados. The \(b-a\) term is just the range between the low and high hypothesized means. The \((a+b)/2\) term is the arithmetic average of the two hypothesized means. Doesn't this look suspiciously like some sort of uniform distribution magic? No, not magic, but the interplay of Gaussian, and the other distributions, with the uniform distribution building block. It's really like Legos(tm).

We drop this result into the seeming quagmire of our derivation next where We then multiply both sides by \(\frac{\sigma^2}{b-a}\) and add \(\frac{a+b}{2}\) to get much simpler result, almost a gem.

\begin{align}
\left(\frac{b-a}{\sigma^2}\right)\left(y - \frac{a+b}{2}\right) &\geq log\left(\frac{1-p}{p}\right) \\
\left(y - \frac{a+b}{2}\right) &\geq \left(\frac{\sigma^2}{b-a}\right) log\left(\frac{1-p}{p}\right) \\
y &\geq \frac{a+b}{2} + \left(\frac{\sigma^2}{b-a}\right) log\left(\frac{1-p}{p}\right)
\end{align}

There's that arithmetic average at the head of the line. It is followed by an important correction for the strength of our convictions about one or the other price regime. If we favor the high price regime then we subtract from the breakeven. This has the effect of curtailing the range of the low price regime while expanding the primacy of the high price regime. We illustrate this here.

\begin{figure}
\centering
\includegraphics{images/12/coir-thresholds-log-odds.jpg}
\caption{Coir log odds thresholds.}
\end{figure}

The table of varying \(\operatorname{Pr}(H_{high}=p)\) yields the log odds ratio \(log((1-p/p))\) to the measured on the secondary y-axis to the right. As we assign greater plausibility to the high regime hypothesis, we also reduce the odds of the low regime hypotheses. When we reduce the low regime hypothesis below the break even \(p=0.5\) level, we begin to subtract from the breakeven threshold, \(\theta=280\). The opposite happens when we assign greater credibility to the high regime hypothesis.

To belabor the point further, here are some numbers we shouold into the threshold formula! We know that \(b=330\), \(a=230\), \(\sigma=30\) and \(p=0.5\). That the \(\operatorname{Pr}(H_{high})=\operatorname{Pr}(H_{low})\) eliminates that potentially nasty looking, but very helpful we will see, \(log((1-p)/p) = log(0.5/0.5) = log(1) = 0\) term. All we are left with, in the equal \(sigma\) case we have here is this very simple decision rule.

\begin{align}
y &\geq \frac{a+b}{2} \\
  &\geq \frac{230+330}{2} \\ 
  &\geq 280
\end{align}

We did it and it probably did not take too many years off our lives as well. What does this inequality say to us? We choose the low price regime whenever an observed price, or an average of observed prices falls below USD 280/mt net of a correction for the dispersion of prices, \(\sigma\), and taking into account experience with the occurrence of low versus high price regimes, \(p\).

In fact, if we are agnostic, ignorant, or just do not care about whether one or the other hypothesis has ever existed in the field, then we would set \(p=0.5\). The logarithm of the ratio of \(1-p\) to \(p\) is then conveniently zero. We are left with \(y \leq 280\). Any price greater than 280 is consistent with the high price regime centered at USD b/mt.

We have thus derived our first statistical decision rule. We will therefore statistically \textbf{classify} any observed coir price as a \textbf{high price} if we observe that the price is greater than (or equal to) USD280/mt. Otherwise we classify the observed price as a \textbf{low price}. Done!

This is our first foray into a machine learning model, the binary classification model. Yes, to learn is to infer. Here we inferred with an abundance of probability. We now venture into the deeper end of the pool by considering a numerical implementation of this model in Excel. We will ask questions like what would happen if the \(\sigma\)s are different between the two classes of prices? What happens to the threshold if we vary the probabilities that the hypotheses are reasonable in the first place? What would happen if we used the Poisson or binomial or Student's t distributions instead, and why? More to come with a graph or two and certainly a bunch of tables to ponder.

\hypertarget{finally-an-excel-screenshot}{%
\subsection{Finally an excel screenshot}\label{finally-an-excel-screenshot}}

We can build a simple two model (two means, two standard deviation) selection criterion. Yes, you read it right, two standard deviations. In our exhausting reliving of elementary algebra memories we only used one standard deviation. We can rederive the result algebraicly for two or more means and standard deviations should we desire to. It is indeed a good exercise like solving the London Times cross-word puzzle, in ink. Here is our one standard for two models example.

\includegraphics{images/12/coir-hypo-test-gauss.jpg}
We first notice that the plausibility split lands on the \(H_low\) hypothesis, even when both hypotheses are equally plausible. In this case the observed price \(y=270\) is below the decision threshold \(280 - (30^2/100)log(0.5/0.5)=280\) so that some common sense reigns. If we were to observe \(y=290\) exactly the opposite occurs and we land squarely in the high price regime.

\begin{figure}
\centering
\includegraphics{images/12/coir-hypo-test-high.jpg}
\caption{Coir test in the high price regime.}
\end{figure}

The odds clearly no longer favor the low price regime.

When we set the observed price to \(y=280\) we see this result.

\begin{figure}
\centering
\includegraphics{images/12/coir-hypo-test-indifferent.jpg}
\caption{Coir indifference?}
\end{figure}

The odds are even. The decision maker would, statistically only speaking, be indifferent to high or low regimes. We discover an insight here: even odds (1:1) mean both regimes are equally plausible so that the decision threshold is the one value that makes the odds even.

Our next experiment is to set the low price \(\sigma_{low}=0.15\), a phenomenon often observed in commodity markets. Not much activity happens at low price levels due sometimes to a relative lack of arbitrage bid-ask price widths. We use our insight about even odds to find the decision threshold in this practical situation.

We can use Data \textgreater{} what if \textgreater{} Goal Seek to find the price we would have to observe to be the decision threshold.

\begin{figure}
\centering
\includegraphics{images/12/coir-hypo-test-goal-seek.jpg}
\caption{Coir seeking goals.}
\end{figure}

Mathematically what is happening here is a one variable optimization.

\[
min_{y} |OR - 1.00| \\
such\,\,that \\
p = 0.5
\]

We choose \(y\) to minimize the absolute value of difference between the odds ratio \(OR\) and 1.00. There are several implicit constraints embedded in the odds ratio, not the least of which is the calculation of the numerator and denominator of the odds ratio. We focus on the important \(p=0.5\), the so-called uninformative indifference to the two hypotheses. By pressing OK we find this not so surprising result.

\begin{figure}
\centering
\includegraphics{images/12/coir-hypo-test-low-sigma.jpg}
\caption{Coir low \(\sigma\) scenario.}
\end{figure}

The decision threshold is lower than the equal \(\sigma\) case by over \$13. It makes sense since the high price regime is spread out more than the low price regime.

One more thought experiment presents itself. This time we use the low \(\sigma\) threshold of \(y=266.38\) with equal \(\sigma=30\) for each regime. This allows us to find the probability \(p\) of ever observing the low price regime.\footnote{This is not a \emph{p-value}! It is a true probability in an inference sense.} We minimizes the absolute difference between the odds ratio and 1.00 under the constraint that the decision threshold is 266.38. So many words that collapse into this mathematical expression.

\[
min_{p} |OR - 1.00| \\
such\,\,that \\
y = 266.38
\]

\begin{figure}
\centering
\includegraphics{images/12/coir-hypo-test-low-p.jpg}
\caption{Solving for a break-even probability.}
\end{figure}

In this model we discover complementary roles for \(\sigma\) and \(p\). A low value of \(\sigma_{low}=15\) carries the same level of information about the low price regime hypothesis that a low value of \(p = \operatorname{Pr}(H_{low})=0.18\) carries. They are equivalent measures.

In this model the claim that the low price regime is less risky, less variable, less noisy than the high price regime is that same thing as saying that we find the low price regime less credible than the high price regime in general. We are indeed biased to the high price regime in our decison making. We would, on average and in expectation, even prefer the high price regime to the low price regime if \(\operatorname{Pr}(H)\) has any meaning at all. Decision preferences are right at the edge of what we can expect this model to do for us.

\hypertarget{can-we-be-wrong}{%
\section{Can we be wrong?}\label{can-we-be-wrong}}

One more concern. We might want to know what is the probability that the low regime price hypothesis is true even though we chose the high regime, for example if we observe a price, say of 290. this is the statement \(y \mid H_{low} > 266.38\) for the mixed standard deviation / high price preference data. This table and graph zooms into the critical intersection of the high and low price regime distributions.

\begin{figure}
\centering
\includegraphics{images/12/coir-hypo-test-low-sigma.jpg}
\caption{Coir at the intersection.}
\end{figure}

The tail we are concerned with is the blue-hatched nearly triangular shaped area to the right of the dotted line and below the blue low regime normal density curve. We should also note that the low and high regime densities cross each other at the threshold and where the odds ratio is exactly 1. This tail is also the probability of the high regime price given a low regime decision. It is the probability we might be wrong about choosing low versus high. Other have called this a sort of \textbf{p-value}, but its not quite that.

We can calculate the tail's cumulative probability as the area under the low price regime probability density curve in the tail beyond the threshold 266.38. This expression will compute the amount for us: 1 - NORM.DIST( 266.38, 230, 30, TRUE ) = 0.0074, where TRUE means we are to compute the cumulative probability up to 266.38.

There is only a less than 1\% probability that the low regime is consistent with the observation of prices above the threshold. We will probably use this threshold in our decision making. We will also use the idea that if we observe a price below the threshold that price is indeed low, with about 99\% probability.

The opposite red-hatched triangular tail is the area to the left of the dotted threshold line at 266.38 and, this time, underneath the high price distribution. We can calculate, and interpret this area too as the cumulative probability of seeing a price below 266.38 given a high price regime point of view: `NORM.DIST( 266.38, 330, 30, TRUE ) = 0.014, This may or may not be too high for the risk intolerant among the analysts and decision makers. Or, they may take this information into consideration in making a contract for high priced coir.

\hypertarget{yet-another-way}{%
\section{Yet another way}\label{yet-another-way}}

Conventional hypothesis testing reaches back about 100 years. The goal then was to supply researchers with relatively easy to compute and apply tests to what has been called \emph{significance} of a finding. There is a whole theory of knowledge, cognitional operations, logic, epistemology, and methodology behind this approach, and just a little bit of controversy over the past decade as well.

Up to this point we have used what we might call a superset of techniques a subset of which is the conventional hypothesis testing, and confidence (really -- plausibility) interval estimations embedded in the methodology. Both techniques are are the same when the probability of any one hypothesis is the same as any other hypothesis. The results may be interpreted differently, but in the end our supposition is that decision makers hanker for some measure of the plausiblity of a claim, a conjecture, a hypothesis. We have done this the entire course of our investigation into probabilistic reasoning.

Here are the mechanics of a binary hypothesis testing method using conventional techniques without regard to explicit measure of the probability of a hypothesis.

\hypertarget{population-standard-deviation-known}{%
\subsection{Population standard deviation known}\label{population-standard-deviation-known}}

As with confidence and credibility intervals, we sample repeatedly in our experiments from a population. We calculate sampled means and we know these are Gaussian, normally, distribution is a mean of the sampled means equal to the population mean and a standard deviation equal to the population standard deviation divided by the square root of the sample size.

Hypotheses face off with data. There are two hypotheses in the 1928 approach by Neyman and Pearson. First there is noise only. It might be Gaussian, or Poisson generated, but it is the status quo. This is called the \textbf{null hypothesis} \(H_0\), which we believe is true. Against this hypothesis is the speculative, \textbf{alternative hypothesis} \(H_A\) or \(H_1\), which we are trying to refute, since it is speculative after all. The benefit of the doubt is given to the null hypothesis in this approach.

Two errors are possible. Here we build more into the tails of the overlapping distributions we experienced before.

\begin{itemize}
\item
  \textbf{Type I error}, also known as a \emph{false positive} is when we reject a null hypothesis when it is actually true. This is the error of accepting an alternative hypothesis (the real hypothesis of interest) when the results can be attributed to chance. We think we are observing a difference in the two hypothesis when in truth there is none, probably so.
\item
  Type II error, also known as a \emph{false negative} occurs when the error of not rejecting a null hypothesis when the alternative hypothesis is the true state of nature. We fail to accept an alternative hypothesis when we don't have adequate power. We are fail to observe a difference when in truth there is one.
\end{itemize}

\begin{table}
\centering
\begin{tabular}{l|l|l}
\hline
  & Null is true & Null is not true\\
\hline
Reject  Null & OK & False Negative\\
\hline
Keep Null & False Positive & OK\\
\hline
\end{tabular}
\end{table}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Type I Error}: the False Negative means the researcher did not retrieve relevant information.
\item
  \textbf{Type II Error}: the False Positive means that the research retrieved irrelevant information.
\end{enumerate}

How can the company control for error? Let's be more specific. How can the company ensure that the processed coconut price is as accurate as possible?

\hypertarget{control-is-probability}{%
\subsection{Control is probability}\label{control-is-probability}}

Here is what the company does:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Management makes an assumption and forms a hypothesis about the average price of processed coconuts found in searches of contracts and other market documents. This is a precise statement about a specific metric. Here the metric is the average price of processed coconut per metric ton, \(\mu\), Suppose this target level is USD 1000/mt.
\end{enumerate}

\begin{itemize}
\item
  The \emph{null hypothesis} (\(H_0\)) is that the population metric equals a target value \(\mu_0\) or \(H_0: \mu = \mu_0\). Suppose that \(H_0: \mu = 1000\). This is business as usual. Those who favor this hypothesis would hope that the only difference is prices is random.
\item
  The \emph{alternative hypothesis} (\(H_1\)) is that the population metric does not equal (or is just greater or less than) the target value. Thus we would have \(H_1: \mu \neq 1000\). These speculators think that prices will rise or possibly fall. Their detractors scowl at the thought.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Corporate policy sets a degree of confidence in accepting as true the assumption or hypothesis about the metric. The company determines that 95\% of the time \(\mu = 1000\). This means there is an \(\alpha =\) 5\% significance that the company would be willing to be wrong about rejecting that \(H_0: \mu = 1000\) is true.
\end{enumerate}

\begin{itemize}
\item
  Under the null hypothesis it is probable that above or below a mean value of 1000 there is an error of \(\alpha = 0.05\) in total, or \(\alpha / 2 = 0.025\) above and \(\alpha / 2 = 0.025\) below the mean. This practically means that management is willing to have only a 1 in 20 chance of being wrong about their view of the business, that is about the null hypothesis.
\item
  Because management expresses the alternative hypothesis, \(H_1: \mu \neq 1000\), as ``not equal'' then this translates into a two-tailed test of the null hypothesis.
\end{itemize}

\textbf{What if management expressed the alternative hypothesis as \(H_1 > 1000\)?}

show / hide

\leavevmode\hypertarget{myDIV1}{}%
If \(H_1: \mu > 1000\), then management in effect specifies a \textbf{one-tailed} test.

This means that management believes that under the null hypotheses \(H_0:\, \mu = 0\), that the distribution of documents per day for which the null hypothesis is probably true extends from the current price all the way up to the 95\%tile of occurrences of the level of prices.

The region of the distribution beyond the 95\%tile is 5\% of the time and represents the highest range of prices.

Similarly for the \(H_1:\,\mu<1000\) case.

\hypertarget{on-to-the-unknown}{%
\subsection{On to the unknown}\label{on-to-the-unknown}}

Let's suppose we \emph{do not know} the population standard deviation. Now the sampled standard deviation is also a random variable, like the sampled mean. In practice this is nearly always the case. What do we do now?

\begin{itemize}
\item
  Use the Student's t distribution to correct for small sample errors as well as the idea that sampled standard deviations are random processes themselves, just like the sampled means used to compute the sampled standard deviations. Quite a web we are weaving!
\item
  Here's a plot (again) of the Student's t overlaid with the normal distribution.
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-34-1.pdf}

What do we notice?

\begin{itemize}
\item
  Normal is more pinched in than t (kurtosis? right!)
\item
  t has thicker tails than normal
\item
  Let's check that: in Excel use \texttt{=T.INV(2.5\%,3)} which returns \texttt{-3.18}, and where the degrees of freedom \(df\) of our 4 sample prices from our work in confidence intervals is \(df = n - k = 4 - 1 = 3\). Here \(n\) is the sample size of 4 randomly sampled prices and \(k\) is the number of estimators we are building, just one in this case \(\mu\).
\item
  Thus for the t distribution it takes 3.18 standard deviations below the mean to hit the 2.5\% level of cumulative probability. It would only take 1.96 standard deviations on the normal distribution.
\item
  There are \(k=3\) degrees of freedom because it only takes 3 out of the 4 sampled prices to get the fourth sampled price (we do this by using 1 estimator, the mean we calculated).
\item
  That it took fewer standard deviations for the normal than for the t distribution to hit the 2.5\% level of cumulative probability means that the t distribution is thicker tailed than the normal.
\end{itemize}

\includegraphics{_main_files/figure-latex/unnamed-chunk-35-1.pdf}

\hypertarget{on-with-our-story}{%
\subsection{On with our story\ldots{}}\label{on-with-our-story}}

When management does not know the population standard deviation, the analyst must use the Student's t distribution to correct for small sample sizes. As this is almost always the case for hypothesis testing, management has decreed that the Student-t distribution will be used for hypothesis testing.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  CONTINUED --- management decides on regions of the distribution for acceptance that the null hypothesis is probably true and for rejection of the null hypothesis as well. This picture tells those and about 900+ more words.
\end{enumerate}

\includegraphics{_main_files/figure-latex/unnamed-chunk-36-1.pdf}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  Suppose management can take a random sample of \(n = 100\) prices. An analyst then computes the sample average \(\overline{X} = 980\) of prices (USD/mt, that is) with a standard deviation of \(s = 80\), meant to represent the very unknown population \(\sigma\).
\item
  They then compute the \(t\) score, just like the z-score for the normal distribution:
\end{enumerate}

\[
t = \frac{\overline{X} - \mu_0}{s / \sqrt{n}} = \frac{980 - 1000}{80 / \sqrt{100}} = -2.5
\]

and compare this value with the the acceptance region of the null hypotheses \(H_0\). So, what is this value?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  For a sample size of \(n = 100\) and \(k = 1\) estimator (\(\overline{X}\)), the degrees of freedom \(df = n - k = 100 - 1\). Under a Student's t distribution with 99 \(df\), and using Excel's \texttt{=T.INV(0.025,\ 99)}, the region is bounded by t scores between \(-1.98\) and \(+1.98\).
\end{enumerate}

\begin{itemize}
\item
  The computed t score is -2.5 and falls in the rejection region of the null hypothesis.
\item
  The analyst can report that it is 95\% plausible that the data rejects the null hypothesis that the market is holding steady at USD 1000/mt.
\item
  Another way of reporting this might be that there is a 5\% probability that the data is compatible with the null hypothesis.
\end{itemize}

\hypertarget{exercises-1}{%
\section{Exercises}\label{exercises-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  An electric car manufacturer buys aluminum alloy sheets of 0.05 of an inch in thickness. Thick sheets are too heavy and thin sheets imbalance the axle loads on icy and rainy road surfaces. The purchasing officer along with a manufacturing engineer samples 100 sheets of a shipment before accepting it and calculates an average of 0.048 inches in thickness with a standard deviation of 0.01 of an inch.
\end{enumerate}

\begin{itemize}
\item
  At a 5\% level of being wrong, or in error, should the purchasing officer accept the shipment?
\item
  What is the probability that the purchasing officer is wrong about rejecting the null hypothesis?
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  A real estate developer is comparing construction wages in two markets. In New York, using a random sample of 150 workers, the average daily wage is \$1,800 with a standard deviation of \$500 per day. In Los Angeles, for the same skills and experience, a random sample of 125 workers yields a daily wage average of \$1,700 per day with a standard deviation of \$450 per day.
\end{enumerate}

\begin{itemize}
\item
  Is there a significant difference in wage levels between the two cities at the 5\% level? This is the same thing as asking if there are two distinct sub-markets at work.
\item
  What is the probability of being wrong about rejecting the null hypothesis if we were to use this approach?
\end{itemize}

\hypertarget{topic-3-the-test-of-a-simple-relationship}{%
\chapter*{Topic 3 -- The Test of a Simple Relationship}\label{topic-3-the-test-of-a-simple-relationship}}
\addcontentsline{toc}{chapter}{Topic 3 -- The Test of a Simple Relationship}

This topic will pull us through two models. One is a waiting time model which is a template for any binary classification approach. The other is a simple linear regression, built on top of the mechanics of the waiting time model. We will encounter lots of algebra here as we use a deductive expectational algebra to develop measures that matter. The math is the same we learned from middle through high school. If it is tedious, at least we can build strength in this suffering! They matter because they unlock the degree of relationship between two variables in a statistical causal model. The example is wages and education from data stored at the St.~Louis Fed.

Yet another simple regression model allows us to examine the relationship between the Terrain Ruggedness Index and per capita GDP in a sample of continguous west African countries. Hypothesis testing and credibility intervals; exploratory data analysis; expectational models of metrics that matter with grid approximations; Gaussian, Poisson, Binomial distributions; relative and cumulative frequency, nay, probability, tables and plots; contingency tables and conditional, logically consistent, probability; extensive use of dynamic titles in plots, INDEX and MATCH querying of data, and the basics of spreadsheet hygiene with named ranges, formula documentations with FORMULATEXT, formatting, one worksheet for each task. Yes, we covered a lot of ground!

\hypertarget{relationships-put-to-the-test}{%
\chapter{Relationships Put to the Test}\label{relationships-put-to-the-test}}

\hypertarget{its-not-so-hard-to-imagine-this}{%
\section{It's not so hard to imagine this\ldots{}}\label{its-not-so-hard-to-imagine-this}}

There are lines everywhere! At the store, in cafeterias, waiting for buses, trains, our friends to pick us up, the line formed by the process of doing homework, baking a cake, getting an insight. All of this takes time. Waiting times exploit our sense of satisfaction and accomplishment. We desire short waiting lines especially when we do not prefer to be in the line in the first place. The opposite happens when we listen to deeply moving music, regard a dramatically poignant painting: we want the moment to last.

Our team is about to assist the College's administration of vaccinations on campus. Organizers have tasked us with analyzing the results of the duration of time students, faculty, and staff stand in line waiting for a vaccination. The times have been observed in morning and afternoon shifts.

\hypertarget{binary-waiting-time}{%
\section{Binary waiting time}\label{binary-waiting-time}}

One way of thinking about waiting times is this model.

\[
\mu_{i} = \alpha_{i} + \beta_{i}A_i
\]

where \(\mu_i\) is the average waiting time in minutes at vaccination station \(i\), \(\alpha_i\) is the average morning waiting time, \(\beta_i\) is the average difference in morning and afternoon waiting times, and \(A_i\) is a zero/one indicator of whether we are in the afternoon shift, 1, or present ourselves to the morning shift, 0.

THe model seems simple enough to understand, and possibly communicate to others. Waiting times, we hypothesize, depend upon a single factor, whether the test occurs in the morning, or in the afternoon. If \(A_i=0\), then we observe a waiting time average in the morning only. The average waiting time is this.

\begin{align}
\mu_{i} \mid (A_i=0) &= \alpha_{i} + \beta_{i}(0) \\
\mu_{i} \mid (A_i=0) &= \alpha_{i}
\end{align}

Only the intercept \(\alpha_i\) matters in this conditional expression.

Given that we observe waiting times in the afternoon, then \(A_i = 1\).

\begin{align}
\mu_{i} \mid (A_i=1) &= \alpha_{i} + \beta_{i}(1) \\
\mu_{i} \mid (A_i=1) &= \alpha_{i} + \beta_{i}
\end{align}

This allows us to add the average waiting time differential for the afternoon \(\beta_i\) for each station \(i\) to the morning average waiting time \(\alpha_i\).

Here is the management problem we might face. If we observe a waiting time of 5 minutes, should we label this a morning-style of waiting? By style, we now abstract from chronological notions of morning and afternoon. We release ourselves from the idea of a clock. We now have two possible regimes: one that looks like a typical chronological morning, the other that mimics an afternoon.

We now focus on the problem of deciding, probabilistically speaking of course, whether a relatively high wait time mimics a morning session or an afternoon session. The implications might influence staffing, scheduling, availability of vaccines on hand, size of facility, effect of weather on waiting lines, and so on, and so forth.

We will focus on a single vaccination station to work out the kinks of our model, and of our thinking. All of this allows us to specify these mutually exclusive hypotheses, at least logically so.

\begin{align}
H_{PM}:&\,\, \mu_{PM} = \alpha + \beta, \, &&&with \, Pr(H_{PM}) &= p \\
H_{AM}:&\,\, \mu_{AM} = \alpha, \,        &&&with \, Pr(H_{AM}) &= 1-p
\end{align}

While logically mutual exclusivity exists under certainty, we can imagine that distributions of \(\mu_{PM}\) and \(\mu_{AM}\) might overlap in probability. The hypotheses are two classifications of a waiting time. There are two of them and thus we often use the term binary classification to describe what we are to do next. We believe also that the \(PM\) regime is \(p\) probable, so that the \(AM\) shift must be \(1-p\) probable.\footnote{We could do very well to check out this spreadsheet implementation of an entropic, almost Bayesian, approach to classification: \url{https://pubsonline.informs.org/doi/10.1287/ited.1100.0060} What is entropy? From thermodynamic irregularities to the delivery of anaesthesia \url{https://clinicalview.gehealthcare.com/quick-guide/entropy-monitoring-valuable-tool-guiding-delivery-anesthesia}, entropy can measure chaos. It has a lot to do with the \(log((1-p)/p)\) portion of the 1:1 odds threshold we discussed.}

Let's insert data into this otherwise very theoretical story. Here is an spreadsheet rendering of these hypotheses and the (Bayesian) binary classification model.

\begin{figure}
\centering
\includegraphics{images/13/waiting-time-am-pm-all.jpg}
\caption{Waiting time classification.}
\end{figure}

The cross-over of the two distributions is fairly high up the frequency axis. The probability to the left of the 1:1 threshold 3.1188 under the AM distribution (blue) curve is much larger than the probability under the red PM distribution curve. Any observations of waiting times greater than 3.1188 would most probably be AM shifts, and any less are PM timings.

So ends our initial foray into conditioning a variate and building hypotheses. The condition is of the simplest kind, 0 or 1. The expected values of waiting times depend on the rules of the conditioning conveyed by an intercept and slope. The model naturally yields the two hypotheses.

\hypertarget{moving-continuously-now}{%
\section{Moving continuously now}\label{moving-continuously-now}}

We now wonder what might happen if the conditioning was more like the wages \(W\) and educational attainment \(E\) model. We hypothesize that wages depend on educational level. We will use a straight-line model again.

\[
\mu_W = \alpha + \beta E
\]
We retrieve wages \(W\) as normally distributed with mean \(\mu_W\) and \(\sigma_W\).

\[
W \sim \operatorname{N}(\alpha + \beta E ,\, \sigma_W)
\]

We believe there is some sort of dependency, at least an association of some sort between \(W\) and \(E\). We might measure this with correlation \(\rho\). We might also wonder what comes of \(\alpha\) and \(\beta\) in the face of \(\rho\). We might as well throw in \(\mu_E\) and \(\sigma_E\) while we are at it. That's our next job.

\hypertarget{the-maths-the-maths-1}{%
\section{The maths! The maths!}\label{the-maths-the-maths-1}}

We can fuss about all we want about the maths, but they are impervious to our feelings. They remain. We can stay, or go. If we stay, and spend the time in active pursuit (just like a waiting time, waiting for insight, or not, waiting for Godot, who never shows up), we might achieve a learning apogee. We suppose that we will stay awhile, for the time being. Now let us dig into our model of waiting times. Our first stop is a set of tools we will need for the excavation.

In what follows we use \(Y\) as the wage, the metric we want to generate from its mean and standard deviation. We conjecture that \(Y\) depends on \(X\), the level of educational attainment through the conditional mean of \(Y \mid X\), just like we did with vaccination waiting times.

\hypertarget{what-did-we-all-expect-1}{%
\subsection{What did we all expect?}\label{what-did-we-all-expect-1}}

We define expectations as aggregations of two kinds of information. One is the information provided by an array of outcomes \(Y_i\) for \(i=1 \ldots N\), where \(i\) indexes \(N\) outcomes. The other is the array of probabilities assigned to each outcome \(\pi_i\). The frequentist will assign \(\pi = f_i/N\), where \(f_i\) is the long-run frequency of occurrence of outcome \(i\). Instead, with \citet{Jeffreys1966} and \citet{Jaynes2004} we will assign \(\pi\) as a normalized index of the logical plausibility of an outcome where all \(\pi\)s add up to one and each is somewhere between 0 and 1. After all we can't wait for the long-run, we have to go to school!

This allows us to interpret probability as an extension of logic, where probability quantifies the reasonable expectation that everyone (even a \emph{robot} or \emph{golem} ) who shares the same knowledge ( experience, understanding, \emph{and} judgment) should share in accordance with the rules of conditional probability.\footnote{Cox's theorem}(\url{https://en.wikipedia.org/wiki/Cox\%27s_theorem}) provides a logical underpinning to this statement: the rules of probability theory need not be derived from a definition of probabilities as relative frequencies (frequentist approach). He goes further to show that the properties of probability as logic but also follow from certain properties one might desire of any system of plausible reasoning about uncertainty. \href{https://reader.elsevier.com/reader/sd/pii/S0888613X03000513?token=7D6FBDB41F8831DD4DE22162DFA2FAA3A8786A9B0CAB6231C759CF725E203785164FE0BAE53EFCB46143DF016439BCD6\&originRegion=us-east-1\&originCreation=20211123164351}{Van Horn} is a tutorial on Cox's approach. Plausible reasoning may be illustrated with this example of the distinction between gradual degrees of possible outcomes, that is, uncertainty, and what is or is not, that is, truth. As an example, one's confidence in the statement \emph{Daniel is well over six feet tall}, after seeing Daniel, legs splayed out, sitting at a desk, is a degree of plausibility. In contrast, the statement \emph{Daniel is tall} may be somewhat true (if Daniel measures five feet eleven inches and the definition of tall is greater than or equal to six feet) or entirely true (if Daniel measures seven feet one inch).{]} All of this ensures we have a complete picture of all of the probability contributions, as weights, of each outcome consistent with a systematic, principled way to reason about uncertainty, as we mash together data and hypotheses about the data.

The aggregation we propose is then this expression for the expectation \(E\) of outcomes \(Y\).

\begin{align}
\operatorname{E}Y = \sum_{i}^{N} \pi_i Y_i
\end{align}

In this way we can say that \(\operatorname{E}\) operates on \(Y\) where \textbf{to operate} means \textbf{to aggregate} several outcomes \(X_i\) into one number (or possibly function) by multiplying probability weights times outcomes and then summing the products. That's really two operations combined into the expectation operation. And so goes the maths!

Using this idea of an operator \(\operatorname{E}Y\) means we define the aggregation as this expression.

\begin{align}
\operatorname{E} = \sum_{i}^{N} \pi_i \times
\end{align}

Here are some of the algebraic rules of the road when we use this highly condensed short-hand notation.

\begin{align}
Y &= \alpha\,X \\
\operatorname{E}Y &= \operatorname{E}[\alpha\,X] \\
                  &= \sum_{i}^{N}[\pi_i\,(\alpha\,X_i) ] \\
                  &= \pi_1\,\alpha\,X_1 + \ldots \pi_N\,\alpha\,X_N \\
                  &= \alpha \, (\pi_1\,X_1 + \ldots \pi_N\,X_N) \\
                  &= \alpha\,\sum_{i}^{N}[\pi_i\,(X_i) ] \\
                  &= \alpha\,\operatorname{E}X
\end{align}

This means that we can take the constant \(\alpha\) outside of the expectation operator. All we did, step by step on the logical staircase, is to use the definition of the operator and then manipulate it algebraicly to deduce an equivalent expression.

If \(X_1=1, \ldots, X_N=1\), and the sum of probabilities \(\sum_{i}^N \, \pi_i = 1\), then we can deduce this expression.

\begin{align}
Y &= \alpha\,X \\
\operatorname{E}Y &= \operatorname{E}[\alpha\,X] \\
                  &= \sum_{i}^{N}[\pi_i\,\dot (\alpha\,X_i) ] \\
                  &= \pi_1\,\alpha\,(1) + \ldots \pi_N\,\alpha\,(1) \\
                  &= \alpha \, (\pi_1\,(1) + \ldots \pi_N\,(1) \\
                  &= \alpha\,\sum_{i}^{N}[\pi_i (1)] \\
                  &= \alpha\,\operatorname{E}1 \\
                  &= \alpha
\end{align}

This may have been immediately clear to some of us before the 7 step deduction, but we might find it reassuring that the deduction verifies, and perhaps validates, our initial conjecture. We also discover another relationship.

\[
\operatorname{E}1 = 1
\]

In algebra we call this the identity operator. For any number or variable, or even another expectation, \(\alpha\), then this is true.

\begin{align}
\alpha \, \operatorname{E}1 &= \alpha\, 1 \\
                            &= \alpha
\end{align}

Yes, this is identity under a multiplication. Is there a zero? Yes, \(\operatorname{E}0 = 0\), the identity operator under addition. Anything added to \(\operatorname{E}0=0\) just returns itself.

What is the expectation of a sum of variables \(X\) and \(Y\)?

\begin{align}
Z &= X+Y \\
\operatorname{E}Z &= \operatorname{E}[X + Y] \\
                  &= \sum_{i}^{N}[\pi_i\,(\,X_i + Y_i) ] \\
                  &= \pi_1\,\,(X_1 + Y_1) + \ldots \pi_N\,(X_N+Y_N) \\
                  &= (\pi_1\,X_1 + \ldots \pi_1\,X_N) + (\pi_N\,Y_N + \ldots \pi_N\,Y_N)  \\
                  &= \sum_{i}^{N}[\pi_i\,(X_i) ] + \sum_{i}^{N}[\pi_i\,(Y_i) ] \\
                  &= \operatorname{E}X + \operatorname{E}Y
\end{align}

The expectation of a sum of outcome variables is the sum of the expectations of each variable.

We just examined a sum of two variables, so it behooves us to look at the product of two variables.

\begin{align}
Z &= XY \\
\operatorname{E}Z &= \operatorname{E}[XY] \\
                  &= \sum_{i}^{N}[\pi_i\,(\,X_i\,Y_i) ] \\
                  &= \pi_1\,X_1 \, Y_1 + \ldots \pi_N\,X_N\,Y_N) \\
                  &= \operatorname{E}XY
\end{align}

Alas, we have reduced this operation to its simplest expression already. If \(Y=X\), going through the same steps as above we find this out.

\begin{align}
if\,\,Z &= XY \\
and \\
Y &= X \\
then \\
Z&= XX\\
\operatorname{E}Z &= \operatorname{E}[XX] \\
                  &= \sum_{i}^{N}[\pi_i\,(\,X_i\,X_i) ] \\
                  &= \pi_1\,X_1 \, X_1 + \ldots \pi_N\,X_N\,X_N) \\
                  &= \pi_1\,X_1^2 + \ldots \pi_N\,X_N^2 \\
                  &= \operatorname{E}X^2
\end{align}

It turns out that we can take an expression like this, \(Y=\alpha + \beta\,X\), multiply it by \(X\) and, then operate on it with \(\operatorname{E} = \sum_{i}^{N} \pi_i \times\) with the tools we now possess.

\begin{align}
Y &=\alpha + \beta\,X \\
XY &= \alpha\,X + \beta\,XX \\
XY &= \alpha\,X + \beta\,X^2 \\
\operatorname{E}XY &= \operatorname{E}[\alpha\,X + \beta\,X^2] \\
                   &= \operatorname{E}[\alpha\,X] + \operatorname{E}[\beta\,X^2] \\
                   &= \alpha\,\operatorname{E}[X] + \beta\,\operatorname{E}[X^2]
\end{align}

This will be very useful indeed. We usually will call \(\operatorname{E}X = \mu_X\) in honor of the \textbf{mean} of the population of all possible realizations of \(X\). We already know this as the weighted average of \(X\) outcomes, where the weights are probabilities, all of which add up to 1. What about \(\operatorname{E}X^2\)? To ponder this we consider the calculation of another very familiar metric, the square of the standard deviation, which has been dubbed the \textbf{variance}. We start with the definition and use all of the new tricks up our sleeves. We define variance as the probability weighted average of squared deviations of outcomes from the expected outcome.

We will need the remembrance of things in our algebraic past that look like this.

\begin{align}
(a + b)^2 &= (a + b)(a + b) \\
          &= a^2 + 2ab + b^2
\end{align}

In what follows \(a = X\) and \(b = -\operatorname{E}X\). We will also need to remember that \(-2b^2 + b^2 = -b^2\).

\begin{align}
define \\
\sigma_X^2 &= Var(X) \\
then \\
Var(X) &= \operatorname{E}(X - \operatorname{E}X)^2 \\
       &= \operatorname{E}(X^2 - 2X\operatorname{E}X + \operatorname{E}X^2) \\
       &= \operatorname{E}X^2 - \operatorname{E}[2X\operatorname{E}X] + \operatorname{E}[\operatorname{E}X^2] \\
       &= \operatorname{E}X^2 - 2(\operatorname{E}X)^2 + (\operatorname{E}X)^2 \\
       &= \operatorname{E}X^2 - (\operatorname{E}X)^2 \\
       &= \operatorname{E}X^2 - \mu_X^2 \\
       thus \\
\sigma_{X}^2 &= \operatorname{E}X^2 - \mu_X^2 \\
rearranging \\
\operatorname{E}X^2 &= \sigma_{X}^2 + \mu_X^2
\end{align}

Yes, we can breathe a collective sigh of relief having accomplished these algebraic acrobatics! But we must now move on to the \emph{piece de resistance} , \(\operatorname{E}XY\). We start with the definition of covariance, for this is where an \(XY\) product resides.

\begin{align}
define \\
\sigma_{XY} &= Cov(X, Y) \\
then \\
Cov(X, Y) &= \operatorname{E}(X - \operatorname{E}X)(Y - \operatorname{E}Y) \\
          &= \operatorname{E}(XY - X\operatorname{E}Y - Y\operatorname{E}X + \operatorname{E}X\,\operatorname{E}Y) \\
          &= \operatorname{E}(XY - \operatorname{E}X\,\operatorname{E}Y  - \operatorname{E}Y\,\operatorname{E}X + \operatorname{E}X\,\operatorname{E}Y) \\
          &= \operatorname{E}XY - 2\operatorname{E}X\,\operatorname{E}Y + \operatorname{E}\,X[\operatorname{E}Y \\
          &= \operatorname{E}XY - \operatorname{E}X\,\operatorname{E}Y \\
          thus \\
\sigma_{XY} &= \operatorname{E}XY - \mu_X\mu_Y \\
rearranging \\
\operatorname{E}XY &= \sigma_{XY} + \mu_X\mu_Y
\end{align}

Now we can go to work on our model with one more stop: solving a simultaneous equation. This tool too will come in handy. We suppose we have the following two equations in \(a\) and \(b\). We will use the row-column convention of subscripts. Thus coefficient \(c_{12}\) will be in row 1, column 2 of a matrix. First the two equations.

\begin{align}
c_{11}a + c_{12}b &= d_1 \\
c_{21}a + c_{22}b &= d_2
\end{align}

In matrix form this is a very tidy arrangement like this.

\begin{align}
\begin{bmatrix}
c_{11} & c_{12}  \\
c_{21} & c_{22}
\end{bmatrix}
\begin{bmatrix}
a  \\
b
\end{bmatrix}
&=
\begin{bmatrix}
d_1  \\
d_2
\end{bmatrix} \\
\mathrm{C}\mathrm{a} &= \mathrm{d}
\end{align}

Very tidy indeed! We might remember that a unique solution exists only if (or is it if and only if?) the determinant of the matrix \(\mathrm{C}\) is not zero. If it is, then the solution \(\mathrm{a}=\mathrm{C}^{-1}d\) does not exist and the model is singular. In what we will do below we will compose our coefficients of means, standard deviations and correlations. Some combinations of these aggregations, constants, will prove to yield a zero determinant, and a singular model results.

The determinant \(\det{\mathrm{C}}\) is

\[
\det{\mathrm{C}} = c_{11}c_{22}-c_{12}c_{21}
\]

The solution proceeds in two sweeps, one for each of \(a\) and \(b\). In the first sweep we replace the first, the \(a\) column, in \(\mathrm{C}\) with the column vector \(d\). We find the determinant of this new \(\mathrm{C}_a\) matrix and divide by \(\det{\mathrm{C}}\). Here we go.

\begin{align}
original \, \, &\mathrm{C} \\
\begin{bmatrix}
c_{11} & c_{12}  \\
c_{21} & c_{22}
\end{bmatrix} \\
swap\,\, out\,\, &first\,\, column \\
\mathrm{C}_a &=
\begin{bmatrix}
d_{1} & c_{12}  \\
d_{2} & c_{22}
\end{bmatrix} \\
then \\
a &= \frac{\det{\mathrm{C_a}}}{\det{\mathrm{C}}} \\
  &= \frac{d_1c_{22}-d_2c_{12}}{c_{11}c_{22}-c_{12}c_{21}}
\end{align}

Now the second sweep in all its glory.

\begin{align}
original \, \, &\mathrm{C} \\
\begin{bmatrix}
c_{11} & c_{12}  \\
c_{21} & c_{22}
\end{bmatrix} \\
swap\,\, out\,\, &first\,\, column \\
\mathrm{C}_b &=
\begin{bmatrix}
c_{11} & d_{1}  \\
c_{21} & d_2
\end{bmatrix} \\
then \\
b &= \frac{\det{\mathrm{C_b}}}{\det{\mathrm{C}}} \\
  &= \frac{c_{11}d_2-c_{21}d_1}{c_{11}c_{22}-c_{12}c_{21}}
\end{align}

Very much a formula for the ages.

\hypertarget{walking-the-straight-line-1}{%
\subsection{Walking the straight line}\label{walking-the-straight-line-1}}

Here is our model where both \(Y\) and \(X\) have some distribution with \(\pi\) probabilities for each. Here we use \(\pi\) as the Greek letter for \(p\), not as the \(\pi\) of circle fame. Both \(Y\) and \(X\) are what we will very loosely call \textbf{random variables}, because they have outcomes with associated probabilities of occurrence.

\[
Y = \alpha + \beta\, X
\]

We now ask the question, what is \(\operatorname{E(Y \mid X=x)=\mu_{Y \mid X}}\)? What, on weighted average, can we expect \(Y\) to be? First of all, this must be true.

\begin{align}
if \\
\operatorname{E}(Y \mid X=x) &= \mu_{Y \mid X} \\
then \\
\mu_{Y \mid X} &= \operatorname{E}(\alpha + \beta\, X) \\
                &= \operatorname{E}\alpha (1) + \operatorname{E}(\beta\,X) \\
                &= \alpha\,\operatorname{E}1 + \beta\,\operatorname{E}X \\
                &= \alpha\,(1) + \beta\,\mu_X \\
                &= \alpha + \beta\,\mu_X
\end{align}

Result one is in hand, \(\mu_{Y \mid X}= \alpha + \beta\,\mu_X\) is a true statement according to our many deductions. By the way the statement \(\mu_{Y \mid X} = \alpha\,\operatorname{E}1 + \beta\,\operatorname{E}X\) is an example of the distributive property of multiplication over addition.

Now for our second trick we multiply \(Y\) by \(X\) to get a second result and a second true statement. We will condense \(Y \mid X = Y\) to save what's left of our eyesight. We remember all of our hard work above, especially this inventory of results.

\begin{align}
\operatorname{E}Y &= \mu_{Y} \\
\operatorname{E}X &= \mu_{X} \\
\operatorname{E}X^2 &= \sigma_{X}^2 + \mu_X^2 \\
\operatorname{E}XY &= \sigma_{XY} + \mu_X\mu_Y
\end{align}

Using this inventory more than a few times we get these results.

\begin{align}
Y &= \alpha + \beta\, X \\
then \\
XY &= \alpha\,X + \beta\, XX \\
   &= \alpha\,X + \beta\, X^2 \\
so\,\,that \\
\operatorname{E}XY &= \operatorname{E}(\alpha\,X + \beta\, X^2) \\
                &= \operatorname{E}\alpha\,X + \operatorname{E}\beta\,X^2 \\
                &= \alpha\,\operatorname{E}X + \beta\,\operatorname{E}X^2 \\
                &= \alpha\,\mu_X + \beta\,(\sigma_X^2 + \mu_X^2) \\
but\,\,we\,\,know\,\,that \\
\operatorname{E}XY &= \sigma_{XY} + \mu_X\mu_Y \\
thus,\,\, again \\
\sigma_{XY} + \mu_X\mu_Y &= \alpha\,\mu_X + \beta\,(\sigma_X^2 + \mu_X^2)
\end{align}

We now have two equations in two, as yet to be determined, unknowns. They are unobserved data, \(\alpha\) and \(\beta\). Both equations are true, and true jointly. This means we can stack one on top of the other as a simultaneous equation system and, we hope this time, solve them for unique values of \(\alpha\) and \(\beta\). Yes, we demand a formula!

Here are the two equations with \(\alpha\) and \(\beta\) terms on the left-hand side and constant terms, the expectations are all constant aggregations, on the right-hand side of the equation. We also commutes the terms so that our unknowns are pre-multiplied by coefficients.

\begin{align}
\alpha + \mu_X\,\beta &= \mu_Y \\
\mu_X\,\alpha + (\sigma_X^2 + \mu_X^2)\,\beta &= \sigma_{XY} + \mu_X\mu_Y
\end{align}

The matrix representation will help us easily match coefficients with our simultaneous equation model, way above as we replicate below.

\begin{align}
\begin{bmatrix}
c_{11} & c_{12}  \\
c_{21} & c_{22}
\end{bmatrix}
\begin{bmatrix}
a  \\
b
\end{bmatrix}
&=
\begin{bmatrix}
d_1  \\
d_2
\end{bmatrix} \\
\mathrm{C}\mathrm{a} &= \mathrm{d}
\end{align}

Our simultaneous equations of expected values for the linear model \(Y=\alpha+\beta X\) yields this structure.

\begin{align}
\alpha + \mu_X\,\beta &= \mu_Y \\
\mu_X\,\alpha + (\sigma_X^2 + \mu_X^2)\,\beta &= \sigma_{XY} + \mu_X\mu_Y \\
becomes \\
\begin{bmatrix}
1     & \mu_X  \\
\mu_X & \sigma_X^2 + \mu_X^2
\end{bmatrix}
\begin{bmatrix}
\alpha  \\
\beta
\end{bmatrix}
&=
\begin{bmatrix}
\mu_Y  \\
\sigma_{XY} + \mu_X\mu_Y
\end{bmatrix} \\
\mathrm{C}\mathrm{a} &= \mathrm{d}
\end{align}

We can solve for the unobserved, unknown, and otherwise conjectured (we might smell a hypothesis brewing here) \(\alpha\) and \(\beta\) using our trusty determinant solutions.

\begin{align}
\alpha &= \frac{\mu_Y(\sigma_x^2 + \mu_X^2) - \mu_X(\sigma_{XY} + \mu_X\mu_Y)}{\sigma_X^2 + \mu_X^2 - \mu_x^2} \\
       &= \frac{\mu_Y\sigma_X^2 -  \mu_X\sigma_{XY}}{\sigma_X^2} \\
       &= \mu_Y - \mu_X\frac{\sigma_{XY}}{\sigma_X^2}\\
       and\,\, then \\
\beta &= \frac{\det{\mathrm{C_{\beta}}}}{\det{\mathrm{C}}} \\
  &= \frac{c_{11}d_2-c_{21}d_1}{c_{11}c_{22}-c_{12}c_{21}} \\
  &= \frac{\sigma_{XY} + \mu_X\mu_Y - \mu_X\mu_Y}{\sigma_X^2 + \mu_X^2 - \mu_x^2} \\
  &= \frac{\sigma_{XY}}{\sigma_X^2}
\end{align}

Yeow! All that work to get at this simplification all due to the wonderful result that \(\alpha\) has \(\beta = \sigma_{XY}/\sigma_X^2\) in it.

\begin{align}
\operatorname{E}(Y \mid X) &= \alpha + \beta\,X \\
\operatorname{E}(Y \mid X) &= \left(\mu_Y - \mu_X\frac{\sigma_{XY}}{\sigma_X^2}\right) + \frac{\sigma_{XY}}{\sigma_X^2}\,X \\
rearranging\,\,terms\\
\operatorname{E}(Y \mid X) &= \mu_Y + \frac{\sigma_{XY}}{\sigma_X^2}(X - \mu_X)
\end{align}

The second formulation is also the basis for the vaunted Capital Asset Pricing Model in finance, where \(Y\) is the return on a security (stock, bond, etc.) and \(X\) is the return on a market index (e.g., S\&P 500).

We have, yes, one more stop, before we drop. The definition of correlation is here.

\[
\rho = \frac{\sigma_{XY}}{\sigma_X\,\sigma_Y}
\]

We can use this definition to rearrange the deck chairs on this Titanic of a beast of gnarly maths (all algebra! and nary a faint odor of calculus?).

\begin{align}
if \\
\rho &= \frac{\sigma_{XY}}{\sigma_X\,\sigma_Y} \\
then \\
\sigma_{XY} &= \rho\,\sigma_X\,\sigma_Y\\
thus \\
\beta &= \frac{\sigma_{XY}}{\sigma_X^2} \\
      &= \frac{\rho\,\sigma_X\,\sigma_Y}{\sigma_X^2} \\
      &= \frac{\rho\,\sigma_Y}{\sigma_X}
\end{align}

We need numbers, \emph{stat(im)}! But we should hold on. One more calculation to make. After we have the mean, but what about the conditional standard deviation?

\hypertarget{a-short-variance-diversion-1}{%
\subsection{A short variance diversion}\label{a-short-variance-diversion-1}}

Here we take the standard deviation as given, perhaps at our peril. The variance of waiting times is

\[
Var(Y \mid X) = (1-\rho^2)\sigma_Y^2
\]

How do we get this? A lot easier than the preceding. There are no simultaneous equations to worry about. Here's the algebra for the stout-hearted.

\begin{align}
Var(Y \mid X) &= \operatorname{E}[Y - \operatorname{E}(Y \mid X)]^2 \\
              &= \operatorname{E}[Y - (\mu_Y + \frac{\sigma_{XY}}{\sigma_X^2}(X - \mu_X))]^2 \\
              &= \operatorname{E}[(Y- \mu_Y) + \frac{\sigma_{XY}}{\sigma_X^2}(X - \mu_X)]^2 \\
              &= \operatorname{E}[(Y- \mu_Y)^2 +         \frac{\rho_{XY}^2\sigma_Y^2}{\sigma_X^2}(X - \mu_X)^2 - 2\frac{\rho_{XY}\sigma_Y}{\sigma_X}(Y- \mu_Y)(X - \mu_X)] \\
              &= \operatorname{E}[(Y- \mu_Y)^2] +         \frac{\rho_{XY}^2\sigma_Y^2}{\sigma_X^2}\operatorname{E}[(X - \mu_X)^2] - 2\frac{\rho_{XY}\sigma_Y}{\sigma_X}\operatorname{E}[(Y- \mu_Y)(X - \mu_X)] \\
              &= \sigma_Y^2 + \frac{\rho_{XY}^2\sigma_Y^2}{\sigma_X^2} \sigma_X^2 - 2\frac{\rho_{XY}\sigma_Y}{\sigma_X}(\rho_{XY}\sigma_X\,\sigma_Y) \\
              &= \sigma_Y^2 - \rho_{XY}^2\sigma_Y^2 \\
              &= (1 - \rho_{XY}^2)\sigma_Y^2
\end{align}

Done! Yes, really.

If the joint distribution of \(X\) and \(Y\) is Gaussian, then we can generate \(Y \mid X \sim \operatorname{N}( \alpha + \beta X, (1 - \rho_{XY}^2)\sigma_Y^2)\). Now we can infer \(Y\) behavior.

\hypertarget{does-education-matter}{%
\section{Does education matter?}\label{does-education-matter}}

Well it certainly should! Finally, some numbers? Suppose this is the data and an Excel trendline through the scatter plot for wages and education from way long ago.

\begin{figure}
\centering
\includegraphics{images/13/wages-educ-data-plot.jpg}
\caption{Wages and education scatterplot.}
\end{figure}

Excel would calculate an intercept \(\alpha = -35.47\), and slope \(\beta=3.834\).

Here we calculate intercepts and slopes based on the expectation of wages conditional on education level.

\begin{figure}
\centering
\includegraphics{images/13/wage=educ-calculations.jpg}
\caption{Wages and education calculations.}
\end{figure}

The calculations align exactly with Excel's view of the universe. Why not just rely on Excel? My main reason is that Excel only uses the equivalent of equally likely priors for hypotheses (the \(\alpha\), \(\beta\) and \(\sigma\) choices) and only Gaussian distributions through a mean square error criterion for parameter choices. What if we have a different opinion about priors? What if life, at least as represented by a geocentric view, is not Gaussian?

\hypertarget{back-to-the-business-at-hand}{%
\section{Back to the business at hand}\label{back-to-the-business-at-hand}}

We used all of that math, and a small diatribe about Excel and Gauss, to understand the ins and outs of conditional expectations. The condition influences at least the expectation, and so it also influences the conditional standard deviation. Here we take the standard deviation as given, again as we said before perhaps at our peril. The variance of wages \(W\) conditional (\(\mid\)) on education \(E\) is

\[
Var(Y \mid X) = (1-\rho^2)\sigma_Y^2
\]

\emph{Here's the question before us:} if we spend more years being educated, do we have a higher wage? We can mold this question into two hypotheses to put a point on it. How do wages compare between 18 years of schooling (4 years of post-secondary education) and 16 years (0 years of post-secondary education)?

\begin{align}
H_{college}:&\,\, \mu_{college} &= \alpha + \beta (16), \, &&&with \, Pr(H_{college}) &= p \\
H_{high school}:&\,\, \mu_{highschool} &= \alpha + \beta (12), \, &&&with \, Pr(H_{highschool}) &= 1-p
\end{align}

Let's compute some parameters.

\begin{figure}
\centering
\includegraphics{images/13/wage=educ-calculations.jpg}
\caption{Wages and education calculations, again.}
\end{figure}

It is now a matter of depositing these values into our hypothesis testing model, the one we used with waiting times, modified for wages and education.

\begin{figure}
\centering
\includegraphics{images/13/wage-educ-hypo-test-1.jpg}
\caption{Wages and education - two state comparison.}
\end{figure}

We have a perfectly symmetrical solution for the equal probability, even odds, experiment. Consistent with this sample only, 16 years of education seems to work for a wage less than \$18.20/hour. Any wage greater than that is consistent with 18 years of education, in this sample, and probably so.

Oh, and yes, education matters in the sense of the consistent of hypotheses with this very small, perhaps not very representative, sample, and at least financially in wage rates, probably so.

\hypertarget{does-it-really-matter}{%
\section{Does it really matter?}\label{does-it-really-matter}}

Here is another binary hypothesis test. And we see that hypothesis testing is what we have been doing since day one with voting. It is all about comparing two or more models, two or more conjectures. One is not favored over the other. They stand in Goethian\footnote{We might, in another course on another planet, compare our standard approach of positivist science with a transcendent realist approach. The former is associated with Francis Bacon, Isaac Newton, Descartes, Kant and Mill, and more recently Ronald Fisher, Thomas Kuhn and Karl Popper. Their's is science based dissection of the parts to make the whole. The latter is associated with Johann Goethe, Carl Friedrich Gauss, Charles Peirce, Harold Jeffreys, John Maynard Keynes, Edwin Jaynes, Hans Urs von Balthasar and Bernard Lonergan. Their's is science based on the whole with parts reflecting and participating in the whole.} objectivity as two to be related, or not, to one another. Suppose we are still wary of all of the math and its interpretation and even the data and anecdotal, personal, experience. The skeptic says educational attainment does not matter. The critical thinker says, let's use data to help us understand whether the skeptic's claims are true or not, probably.

Here are binary hypotheses for us to consider. We whittle the skeptic down to a level of educational attainment the skeptic can live with, at least for this test. The level is \(E=12\) years for a \emph{yes} answer. The skeptic also agrees to the same sample we used before and \emph{no} means \(\beta=0\). For the skeptic, and the model, this means that \(E\) has no impact, no relationship with wages.

\begin{align}
H_{no}:\,\, \mu_{no} &= \alpha, \, &&with \, Pr(H_{no}) = p \\
H_{yes}:\,\, \mu_{yes} &= \alpha + \beta (12), \, &&with \, Pr(H_{yes}) = 1-p
\end{align}

The \(\mu_{no}\) and \(\mu_{yes}\) are the results of two different ways, two different conjectures, two different models of behavior. We build no and yes into our computational model in the next round. The problem is that in this sample, the intercept \(\alpha < 0\).

Yes, it's a cliff-hanger of a chapter.

\hypertarget{exercising-our-new-found-prowess}{%
\section{Exercising our new found prowess}\label{exercising-our-new-found-prowess}}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-38}{}\label{exr:unnamed-chunk-38}Don't wait a moment longer! Rerun the model in this chapter using the \(E=12\) for the skeptic above. State the new hypotheses. Review the probabilities. Interpret the results.
\end{exercise}

\begin{figure}
\centering
\includegraphics{images/13/wti-unl-data.jpg}
\caption{WTI and UNL data.}
\end{figure}

\begin{exercise}
\protect\hypertarget{exr:unnamed-chunk-39}{}\label{exr:unnamed-chunk-39}Here is a sample of 12 months of the average West Texas Intermediate (WTI) crude oil price in USD/bbl and the New York Harbor Unleaded Gasoline price in barrel-equivalent USD/bbl.\footnote{Here is some more WTI data at FRED: \url{https://fred.stlouisfed.org/series/WTISPLC}. We can find a similar series for unleaded gasoline prices. We can then create a barrel-equivalent price of unleaded simply by multiplying the USD/gallon price by 42 gallons.}
\end{exercise}

\hypertarget{jointly-jointly-into-that-dark-night}{%
\chapter{Jointly, Jointly into that dark night}\label{jointly-jointly-into-that-dark-night}}

\hypertarget{lets-take-positions-on-the-parade}{%
\section{Let's take positions on the Parade}\label{lets-take-positions-on-the-parade}}

Suppose we took positions at the 4 corners of Edward's Parade to count people passing by? We did this to inform a decision to locate a news and treats kiosk at one of the corners. We investigate how foot traffic behavior at location McShane (\(x\)) is related to, might depend on, or even cause traffic flowing through location Dealy (\(y\)). Both locations are adjacent to one another on the eastern side of Edward's Parade.

\hypertarget{query-1}{%
\subsection{Query 1}\label{query-1}}

Here are the two series we will work with for foot traffic at the two locations arranged in (overlapping) order.

\begin{longtable}[]{@{}ccccccccc@{}}
\toprule
\(x\) & 2 & 2 & 3 & 4 & 6 & - & - & -\tabularnewline
\midrule
\endhead
\(y\) & - & - & - & 4 & 5 & 5 & 7 & 8\tabularnewline
\bottomrule
\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is the Tukey 5-statistic empirical distribution of each series?
\item
  Use this empirical distribution to justify univariate sets of low and high hypotheses, one set for each series.
\end{enumerate}

Here is a rendering of a probabilistic analysis for each series using a Poisson observational distribution to link hypotheses with data. Why Poisson, why not binomial, or normal, or uniform? Verify Excel's \texttt{=POISSON()} calculates 0.0902 for \(Pr(d=4,h=2)\) where d is a data point and h is a hypothesis. What is the definition of \(\lambda\) with a practical example from the context of this study?

\textbf{McShane foot traffic}

\begin{longtable}[]{@{}llllll@{}}
\toprule
\begin{minipage}[b]{0.07\columnwidth}\raggedright
\(\lambda\)\strut
\end{minipage} & \begin{minipage}[b]{0.09\columnwidth}\raggedright
\(Pr(\lambda)\)\strut
\end{minipage} & \begin{minipage}[b]{0.20\columnwidth}\raggedright
2, 2, 3, 4, 6\strut
\end{minipage} & \begin{minipage}[b]{0.20\columnwidth}\raggedright
\(Pr(d \mid h)\)\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright
\(Pr(d \mid h)Pr(\lambda)\)\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedright
\(Pr(h \mid d)\)\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.07\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
0.50\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
0.271, 0.271, 0.18, 0.09, 0.012\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedright
0.36\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
5\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
0.50\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
0.084, 0.084, 0.14, 0.176, 0.146\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedright
0.64\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\textbf{Dealy foot traffic}

\begin{longtable}[]{@{}llllll@{}}
\toprule
\begin{minipage}[b]{0.07\columnwidth}\raggedright
\(\lambda\)\strut
\end{minipage} & \begin{minipage}[b]{0.09\columnwidth}\raggedright
\(Pr(\lambda)\)\strut
\end{minipage} & \begin{minipage}[b]{0.20\columnwidth}\raggedright
4, 5, 5, 7, 8\strut
\end{minipage} & \begin{minipage}[b]{0.20\columnwidth}\raggedright
\(Pr(d \mid h)\)\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright
\(Pr(d \mid h)Pr(\lambda)\)\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedright
\(Pr(h \mid d)\)\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.07\columnwidth}\raggedright
4\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
0.50\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
0.195, 0.156, 0.156, 0.06, 0.03\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedright
0.815\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
9\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
0.50\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
0.034, 0.061, 0.061, 0.117, 0.132\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedright
0.185\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

What are the most plausible hypotheses for Dealy (\(y\)) and McShane (\(x\)) foot traffic average intensities in 3 minute intervals? How did you pick these most plausible hypotheses? In what sense are they the most informative hypotheses?

\hypertarget{query-2}{%
\subsection{Query 2}\label{query-2}}

Next we want to examine the dependency of one location on the other. This exercise amounts to analyzing the incredibly simple social network of passersby to and from McShane and Dealy along Edward's Parade. We start with asking how dependent is Dealy (\(y\)) on McShane (\(x\)) using this simple relationship: \(\mu_{y|x} = a + bx\), where \(\mu_{y|x}\) is the mean level of foot traffic at Dealy conditional on foot traffic from McShane. It is up to us to determine \(a\) and \(b\) and their interpretation. Here we ask what we can expect of Dealy foot traffic in the presence of McShane foot traffic.

A. We will use this parameterized joint probability table to aid our investigation. The parameter \(q\) is uniformly distributed with a minimum of 0 and maximum of 0.2. Explain how it is that these are the two bounds on \(q\) for this dependency structure? What is the role of the marginal probabilities and where did they come from?

\begin{longtable}[]{@{}llll@{}}
\toprule
\((x\downarrow) (y \rightarrow)\) & 4 & 9 & \(Pr(x)\)\tabularnewline
\midrule
\endhead
2 & \(0.2+q\) & \(0.2-q\) & 0.4\tabularnewline
5 & \(0.6-q\) & \(q\) & 0.6\tabularnewline
\(Pr(y)\) & 0.8 & 0.2 & 1.0\tabularnewline
\bottomrule
\end{longtable}

B. Since we already know the probabilities of the two sets of hypothetical \(\lambda\)s, calculate the expected values and variances (and standard deviations) of McShane \(x\) and Dealy \(y\). Using the joint probabilities parameterized by \(q\), calculate the covariance of \(x\) and \(y\) as a function of \(q\).

C. Since \(q\) is drawn from a uniform distribution, any draw is equally likely. What are the two values of \(q\) for lower and upper bounds of a probability interval centered at the median of \(q\) between which 95\% probability resides? Derive two dependency structures, to derive a 95\% probability interval for the \(b\) parameter in the dependency relation \(y=a+bx\).

D. Using the two dependency structures, calculate and interpret the 95\% probability interval for \(b\). Sketch the interval on a distribution. What are the implications for foot traffic at Dealy given foot traffic at McShane?

\newpage

\hypertarget{query-1-compare-and-contrast}{%
\section{Query 1: compare and contrast}\label{query-1-compare-and-contrast}}

A. Here are the two series we will work with for foot traffic at the two locations arranged in (overlapping) order.

\begin{longtable}[]{@{}ccccccccc@{}}
\toprule
\(x\) & 2 & 2 & 3 & 4 & 6 & - & - & -\tabularnewline
\midrule
\endhead
\(y\) & - & - & - & 4 & 5 & 5 & 7 & 8\tabularnewline
\bottomrule
\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is the Tukey 5-statistic empirical distribution of each series?
\item
  Use this empirical distribution to justify univariate sets of low and high hypotheses, one set for each series.
\end{enumerate}

\begin{longtable}[]{@{}ccc@{}}
\toprule
statistic & \(x\) McShane & \(y\) Dealy\tabularnewline
\midrule
\endhead
min & 2 & 4\tabularnewline
25\% & 2 & 5\tabularnewline
50\% & 3 & 5\tabularnewline
75\% & 4 & 7\tabularnewline
max & 6 & 8\tabularnewline
\bottomrule
\end{longtable}

Possible low intensity \(\lambda\) could well be 2 for McShane (min and 25\%) and 4 (just minimum as 5 is prominent) for Dealy. High intensity \(\lambda\) would be the max as 75\% is 2 below max for McShane and 8 for Dealy. Apparently the team went for 9. We will need to interview them to find out something we don't know.

B. Here is a rendering of a probabilistic analysis for each series using a Poisson observational distribution to link hypotheses with data. Why Poisson, why not binomial, or normal, or uniform? Verify Excel's \texttt{=POISSON()} calculates 0.0902 for \(Pr(d=4,h=2)\) where d is a data point and h is a hypothesis. What is the definition of \(\lambda\) with a practical example from the context of this study?

\textbf{McShane foot traffic}

\begin{longtable}[]{@{}llllll@{}}
\toprule
\begin{minipage}[b]{0.07\columnwidth}\raggedright
\(\lambda\)\strut
\end{minipage} & \begin{minipage}[b]{0.09\columnwidth}\raggedright
\(Pr(\lambda)\)\strut
\end{minipage} & \begin{minipage}[b]{0.20\columnwidth}\raggedright
2, 2, 3, 4, 6\strut
\end{minipage} & \begin{minipage}[b]{0.20\columnwidth}\raggedright
\(Pr(d \mid h)\)\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright
\(Pr(d \mid h)Pr(\lambda)\)\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedright
\(Pr(h \mid d)\)\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.07\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
0.50\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
0.271, 0.271, 0.18, 0.09, 0.012\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedright
0.36\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
5\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
0.50\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
0.084, 0.084, 0.14, 0.176, 0.146\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedright
0.64\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\textbf{Dealy foot traffic}

\begin{longtable}[]{@{}llllll@{}}
\toprule
\begin{minipage}[b]{0.07\columnwidth}\raggedright
\(\lambda\)\strut
\end{minipage} & \begin{minipage}[b]{0.09\columnwidth}\raggedright
\(Pr(\lambda)\)\strut
\end{minipage} & \begin{minipage}[b]{0.20\columnwidth}\raggedright
4, 5, 5, 7, 8\strut
\end{minipage} & \begin{minipage}[b]{0.20\columnwidth}\raggedright
\(Pr(d \mid h)\)\strut
\end{minipage} & \begin{minipage}[b]{0.15\columnwidth}\raggedright
\(Pr(d \mid h)Pr(\lambda)\)\strut
\end{minipage} & \begin{minipage}[b]{0.11\columnwidth}\raggedright
\(Pr(h \mid d)\)\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.07\columnwidth}\raggedright
4\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
0.50\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
0.195, 0.156, 0.156, 0.06, 0.03\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedright
0.815\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\raggedright
9\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\raggedright
0.50\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
0.034, 0.061, 0.061, 0.117, 0.132\strut
\end{minipage} & \begin{minipage}[t]{0.20\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.15\columnwidth}\raggedright
0\strut
\end{minipage} & \begin{minipage}[t]{0.11\columnwidth}\raggedright
0.185\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

For example, here is a calculation of \(Pr(d=x=5 \mid h=\lambda=4)\) using the Poisson density.

\[
Pr(d=5 \mid h=4) = e^{-\lambda}\frac{\lambda^x}{x!}
\]
\[
= e^{-5}\frac{4^5}{5!} = (0.007)\frac{1024}{120} = 0.156
\]

In further studies we might consider using the Gaussian density. But I think we get the point nicely with the one parameter Poisson distribution.

\begin{itemize}
\item
  We would not use the binomial because binomial uses binary, either-or 0 and 1 inputs. Its data is the number of trials \(n\) and the number of successes given a hypothesis of the probability of a single success \(p\).
\item
  The Poisson aggregates binomial successes and hypothesizes the intensity of those successes against the number of trials as the probability of a single binomial success: \(p=\lambda/n\). It is right for this job because the aggregation of binomial successes (we observe numbers of persons visiting our location) are integer counts.
\item
  We could use the more complicated Gaussian (normal) distribution which related to rational and so-called real numbers. Two parameters are involved initially. In any case any parameter distribution we estimate will be distributed as Gaussian since those parameters will be weighted average expectations, and according to the Central Limit Theorem, will converge on a Gaussian distribution.
\end{itemize}

C. What are the most plausible hypotheses for Dealy (\(y\)) and McShane (\(x\)) foot traffic average intensities in 3 minute intervals? How did you pick these most plausible hypotheses? In what sense are they the most informative hypotheses?

\begin{itemize}
\item
  For Dealy it is \(\lambda=4\) since this is \(0.8/0.2=4x\) more likely than the alternative to occur given the data. For McShane it is \(\lambda=5\) as this is \(0.6/0.4=1.5x\) as likely as the alternative.
\item
  These odds ratios are probabilities derived from both the probability we believe the hypothesis would occur (50/50) and the probabilities of each of the observations given each hypothesis.
\item
  Multiplied together (both-and counting rule) we have two likelihoods for two hypotheses.
\item
  We then sum the two alternative likelihoods (one for each hypothesis) to get the total probability of the data given the hypotheses.
\item
  We use this sum to normalize the likelihoods, that is, gauge their contribution to the plausibility of hypotheses. This effectively deploys Bayes Rule, to impute the probability of each hypothesis given the data.
\item
  The most plausible hypothesis is the most likely given the data.
\item
  The most plausible is the most informative because it contains the most content of the data given the hypothesis. By content we would mean the highest count of ways in which the hypothesis might occur.
\end{itemize}

That's how and a bit of why!

\hypertarget{query-2-cause-and-effect}{%
\section{Query 2: cause and effect}\label{query-2-cause-and-effect}}

Here we examine the dependency of one location on the other. TWe start with asking how dependent is Dealy (\(y\)) on McShane (\(x\)) using this simple relationship: \(\mu_{y|x} = a + bx\), where \(\mu_{y|x}\) is the mean level of foot traffic at Dealy conditional on foot traffic from McShane.

A. We will use this parameterized joint probability table to aid our investigation. The parameter \(q\) is uniformly distributed with a minimum of 0 and maximum of 0.2. Explain how it is that these are the two bounds on \(q\) for this dependency structure? What is the role of the marginal probabilities and where did they come from?

\begin{longtable}[]{@{}llll@{}}
\toprule
\((x\downarrow) (y \rightarrow)\) & 4 & 9 & \(Pr(x)\)\tabularnewline
\midrule
\endhead
2 & \(0.2+q\) & \(0.2-q\) & 0.4\tabularnewline
5 & \(0.6-q\) & \(q\) & 0.6\tabularnewline
\(Pr(y)\) & 0.8 & 0.2 & 1.0\tabularnewline
\bottomrule
\end{longtable}

\begin{itemize}
\item
  We will take \(x=\lambda=[2,5]\) average intensities of foot traffic at the McShane location. These read either 2 or 5 persons show up in 3-minute intervals on average.
\item
  We similarly take \(x=\lambda=[4,9]\) average intensities of foot traffic at the Dealy location. These read either 4 or 9 persons show up in 3-minute intervals on average.
\item
  Both of these are hypotheses about foot traffic data, assumed to be observed with a Poisson distribution. THe marginal probabilities encode the information contained in the sampled data in concert with unobserved information in hypotheses about the sampled data.
\end{itemize}

We have parameterized our social network of 2 nodes: \(McShane \rightarrow Dealy\), with \(q\in [0,0.2]\).

\begin{itemize}
\item
  We need to parameterize the joint probability table simply because there are many possible dependency structures. In our framework we will inject different values of \(q\) to generate different values of \(\sigma_{xy}= covariance\) the only statistic which depends on the joint probabilities. Mean and standard deviations are expectations and depend only on the marginal probabilities.
\item
  A simple test of putting in \(q<0\) results in a negative probability in the lower right \(Pr(2,9)\). As it is probabilities are entities only in our minds, and a negative probability has no meaning in the range of false (probability=0) to true (probability=1) plausibilities.
\item
  If \(q>0.2\), say \(q=2.01\), then the joint probability of 2 and 9 is \(0.2-2.01=-1.81\), definitely not a probability since it too is negative and also exceeds 1 in absolute value!
\end{itemize}

We will use these results later, and more importantly, the underlying logic and reasoning in these parameterized dependency structures.

B. Since we already know the probabilities of the two sets of hypothetical \(\lambda\)s, calculate the expected values and variances (and standard deviations) of McShane \(x\) and Dealy \(y\). Using the joint probabilities parameterized by \(q\), calculate the covariance of \(x\) and \(y\) as a function of \(q\).

Whatever the value of \(q\) we have these results.

\(\mu_x = Ex = 0.4(2) + 0.6(5) = 0.8 + 3 = 3.8\)
\(\mu_y = Ey = 0.8(4) + 0.2(9) = 3.2 + 1.8 = 5\)

\(\sigma_x^2 = Var(x) = Ex^2 - \mu_x^2 = 0.4(4)+0.6(25) - 3.8^2 = 2.16\)
\(\sigma_x = 2.16^{1/2} = 1.47\)

\(\sigma_y^2 = Var(y) = Ey^2 - \mu_y^2 = 0.8(16)+0.2(81) - 5^2 = 4\)
\(\sigma_y = 4^{1/2} = 2\)

The covariance depends on the value of \(q\) because joint probabilities determine the expectation structure of covariance.'

\(\sigma_{xy} = Cov(x,y) = Exy - \mu_x \mu_y\)
\(= (0.2+q)(8)+(0.2-q)(18)+(0.6-q)(20)+0.0(45) - (3.8)(5)\)
\(= -1.8+15q\)

C. Since \(q\) is drawn from a uniform distribution, any draw is equally likely. What are the two values of \(q\) for lower and upper bounds of a probability interval centered at the median of \(q\) between which 95\% probability resides? Derive two dependency structures, to derive a 95\% probability interval for the \(b\) parameter in the dependency relation \(y=a+bx\).

\begin{itemize}
\item
  We carve up the \(b\) distribution into three pieces which, when added up, give us 100\% of the distribution governing \(b\): area 1 is the left tail of cumulative probability weighing in at \(2.5\%\) of the distance from \(q=0\) to \(q=0.2\): this gives us \(q_{0.025}=0.025\times 0.2=0.001\).
\item
  We add another \(95\%\) to get us through area 2 to a cutpoint of \(97.5\%\) of the distance from \(q=0\) to \(q=0.2\): this gives us \(q_{0.975}=0.975\times 0.2=0.195\). This leaves another \(2.5\%\) in the third area, the upper tail, to achieve \(100\%\) probability under a supposed distribution for \(b\).
\item
  It is worth noting that the median of \(q\) is \(50\%\) of the distance from \(q=0\) to \(q=0.2\): this gives us \(q_{0.5}=0.5\times 0.2=0.1\).
\item
  What is the shape of this distribution? We can invoke a version of the Central Limit Theorem to repeatedly sample data given hypotheses to calculate sampled expectations of covariances and variances, all averages. Thus we would expect the distribution of \(b\) to be Gaussian (normal, bell-shaped) and thus symmetric about the median value of \(b\), where \(q=0.1\).
\end{itemize}

\(q=0.001\) for a bound at \(2.5\%\)

\begin{longtable}[]{@{}llll@{}}
\toprule
\((x\downarrow) (y \rightarrow)\) & 4 & 9 & \(Pr(x)\)\tabularnewline
\midrule
\endhead
2 & \(0.201\) & \(0.199\) & 0.4\tabularnewline
5 & \(0.599\) & \(0.001\) & 0.6\tabularnewline
\(Pr(y)\) & 0.8 & 0.2 & 1.0\tabularnewline
\bottomrule
\end{longtable}

\(q=0.195\) for a bound at \(97.5\%\)

\begin{longtable}[]{@{}llll@{}}
\toprule
\((x\downarrow) (y \rightarrow)\) & 4 & 9 & \(Pr(x)\)\tabularnewline
\midrule
\endhead
2 & \(0.395\) & \(0.005\) & 0.4\tabularnewline
5 & \(0.405\) & \(0.195\) & 0.6\tabularnewline
\(Pr(y)\) & 0.8 & 0.2 & 1.0\tabularnewline
\bottomrule
\end{longtable}

We note that all joint probabilities are positive and rows and columns add up to their marginal probabilities.

D. Using the two dependency structures, calculate and interpret the 95\% probability interval for \(b\). Sketch the interval on a distribution. What are the implications for foot traffic at Dealy given foot traffic at McShane?

Let's suppose we develop lots of values of \(b\) using draws from an uniform distribution (a constant probability density) with a minimum of \(q=0\) to a maximum of \(q=0.2\).

\begin{itemize}
\item
  We can ask what is the value of \(b\) with cumulative probability 2.5\% and corrsponding value of \(b\) with cumulative probability 97.5\%. These levels would correspond to a \(q=0.025*0.2=0.005>0\) the minimum \(q\) for the ostensibly low \(b\) and \(q=0.975*0.2=0.195<0.2\) the maximum \(q\) for the corresponding value of \(b\).
\item
  The median is a cumulative probability of 50\% and is just in the middle of 0 and 0.2, that is, 0.1. Altogether these are very convenient cut-points in what would be a Gaussian distribution of \(b\)'s due to the Central Limit Theorem, which we all know, and often revere.
\end{itemize}

At \(q(0.025)=0.005\)

\(b = \frac{Cov(x,y)}{Var(x)}\)
\(= \frac{-1.8+15(0.005)}{2.16}\)
\(=-0.7986\)

It turns out thw \(a=\mu_y - b_{q=0.005}\mu_x\)
\(= 5-(-0.80)(3.8) = 8.04\) so that \(\mu_{y|x}^{q=0.005} = 8.04 - 0.8 x\)

At \(q(0.500)=0.100\)

\(b = \frac{Cov(x,y)}{Var(x)}\)
\(= \frac{-1.8+15(0.100)}{2.16}\)
\(=-0.1390\)

\(a=\mu_y - b_{q=0.1}\mu_x\)
\(= 5+0.1389(3.8) = 5.54\)

\(\mu_{y|x}^{q=0.1} = 5.54 - 0.1389 x\)

At \(q(0.975)=0.195\)

\(b = \frac{Cov(x,y)}{Var(x)}\)
\(= \frac{-1.8+15(0.195)}{2.16}\)
\(=0.5208\)

\(a=\mu_y - b_{q=0.195}\mu_x\)
\(= 5-0.5208(3.8) = 3.02\)

\(\mu_{y|x}^{q=0.1} = 3.02 + 0.5208 x\){]}

\includegraphics{_main_files/figure-latex/unnamed-chunk-45-1.pdf}

We have just estimated a 95\% probability interval for \(b\). A proper probability statement is this.

\[
Pr( -0.7936 \leq b \leq 0.5208 ) = 0.95
\]

\begin{itemize}
\item
  It is 95\% plausibly consistent with the data to find a \(b\) somewhere between \(-0.7986\) and \(0.5208\), mostly negative.
\item
  Since the distribution is (plausibly symmetric) half of the distance spanned by the lower and upper bounds of \(b\) ought to give us the median (50th quantile) \(b=(0.5208+(-0.7936))/2\approx -0.1364\).
\item
  Thus in the dependency relationship \(\mu{y|x}=a+bx\), the presence of 10 persons \emph{at McShane} (not necessarily doing anything else) could impact foot traffic \emph{at Dealy} in a range from 1) reducing Dealy foot traffic by \(0.7936\times 10 \approx 8\) persons to 2) increasing foot traffic at Dealy by \(0.5208\times 10\approx 5\) persons.
\end{itemize}

The end of the lesson! This is definitely a good place to stop as well.

\hypertarget{topic-4-multiple-regressors-oh-my}{%
\chapter*{Topic 4 -- Multiple Regressors, Oh my!}\label{topic-4-multiple-regressors-oh-my}}
\addcontentsline{toc}{chapter}{Topic 4 -- Multiple Regressors, Oh my!}

\begin{itemize}
\item
  More than one source of explanation
\item
  Confounding
\item
  Causal analysis
\item
  Risk and uncertainty
\end{itemize}

\hypertarget{sampling-the-hypothetical}{%
\chapter{Sampling the Hypothetical}\label{sampling-the-hypothetical}}

Richard \citet{McElreath_2020} starts in earnest with \emph{Sampling the Imaginary}. Insofar as the imaginary, as things, emanate from our imagination, and thus meets reality somewhere, then this makes sense only epistemologically. The imaginary represents a very ontological \emph{thing}, in our minds, and we might even share that thing. \footnote{We can consider: 1. Cognitional operations; 2. Epistemology; 3. Metaphysics; 4. Methodology.} \citet{Taleb_2004} notes that black swans were merely figments of imagination, of course until they were discovered in Australia. The imaginary mental being lurks in our consciousness. An example is the exclamation by DiFenetti (\citet{DiFinetti1958}) that \emph{PROBABILITY DOES NOT EXIST}. So true! This is a concept that many share but it is still just a construct in our minds, like every single model in mental existence, even those written on very real paper. But in order to imagine a thing, a representation of some-thing, there must be a reality inside and outside of us to imagine.

Perhaps the best we can do is to experience things, like hospital admissions, numbers of hawks, time spent producing cars, systematically observe them (already imagining!), formulate (shape) a representation, and deduce the consistency between observations and the representation. Data is what is observed by us. Hypotheses are our imagined conjectures about the data and thus unobserved. We seek levels of consistency between the data and our machinations about the data. This sounds very much like a plan (yet another machination?).

\hypertarget{brute-force}{%
\section{Brute force}\label{brute-force}}

A popular, easy both to imagine and construct, approach to mashing data into hypotheses is the \emph{grid approximation} approach. We imagine the 101 possible and quite hypothetical probabilities of a positive result in, say, testing for a novel virus. This is our grid of hypotheses. In this scenario we are sampling from all 8 zip coded in the Bronx. Suppose we find 2 positive zip codes. Following Kurz's lead we tibble into a solution.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(rethinking)}
\NormalTok{n <-}\StringTok{ }\DecValTok{1000}
\NormalTok{n_success <-}\StringTok{ }\DecValTok{6}
\NormalTok{n_trials  <-}\StringTok{ }\DecValTok{8}
\NormalTok{d <-}
\StringTok{  }\KeywordTok{tibble}\NormalTok{(}
    \DataTypeTok{p_grid =} \KeywordTok{seq}\NormalTok{(}
                  \DataTypeTok{from =} \DecValTok{0}\NormalTok{, }
                  \DataTypeTok{to =} \DecValTok{1}\NormalTok{, }
                  \DataTypeTok{length.out =}\NormalTok{ n),}
    \CommentTok{# note we're still using a flat uniform prior}
    \DataTypeTok{prior  =} \DecValTok{1}
\NormalTok{    ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{likelihood =} \KeywordTok{dbinom}\NormalTok{(n_success, }\DataTypeTok{size =}\NormalTok{ n_trials, }\DataTypeTok{prob =}\NormalTok{ p_grid)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{posterior =}\NormalTok{ (likelihood }\OperatorTok{*}\StringTok{ }\NormalTok{prior) }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(likelihood }\OperatorTok{*}\StringTok{ }\NormalTok{prior))}

\KeywordTok{summary}\NormalTok{( d )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      p_grid         prior     likelihood      posterior       
##  Min.   :0.00   Min.   :1   Min.   :0.000   Min.   :0.000000  
##  1st Qu.:0.25   1st Qu.:1   1st Qu.:0.003   1st Qu.:0.000027  
##  Median :0.50   Median :1   Median :0.065   Median :0.000585  
##  Mean   :0.50   Mean   :1   Mean   :0.111   Mean   :0.001000  
##  3rd Qu.:0.75   3rd Qu.:1   3rd Qu.:0.220   3rd Qu.:0.001984  
##  Max.   :1.00   Max.   :1   Max.   :0.311   Max.   :0.002806
\end{verbatim}

The job is almost done and now for the core reason we are here. We sample these hypothetical values of the probability of a single positive test, and visualize our handiwork.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# how many samples would you like?}
\NormalTok{n_samples <-}\StringTok{ }\DecValTok{10000}
\CommentTok{# make it reproducible}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{samples <-}
\StringTok{  }\NormalTok{d }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{sample_n}\NormalTok{( }\DataTypeTok{size =}\NormalTok{ n_samples, }\DataTypeTok{weight =}\NormalTok{ posterior, }\DataTypeTok{replace =}\NormalTok{ T )}
\KeywordTok{glimpse}\NormalTok{(samples)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 10,000
## Columns: 4
## $ p_grid     <dbl> 0.535, 0.731, 0.853, 0.817, 0.870, 0.738, 0.839, 0.554, 0.6~
## $ prior      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~
## $ likelihood <dbl> 0.142, 0.309, 0.233, 0.279, 0.205, 0.310, 0.253, 0.161, 0.2~
## $ posterior  <dbl> 0.001275, 0.002785, 0.002102, 0.002514, 0.001851, 0.002797,~
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#}
\NormalTok{y_label <-}\StringTok{ "h = proportion of positive tests"}
\NormalTok{x_label <-}\StringTok{ "sample index"}
\NormalTok{samples }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{sample_number =} \DecValTok{1}\OperatorTok{:}\KeywordTok{n}\NormalTok{()) }\OperatorTok{%>%}\StringTok{ }
\CommentTok{# Here's the cloud of unknowning}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{( }\DataTypeTok{x =}\NormalTok{ sample_number, }\DataTypeTok{y =}\NormalTok{ p_grid) ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{( }\DataTypeTok{alpha =} \DecValTok{1}\OperatorTok{/}\DecValTok{10}\NormalTok{ ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{( y_label, }\DataTypeTok{limits =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{( x_label )}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-48-1.pdf}

Let's transform from the cloud of unknowing into the frequency of ways in which 8 zip codes can plausibly have 2 positive results. Let's also add a vertical line to indicate the mode of this posterior distribution. The \texttt{tidybayes} package has a slew of useful summarizing statistics, including the \texttt{Mode()} which we use below to find the proportion of tests that correspond to the posterior's mode. \footnote{We recall from statistics and any good dictionary that the mode is the most frequently occurring element in a series of events.} The \emph{Maximum A Posteriori} (aka MAP) hypothesis is just the mode, that is, the most frequently occurring value of the parameter \(p\), This will be one point estimate we can report.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#library(plotly)}
\KeywordTok{library}\NormalTok{(tidybayes)}
\NormalTok{p_MAP <-}\StringTok{ }\KeywordTok{Mode}\NormalTok{(samples}\OperatorTok{$}\NormalTok{p_grid)}
\NormalTok{title <-}\StringTok{ "Bronx Zip Code Tests"}
\NormalTok{x_label <-}\StringTok{ "proportion of zip codes testing positive"}
\NormalTok{y_label <-}\StringTok{ "posterior density"}
\NormalTok{plt <-}\StringTok{ }\NormalTok{samples }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ p_grid)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_continuous}\NormalTok{(x_label, }\DataTypeTok{limits =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_vline}\NormalTok{(}\DataTypeTok{xintercept=}\NormalTok{p_MAP, }\DataTypeTok{color =} \StringTok{"orange"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{annotate}\NormalTok{( }\StringTok{"text"}\NormalTok{, }\DataTypeTok{x =} \FloatTok{0.50}\NormalTok{, }\DataTypeTok{y =} \DecValTok{2}\NormalTok{, }\DataTypeTok{label =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"MAP = "}\NormalTok{, }\KeywordTok{round}\NormalTok{(p_MAP, }\DecValTok{4}\NormalTok{)) ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(y_label) }\OperatorTok{+}\StringTok{ }\KeywordTok{xlab}\NormalTok{(x_label) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(title)}
\NormalTok{plt }\CommentTok{#plotly::ggplotly(plt) for interactive graphics}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-49-1.pdf}

Other measures of tendency include quantiles. We can summarize our testing results in a number of ways depending on the purpose of the report and the desires of the client. Here is one way using the \texttt{summarize} function from the tidyverse's \texttt{dplyr} package along with a few \texttt{tidybayes} functions, of course, for good measure.

\hypertarget{skin-in-the-game}{%
\subsection{Skin in the game}\label{skin-in-the-game}}

We have let \(h = p\) be our hypothesized proportion of zip codes testing positive. Now we suppose we are at a game of chance (they do exist in the Bronx!) and guess \(d = positive\) for the next text. The house will pay us \$1 if we guess exactly right. If we guess wrong we get docked by an amount proportional to the absolute value of the distance \(d - p\) we deviate. We now have a loss function.

We also now call \(d\) a decision. But it is a decision based on a set of hypotheses about decisions, our prior beliefs about those decisions, and driven by the likelihood of seeing decision outcomes in data, a posterior implication. The implication is the plausibility of those decisions logically compatible with experience of observing decision outcomes.

Following de Moivre's (\citet{DeMoivre1756}) \emph{doctrine of chance} we compute our expected loss. This approach constructs a criterion, also known as an objective function. The absolute value function is also known as the check function and is used by \citet{Koenker2005} to build a whole family of quantile regression models the parameters of which are solutions to linear programming models. Here is what the loss objective function looks like.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{guess_d <-}\StringTok{ }\FloatTok{0.5}
\NormalTok{m <-}\StringTok{ }\NormalTok{guess_d}
\NormalTok{X <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(X }\OperatorTok{<}\StringTok{ }\NormalTok{m, }\DecValTok{-1}\NormalTok{, }\KeywordTok{ifelse}\NormalTok{(X }\OperatorTok{>}\StringTok{ }\NormalTok{m, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\NormalTok{XY <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{X =}\NormalTok{ X, }\DataTypeTok{Y =}\NormalTok{ Y)}
\NormalTok{plt <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(XY, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ X, }\DataTypeTok{y =}\NormalTok{ Y)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{2}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_vline}\NormalTok{(}\DataTypeTok{xintercept =}\NormalTok{ m, }\DataTypeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ m, }\DataTypeTok{y =} \DecValTok{0}\NormalTok{, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{,  }\DataTypeTok{size =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept =} \FloatTok{0.0}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ m, }\DataTypeTok{y =} \DecValTok{-1}\NormalTok{, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ m, }\DataTypeTok{y =} \OperatorTok{+}\DecValTok{1}\NormalTok{, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_segment}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ m, }\DataTypeTok{y =} \DecValTok{-1}\FloatTok{+.03}\NormalTok{, }\DataTypeTok{xend =}\NormalTok{ m, }\DataTypeTok{yend =} \DecValTok{1}\FloatTok{-.03}\NormalTok{ ), }\DataTypeTok{color =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{3}\NormalTok{, }\DataTypeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ m, }\DataTypeTok{y =} \DecValTok{-1}\NormalTok{, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ m, }\DataTypeTok{y =} \OperatorTok{+}\DecValTok{1}\NormalTok{, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Y"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ylab}\NormalTok{(}\StringTok{"f(Y,m) = |Y-m|"}\NormalTok{) }
\NormalTok{plt }\CommentTok{#plotly::ggplotly(plt)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-50-1.pdf}

Here we have deviations of a \texttt{p\_grid} value \(m\) from a range of possible guesses \(Y\). From high school algebra we might remember that the rate of change of the absolute value function is this v-shaped beast. It is a beast because the derivative, that is, the slope, is not defined at \(m\). Instead we must gingerly approach the value from the left and the right. Here finally is the check function for a guess of 0.75, as in our zip code example.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m <-}\StringTok{ }\FloatTok{0.75}
\NormalTok{X <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{abs}\NormalTok{(X}\OperatorTok{-}\NormalTok{m)}
\NormalTok{XY <-}\StringTok{ }\KeywordTok{data_frame}\NormalTok{(}\DataTypeTok{X =}\NormalTok{ X, }\DataTypeTok{Y =}\NormalTok{ Y)}
\NormalTok{plt <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(XY, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ X, }\DataTypeTok{y =}\NormalTok{ Y)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{2.0}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_vline}\NormalTok{(}\DataTypeTok{xintercept =}\NormalTok{ m, }\DataTypeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ m, }\DataTypeTok{y =} \DecValTok{0}\NormalTok{, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{,  }\DataTypeTok{size =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept =} \FloatTok{0.0}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Y"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ylab}\NormalTok{(}\StringTok{"f ' (Y,m) = |Y-m|"}\NormalTok{)}
\NormalTok{plt }\CommentTok{#plotly::ggplotly(plt)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-51-1.pdf}

Let's guess that \(d=0.5\) and try this value on our simulated grid by computed the weighted average of losses with weights equal to the posterior distribution. We can use the \texttt{summarize()} function from the \texttt{dplyr} package in the tidyverse ecosystem.
appl

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarize}\NormalTok{(}\StringTok{`}\DataTypeTok{expected loss}\StringTok{`}\NormalTok{ =}\StringTok{ }\KeywordTok{sum}\NormalTok{(posterior }\OperatorTok{*}\StringTok{ }\KeywordTok{abs}\NormalTok{(}\FloatTok{0.5} \OperatorTok{-}\StringTok{ }\NormalTok{p_grid)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   `expected loss`
##             <dbl>
## 1           0.213
\end{verbatim}

McElreath uses the \texttt{sapply()} function from base R to vectorize a calculation of losses using a loss function. The \texttt{purrr::map()} function essentially does the same job and we can see more applications of the \href{https://purrr.tidyverse.org}{\textbf{purrr} package} \citep{R-purrr}, which is itself part of the \href{https://www.tidyverse.org}{\textbf{tidyverse}} and of the \texttt{map()} family \href{https://purrr.tidyverse.org/reference/map.html}{here} or \href{https://jennybc.github.io/purrr-tutorial/ls01_map-name-position-shortcuts.html}{here} or \href{https://data.library.virginia.edu/getting-started-with-the-purrr-package-in-r/}{here}.

The \texttt{map()} will take an input and run this input through whatever function we might dream up. So let's make up a \texttt{make\_loss()} function out of the \texttt{summarize()} experiment above. Typically this is how we construct functions. We first run through a set of routimes. Once we are satisfied with the behavior of those routines, we enshrine them in functions for repeated use. First a \texttt{mutate()} creates a vector of weighted losses, then the \texttt{summarize()} verb finishes the calculate of the sumproducts.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{make_loss <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(guess_d) \{}
\NormalTok{  d }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{loss =}\NormalTok{ posterior }\OperatorTok{*}\StringTok{ }\KeywordTok{abs}\NormalTok{(guess_d }\OperatorTok{-}\StringTok{ }\NormalTok{p_grid)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{weighted_average_loss =} \KeywordTok{sum}\NormalTok{(loss))}
\NormalTok{\}}
\CommentTok{# }\AlertTok{TEST}\CommentTok{!}
\KeywordTok{make_loss}\NormalTok{(}\FloatTok{0.75}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   weighted_average_loss
##                   <dbl>
## 1                 0.115
\end{verbatim}

We always test our handiwork, and as we hoped (expected?) we get the same result as before. It seems our function works just fine.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  loss <-}
\StringTok{  }\NormalTok{d }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\CommentTok{#select(p_grid) %>% }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{weighted_average_loss =}\NormalTok{ purrr}\OperatorTok{::}\KeywordTok{map}\NormalTok{(p_grid, make_loss)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{unnest}\NormalTok{(weighted_average_loss) }
\KeywordTok{summary}\NormalTok{( loss )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      p_grid         prior     likelihood      posterior       
##  Min.   :0.00   Min.   :1   Min.   :0.000   Min.   :0.000000  
##  1st Qu.:0.25   1st Qu.:1   1st Qu.:0.003   1st Qu.:0.000027  
##  Median :0.50   Median :1   Median :0.065   Median :0.000585  
##  Mean   :0.50   Mean   :1   Mean   :0.111   Mean   :0.001000  
##  3rd Qu.:0.75   3rd Qu.:1   3rd Qu.:0.220   3rd Qu.:0.001984  
##  Max.   :1.00   Max.   :1   Max.   :0.311   Max.   :0.002806  
##  weighted_average_loss
##  Min.   :0.112        
##  1st Qu.:0.152        
##  Median :0.254        
##  Mean   :0.309        
##  3rd Qu.:0.450        
##  Max.   :0.700
\end{verbatim}

Let's identify the optimal, that is, the minimum loss using the \texttt{filter()} verb. In this way we can build a vertical line to indicate the optimal loss. A ribbon will fill the area under the loss curve. We can interact with the graph using the \texttt{ggploty()} function on the \texttt{ggplot()} object \texttt{plt}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# this will help us find the x and y coordinates for the minimum value}
\NormalTok{min_loss <-}
\StringTok{  }\NormalTok{loss }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(weighted_average_loss }\OperatorTok{==}\StringTok{ }\KeywordTok{min}\NormalTok{(weighted_average_loss)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{as.numeric}\NormalTok{()}
\NormalTok{min_label <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \KeywordTok{paste0}\NormalTok{( }\StringTok{"minimum loss = "}\NormalTok{, }\KeywordTok{round}\NormalTok{(min_loss[}\DecValTok{2}\NormalTok{], }\DecValTok{2}\NormalTok{) ), }
  \KeywordTok{paste0}\NormalTok{( }\StringTok{"decision = "}\NormalTok{, }\KeywordTok{round}\NormalTok{(min_loss[}\DecValTok{1}\NormalTok{], }\DecValTok{2}\NormalTok{) )}
\NormalTok{)}
\CommentTok{# update the plot}
\NormalTok{plt <-}\StringTok{ }\NormalTok{loss }\OperatorTok{%>%}\StringTok{   }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ p_grid)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_ribbon}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{ymin =} \DecValTok{0}\NormalTok{, }\DataTypeTok{ymax =}\NormalTok{ weighted_average_loss),}
              \DataTypeTok{fill =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.30}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_vline}\NormalTok{(}\DataTypeTok{xintercept =}\NormalTok{ min_loss[}\DecValTok{1}\NormalTok{], }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{linetype =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept =}\NormalTok{ min_loss[}\DecValTok{2}\NormalTok{], }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{linetype =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{, }\DataTypeTok{x =} \KeywordTok{rep}\NormalTok{(min_loss[}\DecValTok{1}\NormalTok{], }\DecValTok{2}\NormalTok{ ), }\DataTypeTok{y =} \KeywordTok{c}\NormalTok{( }\FloatTok{0.4}\NormalTok{, }\FloatTok{0.45}\NormalTok{), }\DataTypeTok{label =}\NormalTok{ min_label) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"expected proportional absolute deviation loss"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"decision (p_grid)"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{panel.grid =} \KeywordTok{element_blank}\NormalTok{())}
\NormalTok{plt }\CommentTok{#plotly::ggplotly(plt)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-55-1.pdf}

We saved the exact minimum value as \texttt{min\_loss{[}1{]}}, which is 0.714. Within sampling error, this is the posterior 50th quantile, the median, as depicted by our \texttt{samples}, except for the sampling error. It is the median as it is the optimal choice of decision based on the absolute deviation function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{posterior_50 =} \KeywordTok{quantile}\NormalTok{(p_grid, }\FloatTok{0.50}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   posterior_50
##          <dbl>
## 1        0.715
\end{verbatim}

We use the \texttt{quantile()} to establish boundaries within the posterior distribution. This function also provides thresholds for extreme values in the distribution, a measure called the \emph{Value-at-risk} in finance.

\hypertarget{an-aside}{%
\subsubsection{An Aside}\label{an-aside}}

Even more interesting is the idea we can find a middling measure that minimizes the sum of absolute deviations of data around an as yet unspecified parameter (too many \(m\)!).

\begin{equation}
SAD = \Sigma_{i=1}^5 |Y_i - m|
\end{equation}

Yes, it is SAD, the sum of absolute deviations. This is our foray into rank-order statistics, quite a bit different in nature than the arithmetic mean of \(SSE\) fame (sum of squared errors). \footnote{We got to this idea in our previous chapter on Fisher style inference.} We get to basic counting when we try to find the \(m\) that minimizes SAD.

To illustrate these ideas suppose our data is all positive (ratio data in fact). If \(m=0.5\) then the function

\begin{equation}
f(Y;m) = |Y-m|
\end{equation}

has this appearance, the so-called check function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m <-}\StringTok{ }\FloatTok{0.5}
\NormalTok{X <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{abs}\NormalTok{(X}\OperatorTok{-}\NormalTok{m)}
\NormalTok{XY <-}\StringTok{ }\KeywordTok{data_frame}\NormalTok{(}\DataTypeTok{X =}\NormalTok{ X, }\DataTypeTok{Y =}\NormalTok{ Y)}
\NormalTok{p <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(XY, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ X, }\DataTypeTok{y =}\NormalTok{ Y)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{2.0}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_vline}\NormalTok{(}\DataTypeTok{xintercept =}\NormalTok{ m, }\DataTypeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ m, }\DataTypeTok{y =} \DecValTok{0}\NormalTok{, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{,  }\DataTypeTok{size =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept =} \FloatTok{0.0}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Y"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ylab}\NormalTok{(}\StringTok{"f(Y,m) = |Y-m|"}\NormalTok{)}
\NormalTok{p }\CommentTok{#plotly::ggplotly(p)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-57-1.pdf}

Intuitively, half the graph seems to be to the left of \(m=0.5\), the other half is to the right. Let's look at the first derivative of the check function with respect to changes in \(m\). Notice that the (eyeballed) rise over run, i.e., slope, before \(m=5\) is -1, and after it is +1. At \(m=0.5\) there is no slope that's immediately meaningful. \footnote{We might impose some sort of smooth-pasting condition to mask this obvious discontinuity in the space of all possible values of \(m\).}

We have two cases to consider. First \(Y\) can be less than or equal to \(m\) so that \(Y-m \leq 0\). In this case

\begin{equation}
\frac{d\,\,(Y-m)_{\leq0}}{dY} = -1
\end{equation}

This corresponds exactly to negatively sloped line accumulating into our supposed \(m=0.5\) in the plot. \footnote{This is the ancient \emph{method of exhaustion} of Antiphon and later Eudoxus of Cnidus to calculate areas and volumes to a degree of accuracy.}

Second, \(Y\) can be greater than or equal to \(m\) so that \(Y-m \geq 0\). In this case

\begin{equation}
\frac{d\,\,(Y-m)_{\geq 0}}{dY} = +1
\end{equation}

also corresponding to the positively sloped portion of the graph.

Another graph is in order to imagine this derivative.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m <-}\StringTok{ }\FloatTok{0.5}
\NormalTok{X <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{Y <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(X }\OperatorTok{<}\StringTok{ }\NormalTok{m, }\DecValTok{-1}\NormalTok{, }\KeywordTok{ifelse}\NormalTok{(X }\OperatorTok{>}\StringTok{ }\NormalTok{m, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\NormalTok{XY <-}\StringTok{ }\KeywordTok{data_frame}\NormalTok{(}\DataTypeTok{X =}\NormalTok{ X, }\DataTypeTok{Y =}\NormalTok{ Y)}
\NormalTok{p <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(XY, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ X, }\DataTypeTok{y =}\NormalTok{ Y)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{2}\NormalTok{)}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_vline}\NormalTok{(}\DataTypeTok{xintercept =}\NormalTok{ m, }\DataTypeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ m, }\DataTypeTok{y =} \DecValTok{0}\NormalTok{, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{,  }\DataTypeTok{size =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept =} \FloatTok{0.0}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ m, }\DataTypeTok{y =} \DecValTok{-1}\NormalTok{, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ m, }\DataTypeTok{y =} \OperatorTok{+}\DecValTok{1}\NormalTok{, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_segment}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ m, }\DataTypeTok{y =} \DecValTok{-1}\FloatTok{+.03}\NormalTok{, }\DataTypeTok{xend =}\NormalTok{ m, }\DataTypeTok{yend =} \DecValTok{1}\FloatTok{-.03}\NormalTok{ ), }\DataTypeTok{color =} \StringTok{"white"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{3}\NormalTok{, }\DataTypeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ m, }\DataTypeTok{y =} \DecValTok{-1}\NormalTok{, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ m, }\DataTypeTok{y =} \OperatorTok{+}\DecValTok{1}\NormalTok{, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Y"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ylab}\NormalTok{(}\StringTok{"f(Y,m) = |Y-m|"}\NormalTok{) }
\NormalTok{p }\CommentTok{#plotly::ggplotly(p)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-58-1.pdf}

It's all or nothing for the derivative, a classic step function. We use this fact in the following (near) finale in our search for \(m\). Back to \(SAD\).

We are looking for the \(m\) that minimizes \(SAD\):

\begin{equation}
SAD = \Sigma_{i=1}^N |Y_i - m| = |Y_1-m| + \ldots + |Y_N-m|
\end{equation}

If we take the derivative of \(SAD\) with respect to \(Y\) data points, we get \(N\) minus 1s and \(N\) plus ones in our sum because each and every \(|Y_i-m|\) could either be greater than or equal to \(m\) or less than or equal to \(m\), we just just don't know which, so we need to consider both cases at once. We also don't know off hand how many data points are to the left or the right of the value of \(m\) that minimizes \(SAD\)!

Let's play a little roulette and let \(L\) be the number of (unknown) points to the left of \(m\) and \(R\) points to the right. Then \(SAD\) looks like it is split into two terms, just like the two intervals leading up to and away from the red dot at the bottom of the check function.

\begin{equation}
SAD = \Sigma_{i=1}^R |Y_i - m| + \Sigma_{i=1}^R |Y_i - m| = (|Y_1-m| + \ldots + |Y_L-m|) + (|Y_1-m| + \ldots + |Y_R-m|)
\end{equation}

and then,

\begin{equation}
\frac{d\,\,SAD}{dY} = \Sigma_{i=1}^L (-1) + \Sigma_{i=1}^R (+1) = (-1)L+ (+1)R
\end{equation}

When we set this result to zero for the first order condition for an optimum we get a possibly strange, but appropriate result. The tradeoff between left and right must offset one another exactly.

\begin{equation}
(-1)L + (+1)R = 0
\end{equation}

so that \(L = R\).

Whatever number of points are to the left must also be to the right of \(m\). If \(L\) points also include \(m\), then \(L/N \geq 1/2\) as well as for the \(R\) points if they include \(m\) so that \(R/N \geq 1/2\).

We have arrived at what a median is.

Now we come up with a precise statement of the middle of a data series, the notorious median. We let \(P()\) be the proportion of data points at and above (if \(Y \geq M\)) or at and below (\(Y \leq m\)).

The median, \(m\), is the first time a data point in a data series reaches \emph{both}

\begin{itemize}
\item
  \(P(Y \leq m) \geq 1/2\) (from minimum data point) \emph{and}
\item
  \(P(Y \geq m) \geq 1/2\) (from the maximum data point)
\end{itemize}

It's logic again with these conjunctive statements. That definition will work for us whether each data point is equally likely (\(1/N\)) as in a so-called uninformative prior or from grouped data with symmetric or skewed relative frequency distributions.

\hypertarget{the-least-of-the-squares}{%
\section{The least of the squares}\label{the-least-of-the-squares}}

What if the game gangs up on us and docks our ante and subsequent winnings with a loss proportional to the square of the deviation of our decision (guess) from the grid or \((d - p)^2\). What does this suggest? We have a new payoff in our \texttt{make\_loss()} function to deal with.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# amend our loss function}
\NormalTok{make_quad_loss <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(guess_d) \{}
\NormalTok{  d }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{loss =}\NormalTok{ posterior }\OperatorTok{*}\StringTok{ }\NormalTok{(guess_d }\OperatorTok{-}\StringTok{ }\NormalTok{p_grid)}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{weighted_average_loss =} \KeywordTok{sum}\NormalTok{(loss))}
\NormalTok{\}}
\KeywordTok{make_loss}\NormalTok{(}\FloatTok{0.75}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   weighted_average_loss
##                   <dbl>
## 1                 0.115
\end{verbatim}

We reuse the code we know works to solve this problem.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# rebuild the loss data}
\NormalTok{loss_quad <-}
\StringTok{  }\NormalTok{d }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\CommentTok{#select(p_grid) %>% }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{weighted_average_loss =}\NormalTok{ purrr}\OperatorTok{::}\KeywordTok{map}\NormalTok{( p_grid, make_quad_loss)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{unnest}\NormalTok{(weighted_average_loss)}
\CommentTok{# update to the new minimum loss coordinates}
\NormalTok{min_loss <-}
\StringTok{  }\NormalTok{loss_quad }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(weighted_average_loss }\OperatorTok{==}\StringTok{ }\KeywordTok{min}\NormalTok{(weighted_average_loss)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{as.numeric}\NormalTok{()}
\NormalTok{min_label <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \KeywordTok{paste0}\NormalTok{( }\StringTok{"minimum loss = "}\NormalTok{, }\KeywordTok{round}\NormalTok{(min_loss[}\DecValTok{2}\NormalTok{], }\DecValTok{2}\NormalTok{) ), }
  \KeywordTok{paste0}\NormalTok{( }\StringTok{"decision = "}\NormalTok{, }\KeywordTok{round}\NormalTok{(min_loss[}\DecValTok{1}\NormalTok{], }\DecValTok{2}\NormalTok{) )}
\NormalTok{)}
\CommentTok{# update the plot}
\NormalTok{plt <-}\StringTok{ }\NormalTok{loss_quad }\OperatorTok{%>%}\StringTok{   }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ p_grid)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_ribbon}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{ymin =} \DecValTok{0}\NormalTok{, }\DataTypeTok{ymax =}\NormalTok{ weighted_average_loss),}
              \DataTypeTok{fill =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.30}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_vline}\NormalTok{(}\DataTypeTok{xintercept =}\NormalTok{ min_loss[}\DecValTok{1}\NormalTok{], }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{linetype =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept =}\NormalTok{ min_loss[}\DecValTok{2}\NormalTok{], }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{linetype =} \DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{, }\DataTypeTok{x =} \KeywordTok{rep}\NormalTok{(min_loss[}\DecValTok{1}\NormalTok{], }\DecValTok{2}\NormalTok{ ), }\DataTypeTok{y =} \KeywordTok{c}\NormalTok{( }\FloatTok{0.4}\NormalTok{, }\FloatTok{0.45}\NormalTok{), }\DataTypeTok{label =}\NormalTok{ min_label) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"expected proportional quadratic loss"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"decision (p_grid)"}\NormalTok{)}
  \KeywordTok{theme}\NormalTok{(}\DataTypeTok{panel.grid =} \KeywordTok{element_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of 1
##  $ panel.grid: list()
##   ..- attr(*, "class")= chr [1:2] "element_blank" "element"
##  - attr(*, "class")= chr [1:2] "theme" "gg"
##  - attr(*, "complete")= logi FALSE
##  - attr(*, "validate")= logi TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plt }\CommentTok{#plotly::ggplotly(plt)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-60-1.pdf}

Based on quadratic loss \((d - p)^2\), the exact minimum value is 0.7. Within sampling error. There might be a bit of a shock that this is just the arithmetic mean of our \texttt{samples}. An aside will be in order.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{posterior_mean =} \KeywordTok{mean}\NormalTok{(p_grid))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   posterior_mean
##            <dbl>
## 1          0.700
\end{verbatim}

The arithmetic mean takes no notice at all of the possibility that different outcomes have different weights. It is this starting position that often frequentists, including me in some realizations, can fail to notice the shape of data, always as that shape is really formed by the knower-analyst's assumptions.

\hypertarget{yet-another-aside}{%
\subsubsection{Yet another aside}\label{yet-another-aside}}

This plot depicts the sum of squared deviations for a grid of potential values of what the data points deviate from, \(m\). Use of such a criterion allows us a clear and in this case unique calculation of the best linear estimator for the mean. We hover over the graph and brush over the area around bottom of the function a little below the median.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{price <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{( }\DecValTok{300}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.15}\NormalTok{ )}
\NormalTok{m <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{length.out =} \DecValTok{300}\NormalTok{)}
\NormalTok{SSE <-}\StringTok{ }\NormalTok{m}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(m)) \{ }
\NormalTok{  SSE[i] <-}\StringTok{ }\KeywordTok{t}\NormalTok{(price }\OperatorTok{-}\StringTok{ }\NormalTok{m[i]) }\OperatorTok{%*%}\StringTok{ }\NormalTok{(price }\OperatorTok{-}\StringTok{ }\NormalTok{m[i])}
\NormalTok{\}}
\NormalTok{opt_plot <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{m =}\NormalTok{ m, }\DataTypeTok{SSE =}\NormalTok{ SSE)}
\NormalTok{SSE_min_index <-}\StringTok{ }\NormalTok{SSE }\OperatorTok{==}\StringTok{ }\KeywordTok{min}\NormalTok{(SSE) }
\NormalTok{SSE_min <-}\StringTok{ }\NormalTok{SSE[SSE_min_index]}
\NormalTok{m_min <-}\StringTok{ }\NormalTok{m[SSE_min_index]}
\CommentTok{# now for a picture}
\NormalTok{p <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(opt_plot, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ m, }\DataTypeTok{y =}\NormalTok{ SSE)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{1.25}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ m_min, }\DataTypeTok{y =}\NormalTok{ SSE_min, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{size =} \FloatTok{4.0}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_segment}\NormalTok{(}\DataTypeTok{x =} \DecValTok{0}\NormalTok{, }\DataTypeTok{y =}\NormalTok{ SSE_min}\OperatorTok{+}\DecValTok{5}\NormalTok{, }\DataTypeTok{xend =}\NormalTok{ m_min, }\DataTypeTok{yend =}\NormalTok{ SSE_min, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_segment}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ m_min}\OperatorTok{++}\DecValTok{5}\NormalTok{, }\DataTypeTok{y =} \DecValTok{0}\NormalTok{, }\DataTypeTok{xend =}\NormalTok{ m_min, }\DataTypeTok{yend =}\NormalTok{ SSE_min, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{linetype =} \StringTok{"dashed"}\NormalTok{)}
\NormalTok{p }\CommentTok{#plotly::ggplotly(p)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-62-1.pdf}

Simply putting the cursor on the red dot indicates a solution: \(m=\) 1.

A bit of calculus confirms the brute force choice of the arithmetic mean that minimizes the sum of squared deviations about the mean.

First, the sum of squared errors (deviations) of the \(X_i\) data points about a mean of \(m\) is

\begin{equation}
SSE = \Sigma_{i=1}^5 (Y_i - m)^2
\end{equation}

Second, we derive the first derivative of \(SSE\) with respect to \(m\), holding all else (e.g., sums of \(X_i\)) and set the derivative equal to zero for the first order condition for an optimum.

\begin{equation}
\frac{d\,\,SSE}{dm} = -2\left(\Sigma_{i=1}^5 (Y_i - m)\right) = 0
\end{equation}

Here we used the chain and power rules of differentiation.

Third, we solve for \(m\) to find

\begin{equation}
m = \frac{\Sigma_{i=1}^5 Y_i}{N}=0.504
\end{equation}

Close enough for us? This is none other than the arithmetic mean. We can perform similar procedure to get the sample means of the y-intercept \(b_0\) and slope \(b_1\) of the relationship

\begin{equation}
Y_i = b_0 + b_1 X_i + e_i
\end{equation}

where \(x_i\) data points try tp explain movements in the \(Y_i\) data points. This will be the subject of our next chapter, an excursion into geocentric models, the narcissistic approach to statistics.

\hypertarget{look-up-in-the-sky-its-a}{%
\section{Look: up in the sky it's a \ldots{}}\label{look-up-in-the-sky-its-a}}

Let's try Laplace's quadratic approximation technique instead of the brute force grid. This approach takes advantage of the observation that around the mode of the posterior distribution, the distribution is approximately Gaussian-shaped. A Gaussian shape is just a quadratic combination of the hypotheses.

Here is a version for the simplest model that is not binomial (or poisson for that matter). The Laplace quadratic approximation technique has two steps.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Find the optimal values of posterior parameters (MAP).
\item
  Given the optimal value of posterior parameters and their interactions across the data, simulate posterior values of parameters and data predictions.
\end{enumerate}

We will begin to control under and over-flow issues (really small and really large numbers) by using the following relationship between the product, here, of two likelihoods, \(p_1\) and \(p_2\), and the exponent of the sum of logarithms of the two likelihoods.

\begin{equation}
p_1p_2 = e^{log(p_1)+log(p_2)}
\end{equation}

We shift gears out of first gear binomial models into the second gear of a simple mean and standard deviation of a set of continuous data like temperature or electrical usage or even square hectares of land versus water. We suppose our data looks like this (simulated to protect the innocent) and pretend we are energy managers who are reviewing a facility's electricity usage per minute. The dimensions of \(y\) are kilo-watt hours per minute.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{4242}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \DecValTok{20}\NormalTok{, }\DataTypeTok{mean =} \DecValTok{10}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{5}\NormalTok{) }\CommentTok{# kilo-watt hours per minute}
\KeywordTok{c}\NormalTok{(}\DataTypeTok{mean =} \KeywordTok{mean}\NormalTok{(y), }\DataTypeTok{sd =} \KeywordTok{sd}\NormalTok{(y))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## mean   sd 
## 9.99 5.68
\end{verbatim}

That's the data \(y_i\) for \(i=1 \cdots 20\) and its specification in R. Running \emph{toy} simulation models like this help us to be sure our more realistic models are working correctly.

For the priors we will let the mean be normally distributed (Gaussian) and possibly range from 0 to 100, since we do not know any differently at this stage. Normal distributions are symmetric and are almost triangular looking if we squint at them.

We let the standard deviation take on a log Gaussian shape, bounded at 0, since standard deviations are always greater than or equal to zero. The lognormal distribution will have a shape that feels a bit exponential, almost chi-squared. But that's a consideration for later. There's an assumption we can build in! The full model looks like this:

\begin{align}
y_i & \sim \operatorname{Normal}(\mu, \sigma) \\
\mu & \sim \operatorname{Normal}(0,100) \\
\sigma & \sim \operatorname{LogNormal}(0,10) \\
\end{align}

The likelihood function for data \(y_i\) looks like this:

\begin{equation}
\operatorname{Pr}(y_i \mid \mu, \sigma) = \frac{1}{\sigma\sqrt{2\pi}}\operatorname{exp}\left[\frac{(y_i - \mu)^2}{2\sigma}\right]
\end{equation}

Ye gads! But not to worry, this density function is computed in R with \texttt{dnorm()}. This beast of a formula has \(\pi\) in it so it very likely has something to do with circles and trigonometry, again for later, some day, perhaps, maybe. It is definitely easier to use the \texttt{\textasciitilde{}} language above to specify models.

We set the feature to take logarithms of this computation with a \texttt{log=T} argument in \texttt{dnorm()}. This function will compute for the two parameters in \texttt{p} and the data in \texttt{y}, the log posterior density. It we raise this number to the power of \(e\) we get the original product of posterior likelihoods. We note how we incorporate the two assumptions about the parameters as priors in the computation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(p, y) \{}
\NormalTok{    log_lik <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{dnorm}\NormalTok{(y, p[}\StringTok{"mu"}\NormalTok{], p[}\StringTok{"sigma"}\NormalTok{], }\DataTypeTok{log =}\NormalTok{ T))  }\CommentTok{# the log likelihood}
\NormalTok{    log_post <-}\StringTok{ }\NormalTok{log_lik }\OperatorTok{+}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(p[}\StringTok{"mu"}\NormalTok{], }\DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DataTypeTok{log =}\NormalTok{ T) }\OperatorTok{+}\StringTok{ }\KeywordTok{dlnorm}\NormalTok{(p[}\StringTok{"sigma"}\NormalTok{],}
        \DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DataTypeTok{log =}\NormalTok{ T)}
    \KeywordTok{return}\NormalTok{(log_post)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

So this works by inserting the data \(y_i\) and initial guesses about parameter values into an optimizer that searche across values of \texttt{mu} and \texttt{sigma} for the maximum log-likelihood of seeing all of the data \texttt{y} given a \texttt{mu} and \texttt{sigma}. This is a bit different than calculating the arithmetic mean and standard deviation isn't it?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{inits <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DataTypeTok{mu =} \DecValTok{0}\NormalTok{, }\DataTypeTok{sigma =} \DecValTok{1}\NormalTok{)}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{optim}\NormalTok{(inits, model, }\DataTypeTok{control =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{fnscale =} \DecValTok{-1}\NormalTok{), }\DataTypeTok{hessian =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{y =}\NormalTok{ y)}
\NormalTok{fit}\OperatorTok{$}\NormalTok{par}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    mu sigma 
##  10.0   5.4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit}\OperatorTok{$}\NormalTok{hessian}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              mu     sigma
## mu    -0.686507  0.000699
## sigma  0.000699 -1.444500
\end{verbatim}

What we get from this exercise is a lot more than arithmetic means and standard deviations. We are analyst-skeptical, we just need to know (experience - understanding - judgment). We need to learn something about how the variations in the data reflect in variations in the mean and standard deviation, as well as how the mean and standard deviation end up interacting as well.

We get the interactions of the mean and standard deviation from the \texttt{hessian} matrix computed during the computes the variance-covariance of the solution 9.995, 5.398. We are not done!

Next we use the fit parameters that have located the Maximum A Posteriori values of the two parameters \$\mu = \$ 9.995 and \$\sigma = \$ 5.398 to generate posterior predictive samples, plot them, talk about them, and then we are done (for the moment!).

If we invert the negative of the hessian matrix, we get back the variance-covariance matrix of the two parameters. Frequentist-while, the square root of the diagonal of this inverted hessian matrix can form the basis of t-tests of the null hypothesis that \(\mu = 0\) or some other target value \(\mu_0\). But we are not frequentists, so Probabilist-while we return to sampling.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{par_mean <-}\StringTok{ }\NormalTok{fit}\OperatorTok{$}\NormalTok{par}
\NormalTok{par_varcov <-}\StringTok{ }\KeywordTok{solve}\NormalTok{(}\OperatorTok{-}\NormalTok{fit}\OperatorTok{$}\NormalTok{hessian)}
\KeywordTok{round}\NormalTok{( par_mean, }\DecValTok{2}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    mu sigma 
##  10.0   5.4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{round}\NormalTok{( par_varcov, }\DecValTok{4}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           mu  sigma
## mu    1.4567 0.0007
## sigma 0.0007 0.6923
\end{verbatim}

In order to sample predictive posterior views, really hypotheses of what might be, of \(\mu\) and \(\sigma\) as Gaussian variates, we will need to take into account not only the means and variances of the \(\mu\) and \(\sigma\) parameters, but we will need to include how these parameters would vary with one another with covariances 0. The \texttt{mvtnorm} package will come to our aid. First we draw the samples from the posterior probability urn, then attach a simulation of predictions for \(y\).

We use the \texttt{coda} package to display the distributions this time, since it is a popular way to visualize Monte Carlo simulations of the Markov Chain species. This is the package that McElreath's \texttt{rethinking} package employs.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(mvtnorm)}
\NormalTok{samples <-}\StringTok{ }\KeywordTok{rmvnorm}\NormalTok{(}\DecValTok{10000}\NormalTok{, par_mean, par_varcov)}
\NormalTok{samples <-}\StringTok{ }\KeywordTok{as.tibble}\NormalTok{(samples) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{prediction =} \KeywordTok{rnorm}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{n}\NormalTok{(), mu, sigma))}
\CommentTok{# or simplehist(samples %>% select(prediction))}
\CommentTok{# coda plots}
\KeywordTok{library}\NormalTok{(coda)}
\NormalTok{samples_mcmc <-}\StringTok{ }\KeywordTok{mcmc}\NormalTok{(samples)}
\KeywordTok{densplot}\NormalTok{(samples_mcmc)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-66-1.pdf} \includegraphics{_main_files/figure-latex/unnamed-chunk-66-2.pdf} \includegraphics{_main_files/figure-latex/unnamed-chunk-66-3.pdf}

Here is a \textbf{high density interval} view of the modes of the three samplings. A 95\% interval width allows us to say that the values of posterior parameters and predictions are between lower and upper bounds with 95\% compatibility with the data. These are compatibility, plausibility, posterior probability, credibility intervals. That last adjective is for the insurance analysts in the gang. The \texttt{pivot\_longer()} is preferred over \texttt{gather()}. The \texttt{name} is automatically generated from column names in the tibble, while \texttt{value} is the new array of sampled parameters and predictions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{pivot_longer}\NormalTok{(mu}\OperatorTok{:}\NormalTok{prediction) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(name) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mode_hdi}\NormalTok{(value)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 7
##   name       value .lower .upper .width .point .interval
##   <chr>      <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    
## 1 mu          9.66   7.50  12.2    0.95 mode   hdi      
## 2 prediction  9.78  -1.00  21.0    0.95 mode   hdi      
## 3 sigma       5.45   3.82   7.11   0.95 mode   hdi
\end{verbatim}

We can expect 95\% of the time to see a wide range of predictions for our data even though the mean and standard deviation are much tighter.

Here is a posterior predictive plot for kilo-watt usage in our example for the energy manager.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# the simulation}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{4242}\NormalTok{)}
\CommentTok{# the plot}
\NormalTok{samples }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ prediction)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \DecValTok{1}\NormalTok{, }\DataTypeTok{center =} \DecValTok{0}\NormalTok{,}
                 \DataTypeTok{color =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{, }\DataTypeTok{size =} \DecValTok{1}\OperatorTok{/}\DecValTok{10}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_x_continuous}\NormalTok{(}\StringTok{"usage samples"}\NormalTok{,}
                     \DataTypeTok{breaks =} \KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \DecValTok{0}\NormalTok{, }\DataTypeTok{to =} \DecValTok{50}\NormalTok{, }\DataTypeTok{by =} \DecValTok{5}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{breaks =} \OtherTok{NULL}\NormalTok{, }\DataTypeTok{limits =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1200}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Posterior predictive distribution"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_cartesian}\NormalTok{(}\DataTypeTok{xlim =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{50}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{panel.grid =} \KeywordTok{element_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-68-1.pdf}

Was that enough? For now at least. We accomplished a lot. But perhaps the biggest takeaway is our new found ability to deploy simulation with probability with compatibility of hypotheses with data to arrive at learning about the data. We learned because we inferred.

\hypertarget{exercise-until-morale-improves}{%
\section{Exercise until morale improves}\label{exercise-until-morale-improves}}

Up next, we gather our wits together to rebuild the workhorse model, the linear regression, in the image and likeness of a probabilistic simulator. But first, we try some exercises to flex our analytical muscles.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Use the grid model to redo the loss model. Change the loss function from a quadratic loss (exponent is 2) to a fractional loss (exponent is 1.5). What, if anything, do you observe to be different?
\item
  Rerun the Gaussian likelihood quadratic approximation model using, very simply, different priors on \(\mu\) and \(\sigma\) in the model function. Are these priors reasonable, and, of course, why or why not? How different are the results?
\end{enumerate}

\hypertarget{gausss-robots}{%
\chapter{Gauss's Robots}\label{gausss-robots}}

\hypertarget{while-you-wait}{%
\section{While you wait \ldots{}}\label{while-you-wait}}

While Gelman and McElreath use the parable of the Golem, E.T. Jaynes uses Josef ÄŒapek's native Czech word \href{https://en.wikipedia.org/wiki/R.U.R.}{\emph{roboti}} to describe organic machines that end up annihilating humankind in his brother Karel's play \emph{R.U.R (Rossum's Universal Machines)} in 1921. I like this depiction since the word and the play intersect with R.A. Fischer's frequentist hijacking of the \emph{inverse probability} approach to statistical thinking in 1925. After all we are just building machines, some toy, some production, all not thinking, but sometimes acting like they are in our reverie that fantasizes a \href{https://en.wikipedia.org/wiki/Reification_fallacy}{reification}, a fallacy. Whatever the golem or robot does it is because we tell it to do so, and it will inexoribly and logically. \href{}{Carl Friedrich Gauss's} \emph{linear regression} model is such a robot. It will do what we tell it to. But often we will interpret the robot's abstractions as a real thing, which it emphatically is not.

\hypertarget{gauss-gauss-where-is-gauss}{%
\section{Gauss, Gauss, where is Gauss?}\label{gauss-gauss-where-is-gauss}}

Gauss did not have to really invent the normal distribution. We observe much physical, chemical, biological, psychological, social, economic, even financial behavior that appears on first glance to be Gaussian. Here are three examples from the business domain.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compound revenue growth.
\item
  Any financial statement item.
\item
  Continuous stock returns.
\end{enumerate}

\hypertarget{full-time-equivalent}{%
\subsection{Full time equivalent}\label{full-time-equivalent}}

This measure is often deployed to understand staffing against salary and benefits, among other issues. The calculation can be as simple as adding up all the employees, whether full or part-time, say a head count of 5 people, and the hours worked in a week by the 5 people, say 120 hours. If we assume a 40 hour week, then the full time equivalent number of employees per week in a 40 hour week would be 120 hours divided by a 40 hours/full-time work week to equal an FTE of 3. Is this Gaussian?

Suppose we sum up a week's worth of hours per day. Each day's hours is again uniformly distributed, this time from 100 to 200 hours summed over a 5 day period. This is not the same necessarily as multiplying one day's simulation times 5, or is it?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hours <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{( }\DecValTok{10000}\NormalTok{, }\KeywordTok{sum}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{30}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{30}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{30}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{30}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{30}\NormalTok{))) }

\KeywordTok{summary}\NormalTok{(hours)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    78.3   105.8   112.4   112.5   119.2   145.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fte <-}\StringTok{ }\NormalTok{hours }\OperatorTok{/}\StringTok{ }\DecValTok{40}

\KeywordTok{dens}\NormalTok{( fte , }\DataTypeTok{norm.comp=}\OtherTok{TRUE}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-70-1.pdf}

The average FTE is 2.75 people equivalent in this very symmetrical density. Again, why? Whatever the average value of the source distribution, here 113 hours, each sample from the source will be a deviation from this average. The summary shows a nearly equal quartile and minimum or maximum set of fluctuations from the mean. Adding up the deviations from the mean always equals zero algebraicly if we use the arithmetic mean. The range of the uniformly distributed hours is 75 hours per week while the simulation's range is smaller at about 65 hours. The deviations are whittled down as they sum up. Where did they go? They began to offset one another. Large deviations offset large negative ones. The more terms the more ways these large movements offset one another one for one or in sums of smaller deviations that add up to the same large movements. The mostly ways to realize sums are those that aggregate around the mean.

\hypertarget{compound-growth}{%
\subsection{Compound growth}\label{compound-growth}}

Suppose we have 1 quarter of growth in revenue. The quanterly rate of growth takes a \$1 of revenue at the beginning of month 1 of the quarter, which grows at the first month's rate \(g_1\) into \(1+g_1\). This amount grows into \((1+g_1) + (1+g_1)g_2 = (1+g_1)(1+g_2)\). This end of second month accumulated growth becomes \((1+g_1)(1+g_2) + (1+g_1)(1+g_2)g_3 = (1+g_1)(1+g_2)(1+g_3)\) by the end of the quarter and month 3. Is this Gaussian?

Let's suppose that growth rates are uniformly distributed from -0.1 to 0.1. Then one path for quarterly (1 plus) might be \((1+0.08)(1+(-0.02))(1+0.05)=\) 1.111. Simulating growth 10000 times and viewing our work in the density plot shows the approximation to a theoretical normal distribution.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{growth <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{( }\DecValTok{10000}\NormalTok{ , }\KeywordTok{prod}\NormalTok{( }\DecValTok{1} \OperatorTok{+}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{12}\NormalTok{,}\OperatorTok{-}\FloatTok{0.1}\NormalTok{,}\FloatTok{0.1}\NormalTok{) ) )}
\KeywordTok{dens}\NormalTok{( growth , }\DataTypeTok{norm.comp=}\OtherTok{TRUE}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-71-1.pdf}

Not bad, but look at those tails! Yes, if we are willing to accept no wild swings on average in growth, then this model might help us understand patterns in revenue growth. Why Gaussian? It appears that growth will grow and decline, sometimes adding a lot or deducting a lot, but the cumulative impact will tail off as the month to month changes may even wipe each other out. As long as growth changes impact each successive month in ever smaller ways relative to the accumulative of previous months, then the impact will be Gaussian distributed. Isn't multiplication a shill for summation?

\hypertarget{log-products}{%
\section{Log products}\label{log-products}}

\ldots{} are 2x4 boards. Yes, and no. Our log products again turn into sums. Let's see how. The logarithm can be defined in terms of growth.

\[
g = log(e^g)
\]

Logarithms to a base \(b\) just return the exponents \(x\) of the exponential \(b^x\). Also since

\[
e^{g_1}e^{g_2}e^{g_3} = e^{g_1+g_2+g_3}
\]

Then

\[
g_1+g_2+g_3 = log(e^{g_1+g_2+g_3})=log(e^{g_1} e^{g_2} e^{g_3})
\]

Products become sums with logarithms. And so it is with compound growth.

\[
log[(1+g_1)(1+g_2)(1+g_3)] = log(1+g_1)+log(1+g_2)+log(1+g_3)
\]

Just another sum and also Gaussian? Let's see.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{growth <-}\StringTok{ }\KeywordTok{replicate}\NormalTok{( }\DecValTok{10000}\NormalTok{ , }\KeywordTok{log}\NormalTok{(}\KeywordTok{prod}\NormalTok{( }\DecValTok{1} \OperatorTok{+}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DecValTok{12}\NormalTok{,}\OperatorTok{-}\FloatTok{0.1}\NormalTok{,}\FloatTok{0.1}\NormalTok{)) ) )}
\KeywordTok{dens}\NormalTok{( growth , }\DataTypeTok{norm.comp=}\OtherTok{TRUE}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-72-1.pdf}

Again large divations from the mean of 0 will offset large negative deviations. Sums of small large deviations will offset large negative deviations and vice-versa. What's left is an accumulation of the rest which become the most likely deviations large negatives with small positives and large positives with small negatives. If our data looks like that kind of a process, then a Gaussian distribution might be useful to approximate behavior.

\hypertarget{assume}{%
\section{Assume}\label{assume}}

We will build our first Gaussian model here and now. Our first assumption is

\begin{quote}
Draws of variates are independent of one another, and identically distributed too.
\end{quote}

We take a deep breath and sigh, that can't be true! It is not, only in our mind and in the programming of our Gaussian robot. IID does not really exist, it is an artificial mind's version of a can-opener that allows us to sort sytematic from unsystematic movements in variables. If systematic, then we have the job of describing how unsystematic deviations from the systematic might occur, how large they might be, how frequent they might occur, how lop-sided they make the distribution of our dreams. That is the face of data we imagine and program into our robot.

Our second assumption is that

\begin{quote}
If all you know is the mean and standard deviation, use Gaussian.
\end{quote}

Less of an assumption than an implication of how we know or don't know about anything. That's epistemology for you! This branch of philosophy asks the profound question, whose answer is well beyond what we are considering here, but which we use anyway,

\begin{quote}
A variate will depend on another variate(s) only in a straight, so-called linear, way.
\end{quote}

From a distance, a twenty-one sided polygon, each side of which is a straight line, can look almost like a circle. It's an approximation, and if it's close enough, it may work for our purposes. We usually start with intercept and slope. Later on in school we learned that slopes can change, and, voila a quadratic, a cubic, a whatever, is fomenting by our fervid imaginations. We trade off these shapes with the need to explain more about that dependent variable.

\begin{quote}
There shalt be only one standard deviation of residuals.
\end{quote}

For otherwise we have the disease of heterskedasticity where residuals, the unsystematic side of the relationship and why we need to reason probabilistically. A prominent reason why there might be a multiplicity of different Gaussian distributions hidden, latent, otherwise obfuscating the view.

\begin{quote}
There shalt not be any meaningful relationships among explanatory variables.
\end{quote}

For otherwise we shall have confusion and be confounded. Independent sources mean clear results, usually. The contrary condition is called \textbf{multicollinearity.} This is another source of obfuscation that often arises in inflammatory oratory where the orator lists ten reasons why we should vote for his or her cause, and they all wind up being just one reason, and often not a good one either.

Regarding the reasons for or against at least an initial use of a simple Gaussian model, we can say these three things though about cognition, epistemology, ontology, all implications for a fourth consideration: an analytical methodology. When we confuse what we dreamt up, imagined, and programmed our probalistic robit with reality, that's when error, fallacy, and obfuscation occurs. This will get deep, but raises the questions in an order familiar to those who diagnose, then prognosticate, for a living.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A \textbf{cognitional theory} asks, ``What do I do when I know?'' It encompasses the operations that give rise to judgments of fact and value. These operations include all of the techniques in Heuer and Pherson (2011). But we must also include the integrating notion that ``operators'' are intending to know in the first place. When that intention is unbiased, then horizons become less limiting. Less limited horizons then play to strengths of an analyst's use of a particular analytical technique applied to a growing base of data.
\item
  An \textbf{epistemology} asks, ``Why is doing that (cognition) knowing?'' It demonstrates how these occurrences may appropriately be called ``objective.'' This is where dialectic certainly resides. We have subjects and objects (analysts and the objects of analysis; insurgents and other subjects who are the objects of their insurgency). What is objective is found in the widest possible horizon where there are no further relevant questions to be asked, and answered.
\item
  An \textbf{ontology} (also known as \textbf{metaphysics}) asks ``What do I know when I do it (cognition)?'' It lines up decision maker's priorities because it identifies corresponding structures of the realities we know and value, what is possible, what is not. It provides objective criteria for determining what is, as opposed to what is not, and what is valuable, as opposed to what is worthless.
\item
  A \textbf{methodology} asks, ``What therefore should we do?'' It lays out a framework for collaboration among collectors, analysts, policy makers, assets, and others, based on the answers to the first three questions. It is this framework that analysts use as reference, and guide. Importantly it gives analysts employed at various levels in various organizations a common vocabulary, road map, and expectations from the analytic function.
\end{enumerate}

Thus we build golems and robots to aid our discovery (heuristic) of reality. What happens when our cognitional operations come up with zilch (a technical term for we don't know yet)? We fall back to a Gaussian position. At least we know some trends (means) that we can't be sure of (standard deviations). Various trends and their standard deviations themselve might even be correlated, relative, not independent of one another. If they are unsystematic, then we tell the golem / robot they are independently distributed. If we only have one set of means and standard deviations to work from, then they are also identically distributed.

But we must, in our methodology, be acutely aware of the limitations of these assumptions. THey are but an approximation, a starting point, a stalking something against which we might benchmark a better, wider angle view of the behaviors we model. Setting the expectations of peers and consumers of our analytical products is paramount.

\hypertarget{load-em-up}{%
\section{Load 'em up}\label{load-em-up}}

Okay, now let's use the Gaussian version of our perception of truth, as a first stab at understanding our data and its many possible relationships. We will

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Pose crisp, crunchy, questions} about surprising situations;
\item
  \textbf{Hypothesize relationships} and at least trends and deviations from trends;
\item
  \textbf{Collect (sample), wrangle, and explore data} relevant to the hypotheses
\item
  Condition the data in the presence of hypotheses, effectively \textbf{counting the ways};
\item
  \textbf{Infer plausible relationships} through deriving a schedule of plausibilities of hypotheses compatible with the data.
\end{enumerate}

Quite a full-employment program! Our crunchy question:

\begin{quote}
I've got a lot of education, and experience. Will I command a higher wage?
\end{quote}

Folklore sometimes does, sometimes does not bear this out. \href{https://angeladuckworth.com/grit-book/\#:~:text=About\%20the\%20Book\%20In\%20this\%20instant\%20New\%20York,blend\%20of\%20passion\%20and\%20persistence\%20she\%20calls\%20\%E2\%80\%9Cgrit.\%E2\%80\%9D}{\emph{Grit, passion, perseverance}}, also known as work ethic, might be other factors we can explore later. Our data is from We will explore data from the Population Survey from the U.S. Census Bureau found in the \texttt{AER} package. There is an extensive literature that attempts to model labor supply and demand decisions, all of which include wages, education, and experience.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#install.packages("AER")}
\KeywordTok{library}\NormalTok{(GGally)}
\KeywordTok{library}\NormalTok{(AER)}
\KeywordTok{data}\NormalTok{(CPS1988)}
\KeywordTok{head}\NormalTok{(CPS1988)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   wage education experience ethnicity smsa    region parttime
## 1  355         7         45      cauc  yes northeast       no
## 2  123        12          1      cauc  yes northeast      yes
## 3  370         9          9      cauc  yes northeast       no
## 4  755        11         46      cauc  yes northeast       no
## 5  594        12         36      cauc  yes northeast       no
## 6  377        16         22      cauc  yes northeast       no
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(CPS1988)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       wage         education      experience   ethnicity     smsa      
##  Min.   :   50   Min.   : 0.0   Min.   :-4.0   cauc:25923   no : 7223  
##  1st Qu.:  309   1st Qu.:12.0   1st Qu.: 8.0   afam: 2232   yes:20932  
##  Median :  522   Median :12.0   Median :16.0                           
##  Mean   :  604   Mean   :13.1   Mean   :18.2                           
##  3rd Qu.:  783   3rd Qu.:15.0   3rd Qu.:27.0                           
##  Max.   :18777   Max.   :18.0   Max.   :63.0                           
##        region     parttime   
##  northeast:6441   no :25631  
##  midwest  :6863   yes: 2524  
##  south    :8760              
##  west     :6091              
##                              
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{precis}\NormalTok{( CPS1988 )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             mean    sd 5.5% 94.5%      histogram
## wage       603.7 453.5  131  1284     â–‡â–â–â–â–â–â–â–â–â–
## education   13.1   2.9    8    18      â–â–â–â–â–â–‡â–ƒâ–ƒâ–‚
## experience  18.2  13.1    1    43 â–‚â–‡â–‡â–‡â–‡â–…â–ƒâ–ƒâ–‚â–‚â–â–â–â–
## ethnicity    NaN    NA   NA    NA               
## smsa         NaN    NA   NA    NA               
## region       NaN    NA   NA    NA               
## parttime     NaN    NA   NA    NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wage_all <-}\StringTok{ }\KeywordTok{as_tibble}\NormalTok{(CPS1988)}
\KeywordTok{ggpairs}\NormalTok{(wage_all)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-73-1.pdf}

Way too much! But this can be reduced or zoomed into. Lots of relationships to think about relative to our question. Let's just look at two variables relevant to our question, wages (\$/hr) and education (years), and a two factor categorical variable ethnicity (caucasion or african american).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wage <-}\StringTok{ }\NormalTok{wage_all}\OperatorTok{$}\NormalTok{wage}
\NormalTok{education <-}\StringTok{ }\NormalTok{wage_all}\OperatorTok{$}\NormalTok{education}
\NormalTok{ethnicity <-}\StringTok{ }\NormalTok{wage_all}\OperatorTok{$}\NormalTok{ethnicity}
\NormalTok{wage_educ <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{wage =}\NormalTok{ wage, }\DataTypeTok{education =}\NormalTok{ education, }\DataTypeTok{ethnicity =}\NormalTok{ ethnicity)}
\KeywordTok{ggpairs}\NormalTok{(wage_educ)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-74-1.pdf}

A bit more revealing. Nothing is Gaussian here! There are probably several Gaussians or other distributions at work in each of the wage and education densities. The ethnicity variate is binary. The wage-education scatterplots is amazingly opaque as well. We can explore a bit more by looking at the two histograms in teh bottom left pane: African Americans in the survey have significantly lower wages and education, and tightly dispersed, relative to caucasian americans. We don't need any further diagnosis about what to estimate here. Our plan of action will be to build two statistical relationships, one each for the ethnicity category.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wage_educ }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{( ethnicity ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{( }\StringTok{`}\DataTypeTok{mean wage}\StringTok{`}\NormalTok{ =}\StringTok{ }\KeywordTok{mean}\NormalTok{(wage), }\StringTok{`}\DataTypeTok{mean education}\StringTok{`}\NormalTok{ =}\StringTok{ }\KeywordTok{mean}\NormalTok{(education) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 3
##   ethnicity `mean wage` `mean education`
##   <fct>           <dbl>            <dbl>
## 1 cauc             617.             13.1
## 2 afam             447.             12.3
\end{verbatim}

Let's filter only \texttt{afam} observations with years of education less than or equala to 8 into our wage model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wage_educ_afam <-}\StringTok{ }\NormalTok{wage_educ }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{( ethnicity }\OperatorTok{==}\StringTok{ "afam"}\NormalTok{, education }\OperatorTok{<=}\StringTok{ }\DecValTok{8}\NormalTok{)}
\NormalTok{wage_educ_afam }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarize}\NormalTok{( }\DataTypeTok{mean_wage =} \KeywordTok{mean}\NormalTok{(wage), }\DataTypeTok{sd_wage =} \KeywordTok{sd}\NormalTok{(wage))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 2
##   mean_wage sd_wage
##       <dbl>   <dbl>
## 1      348.    219.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{precis}\NormalTok{(wage_educ_afam)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             mean     sd 5.5% 94.5%       histogram
## wage      347.56 218.71  117   712 â–â–…â–‡â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–
## education   6.12   2.16    2     8        â–â–â–â–‚â–‚â–‚â–…â–‡
## ethnicity    NaN     NA   NA    NA
\end{verbatim}

\hypertarget{a-model-emerges}{%
\subsection{A model emerges}\label{a-model-emerges}}

The likelihood for our model will be for wages \(w_i\)

\[w_i \sim \operatorname{Normal}(\mu, \sigma),\]
our \(\mu\) prior will be

\[\mu \sim \operatorname{Normal}(178, 20),\]

and our prior for \(\sigma\) will be

\[\sigma \sim \operatorname{Uniform}(0, 400).\]

Here's the shape of the prior for \(\mu\) in \(N(347, 218)\), using the statistics from the \texttt{precis()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \DecValTok{300}\NormalTok{, }\DataTypeTok{to =} \DecValTok{600}\NormalTok{, }\DataTypeTok{by =} \FloatTok{.1}\NormalTok{)), }
       \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{y =} \KeywordTok{dnorm}\NormalTok{(x, }\DataTypeTok{mean =} \DecValTok{446}\NormalTok{, }\DataTypeTok{sd =} \DecValTok{12}\NormalTok{))) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"density"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-77-1.pdf}

Let's also view a prior for \(\sigma\), a uniform distribution with a minimum value of 0 and a maximum value of 50. We don't really need the y axis when looking at the shapes of a density, so we'll just remove it with \texttt{scale\_y\_continuous()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \DecValTok{0}\NormalTok{, }\DataTypeTok{to =} \DecValTok{400}\NormalTok{, }\DataTypeTok{by =} \FloatTok{.1}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{  }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{y =} \KeywordTok{dunif}\NormalTok{(x, }\DataTypeTok{min =} \DecValTok{0}\NormalTok{, }\DataTypeTok{max =} \DecValTok{400}\NormalTok{))) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{breaks =} \OtherTok{NULL}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{panel.grid =} \KeywordTok{element_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-78-1.pdf}

We can simulate from both priors at once to get a prior probability distribution of \texttt{wage}. The \texttt{mutate()} function does the hard work of convolving the two priors into one prior predictive distribution.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\FloatTok{1e4}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{4}\NormalTok{)}
\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{sample_mu    =} \KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{mean =} \DecValTok{446}\NormalTok{,       }\DataTypeTok{sd  =} \DecValTok{218}\NormalTok{),}
       \DataTypeTok{sample_sigma =} \KeywordTok{runif}\NormalTok{(n, }\DataTypeTok{min  =} \DecValTok{0}\NormalTok{,         }\DataTypeTok{max =} \DecValTok{400}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{mean =}\NormalTok{ sample_mu, }\DataTypeTok{sd  =}\NormalTok{ sample_sigma)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{0}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{breaks =} \OtherTok{NULL}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{subtitle =} \KeywordTok{expression}\NormalTok{(Prior}\OperatorTok{~}\NormalTok{predictive}\OperatorTok{~}\NormalTok{distribution}\OperatorTok{~}\StringTok{"for"}\OperatorTok{~}\KeywordTok{italic}\NormalTok{(w[i])),}
       \DataTypeTok{x =} \OtherTok{NULL}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{panel.grid =} \KeywordTok{element_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-79-1.pdf}
\#\#\# Grid approximation of the posterior distribution.

As McElreath explained, we will very likely never use the grid approximation approach for practical data analysis. But this example is simple enough to use and also provides some insight into the procedures that we will deploy under the cover of quadratic approximation and Markov Chain Monte Carlo techniques. They all will converge in the neighborhood of the same solution.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\DecValTok{100}
\NormalTok{wage_grid <-}
\StringTok{  }\CommentTok{# we'll accomplish with `tidyr::crossing()` what McElreath did with base R `expand.grid()`}
\StringTok{  }\KeywordTok{crossing}\NormalTok{(}\DataTypeTok{mu    =} \KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \DecValTok{300}\NormalTok{, }\DataTypeTok{to =} \DecValTok{400}\NormalTok{, }\DataTypeTok{length.out =}\NormalTok{ n),}
           \DataTypeTok{sigma =} \KeywordTok{seq}\NormalTok{(}\DataTypeTok{from =} \DecValTok{100}\NormalTok{,   }\DataTypeTok{to =} \DecValTok{300}\NormalTok{,   }\DataTypeTok{length.out =}\NormalTok{ n))}
\KeywordTok{glimpse}\NormalTok{(wage_grid)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 10,000
## Columns: 2
## $ mu    <dbl> 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300, 300,~
## $ sigma <dbl> 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124,~
\end{verbatim}

We are literally combining a list of hypothetical \(\mu\)s with hypothetical \(\sigma\)s: 100 \(\mu\)s each fanning across 100 \(\sigma\)s yields \$100 \times 100= \$ the 10,000 rows. Let's write a function to calculate the likelihood of each \(\mu, \sigma\) hypothesis. We notice that we are in \texttt{log()} world now to prevent under- and over-flow (really small, really large) numbers. This is where data finally gets into the picture.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid_function <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(mu, sigma) \{}
  
  \KeywordTok{dnorm}\NormalTok{(wage_educ_afam}\OperatorTok{$}\NormalTok{wage, }\DataTypeTok{mean =}\NormalTok{ mu, }\DataTypeTok{sd =}\NormalTok{ sigma, }\DataTypeTok{log =}\NormalTok{ T) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{sum}\NormalTok{()}
  
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We calculate the log likelihood with the function and then mutate priors, sums of logs (same as products), and finally the posterior probabilities into the tibble

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wage_grid <-}
\StringTok{  }\NormalTok{wage_grid }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{log_likelihood =} \KeywordTok{map2}\NormalTok{(mu, sigma, grid_function)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{unnest}\NormalTok{(log_likelihood) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{prior_mu    =} \KeywordTok{dnorm}\NormalTok{(mu,    }\DataTypeTok{mean =} \DecValTok{446}\NormalTok{, }\DataTypeTok{sd  =} \DecValTok{218}\NormalTok{, }\DataTypeTok{log =}\NormalTok{ T),}
         \DataTypeTok{prior_sigma =} \KeywordTok{dunif}\NormalTok{(sigma, }\DataTypeTok{min  =} \DecValTok{0}\NormalTok{,   }\DataTypeTok{max =} \DecValTok{300}\NormalTok{, }\DataTypeTok{log =}\NormalTok{ T)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{product =}\NormalTok{ log_likelihood }\OperatorTok{+}\StringTok{ }\NormalTok{prior_mu }\OperatorTok{+}\StringTok{ }\NormalTok{prior_sigma) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{probability =} \KeywordTok{exp}\NormalTok{(product }\OperatorTok{-}\StringTok{ }\KeywordTok{max}\NormalTok{(product)))}
  
\KeywordTok{head}\NormalTok{(wage_grid)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 7
##      mu sigma log_likelihood prior_mu prior_sigma product probability
##   <dbl> <dbl>          <dbl>    <dbl>       <dbl>   <dbl>       <dbl>
## 1   300  100          -1395.    -6.53       -5.70  -1407.    2.58e-92
## 2   300  102.         -1381.    -6.53       -5.70  -1393.    1.91e-86
## 3   300  104.         -1369.    -6.53       -5.70  -1381.    5.69e-81
## 4   300  106.         -1357.    -6.53       -5.70  -1369.    7.30e-76
## 5   300  108.         -1346.    -6.53       -5.70  -1358.    4.31e-71
## 6   300  110.         -1335.    -6.53       -5.70  -1348.    1.24e-66
\end{verbatim}

In the final \texttt{wage\_grid}, the \texttt{probability} vector contains the posterior probabilities across values of \texttt{mu} and \texttt{sigma}. We can make a contour plot with \texttt{geom\_contour()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wage_grid }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ mu, }\DataTypeTok{y =}\NormalTok{ sigma, }\DataTypeTok{z =}\NormalTok{ probability)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_contour}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{expression}\NormalTok{(mu),}
       \DataTypeTok{y =} \KeywordTok{expression}\NormalTok{(sigma)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_cartesian}\NormalTok{(}\DataTypeTok{xlim =} \KeywordTok{range}\NormalTok{(wage_grid}\OperatorTok{$}\NormalTok{mu),}
                  \DataTypeTok{ylim =} \KeywordTok{range}\NormalTok{(wage_grid}\OperatorTok{$}\NormalTok{sigma)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{panel.grid =} \KeywordTok{element_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-83-1.pdf}

We'll make our heat map with \texttt{geom\_raster(aes(fill\ =\ probability))}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wage_grid }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ mu, }\DataTypeTok{y =}\NormalTok{ sigma)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_raster}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =}\NormalTok{ probability),}
              \DataTypeTok{interpolate =}\NormalTok{ T) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_viridis_c}\NormalTok{(}\DataTypeTok{option =} \StringTok{"A"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{expression}\NormalTok{(mu),}
       \DataTypeTok{y =} \KeywordTok{expression}\NormalTok{(sigma)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{panel.grid =} \KeywordTok{element_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-84-1.pdf}

Thank you Solomon Kurz for the tidyverse recoding of McElreath's base R work.

\hypertarget{sampling-from-the-posterior.}{%
\subsection{Sampling from the posterior.}\label{sampling-from-the-posterior.}}

Let's continue, we are not finished. Our next step is to use the posterior probability distributoin to generate samples. We will use the \texttt{dplyr::sample\_n()}funtion to sample rows, with replacement, from \texttt{wage\_grid}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{4}\NormalTok{)}
\NormalTok{wage_grid_samples <-}\StringTok{ }\NormalTok{wage_grid }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{sample_n}\NormalTok{(}\DataTypeTok{size =} \FloatTok{1e4}\NormalTok{, }\DataTypeTok{replace =}\NormalTok{ T, }\DataTypeTok{weight =}\NormalTok{ probability)}
\NormalTok{wage_grid_samples }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ mu, }\DataTypeTok{y =}\NormalTok{ sigma)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \FloatTok{.9}\NormalTok{, }\DataTypeTok{alpha =} \DecValTok{1}\OperatorTok{/}\DecValTok{15}\NormalTok{, }\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_viridis_c}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{expression}\NormalTok{(mu[samples]),}
       \DataTypeTok{y =} \KeywordTok{expression}\NormalTok{(sigma[samples])) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{panel.grid =} \KeywordTok{element_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-85-1.pdf}

This sample of posteriors looks (a phenomenology is at work here ) a lot like the ellipses and volcanic (super-nebula?) heat map we just produced. The purpose of the sampling is to provide us with many more implicit interpolations of the original grid. We will use \texttt{gather()} and then \texttt{facet\_wrap()} to plot the densities for both \(\mu\) and \(\sigma\) facets into one plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wage_grid_samples }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\CommentTok{#select(mu, sigma) %>% }
\StringTok{  }\KeywordTok{gather}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ value)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{0}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{breaks =} \OtherTok{NULL}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\OtherTok{NULL}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{panel.grid =} \KeywordTok{element_blank}\NormalTok{()) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\NormalTok{key, }\DataTypeTok{scales =} \StringTok{"free"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-86-1.pdf}

Next We use the tidybayes package to compute their posterior modes and 95\% HPDIs (a move from mere phenomena to episteme or knowledge). We impose a probability on the areas under the posterior densities. There is not a single statistical reason for doing this. But if we widen our horizons and think a little about how important our study is for hiring, firing, the competitive search to talent, the policies we have systemically, then that informs our choice of a structure imposed on the phenomenologically situated tatistics.

This is also where the long format table generated by \texttt{gather()} earns its keep. The \texttt{key} allows us to \texttt{group()} the two estimates in \texttt{value} with all of the grid probabilities matched to the two parameters.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidybayes)}
\NormalTok{wage_grid_samples }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\CommentTok{#select(mu, sigma) %>% }
\StringTok{  }\KeywordTok{gather}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(key) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mode_hdi}\NormalTok{(value)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 11 x 7
##    key               value     .lower    .upper .width .point .interval
##    <chr>             <dbl>      <dbl>     <dbl>  <dbl> <chr>  <chr>    
##  1 log_likelihood -1184.   -1188.     -1187.      0.95 mode   hdi      
##  2 log_likelihood -1184.   -1187.     -1184.      0.95 mode   hdi      
##  3 mu               348      316.       381.      0.95 mode   hdi      
##  4 prior_mu          -6.40    -6.48      -6.35    0.95 mode   hdi      
##  5 prior_sigma       -5.70    -5.70      -5.70    0.95 mode   hdi      
##  6 probability        1        0.0197     0.417   0.95 mode   hdi      
##  7 probability        1        0.432      1       0.95 mode   hdi      
##  8 product        -1196.   -1200.     -1200.      0.95 mode   hdi      
##  9 product        -1196.   -1200.     -1199.      0.95 mode   hdi      
## 10 product        -1196.   -1199.     -1196.      0.95 mode   hdi      
## 11 sigma            219.     197.       243.      0.95 mode   hdi
\end{verbatim}

Let's say you wanted their posterior medians and 50\% quantile-based intervals, instead. Just switch out the last line for \texttt{median\_qi(value,\ .width\ =\ .5)}. It is not a surprise that we get back the prior estimates of mu and sigma. What we are after with inference is the range of estimates that are compatible with prior beliefs and the data, here with 95\% credibility.

\hypertarget{regressing-away}{%
\section{Regressing away}\label{regressing-away}}

Once again we travelled through the entire work flow for a two parameter model. This model is a subset of a regression model with intercept only. Onward to the simple linear regression. While it is possible to use the \texttt{crossing()} function for the three parameters (intercept, slope, sigma) of the simple linear regression, we will not do that. FOr that exercise entails a \$100\^{}3 = \$ 1 million row grid. There are more efficient ways to estimate the posterior probabilities. Instead we will use the \texttt{quap()} quadratic approximation routine to generate Maximum A Posteriori (MAP) point estimates, which we then use to generate samples of the posterior distribution.

For the simple two parameter model of wages we have

\begin{align}
w_i &\sim \operatorname{Normal}(\mu, \sigma) \\
\mu &\sim \operatorname{Normal}(347, 218) \\
\sigma &\sim \operatorname{Uniform}(0, 400)
\end{align}

We write this model into R this way, as a list. If we try the uniform distribution for \texttt{sigma} with this data, we will run into non-finite numerical derivatives in the \texttt{quap()} routine. That means we cannot climb the posterior probability hill from the \texttt{sigma} direction. No solution is available for us. Much experience with the probability distribution of volatility might direct us to use an exponential function. The following will render results compatible with a simplistic view of the data as mean and standard deviation.

What if we concentrate the prior of the mean to a nearly certain outcome?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model <-}\StringTok{ }\KeywordTok{alist}\NormalTok{(}
\NormalTok{wage }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{( mu , sigma ) ,}
\NormalTok{mu }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{( }\DecValTok{400}\NormalTok{ , }\FloatTok{.2}\NormalTok{ ),}
\NormalTok{sigma }\OperatorTok{~}\StringTok{ }\KeywordTok{dexp}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{)}
\NormalTok{data <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \DataTypeTok{data=}\NormalTok{wage_educ_afam}\OperatorTok{$}\NormalTok{wage}
\NormalTok{  )}
\NormalTok{start <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}
  \DataTypeTok{mu =} \KeywordTok{mean}\NormalTok{(data}\OperatorTok{$}\NormalTok{data),}
  \DataTypeTok{sigma =} \KeywordTok{sd}\NormalTok{(data}\OperatorTok{$}\NormalTok{data)}
\NormalTok{)}

\NormalTok{wage_afam_fit <-}\StringTok{ }\KeywordTok{quap}\NormalTok{( model , }\DataTypeTok{data =}\NormalTok{ data, }\DataTypeTok{start =}\NormalTok{ start )}
\KeywordTok{summary}\NormalTok{(wage_afam_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       mean    sd 5.5% 94.5%
## mu     402 0.199  402   402
## sigma  335 0.848  334   337
\end{verbatim}

Using a very concentrated prior for \(\mu\) will allow our robot to compute a maximized posterior probability very quickly. With that in hand the robot then computes the conditional \(\sigma\). More importantly we get a compatibility interval that starkly illustrates the tight range we see computed.

Now for the samples.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rethinking)}
\KeywordTok{library}\NormalTok{(MASS)}
\NormalTok{var_cov <-}\StringTok{ }\KeywordTok{vcov}\NormalTok{(wage_afam_fit)}
\NormalTok{post <-}\StringTok{ }\KeywordTok{mvrnorm}\NormalTok{( }\DataTypeTok{n=}\FloatTok{1e4}\NormalTok{ , }\DataTypeTok{mu=}\KeywordTok{coef}\NormalTok{(wage_afam_fit) , }\DataTypeTok{Sigma=}\NormalTok{var_cov )}
\KeywordTok{summary}\NormalTok{( post )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        mu          sigma    
##  Min.   :401   Min.   :332  
##  1st Qu.:402   1st Qu.:335  
##  Median :402   Median :335  
##  Mean   :402   Mean   :335  
##  3rd Qu.:402   3rd Qu.:336  
##  Max.   :403   Max.   :339
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{post <-}\StringTok{ }\KeywordTok{extract.samples}\NormalTok{(wage_afam_fit)}
\KeywordTok{head}\NormalTok{( post )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    mu sigma
## 1 402   335
## 2 402   334
## 3 402   335
## 4 402   337
## 5 402   336
## 6 402   336
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{precis}\NormalTok{( post )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       mean    sd 5.5% 94.5%     histogram
## mu     402 0.198  402   402      â–â–â–‚â–‡â–‡â–ƒâ–â–
## sigma  335 0.840  334   337 â–â–â–â–â–ƒâ–…â–‡â–‡â–ƒâ–‚â–â–â–
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{post }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as_tibble}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ value)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{0}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{breaks =} \OtherTok{NULL}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\OtherTok{NULL}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{panel.grid =} \KeywordTok{element_blank}\NormalTok{()) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\NormalTok{key, }\DataTypeTok{scales =} \StringTok{"free"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-89-1.pdf}

\hypertarget{adding-some-education}{%
\subsection{Adding some education}\label{adding-some-education}}

Let's move on to explaining variations in wage with years of education, \(x_i\). We now explore the one factor regression model

\begin{align}
w_i &\sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i &\sim \alpha + \beta (x_i - \overline{x}) \\
\alpha &\sim \operatorname{Normal}(300, 20) \\
\beta &\sim \operatorname{Log-Normal}(0, 1) \\
\sigma &\sim \operatorname{Uniform}(0, 400)
\end{align}

Things are a little different now. The \(\mu\)s are legion. The \(\mu_i\)
are a pass-through for the two free parameters \(\alpha\) and \(\beta\), conditional on \(x_i\) into the normality of wages.There is still one \(\sigma\) a requirement of this style of regressions. When we center the education variable \(x_i\) about its mean \(\overline{x}\), and at the possible junction of \(x_i = \overline{x}\), then we are back to \(\mu = \alpha\),

Let's code this up.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# define the average years of education, x-bar}
\NormalTok{xbar <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(wage_educ_afam}\OperatorTok{$}\NormalTok{education)}
\CommentTok{# fit model}
\NormalTok{fit_wage_educ <-}\StringTok{ }\KeywordTok{quap}\NormalTok{(}
\KeywordTok{alist}\NormalTok{(}
\NormalTok{  wage }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{( mu , sigma ) ,}
\NormalTok{  mu <-}\StringTok{ }\NormalTok{a }\OperatorTok{+}\StringTok{ }\NormalTok{b}\OperatorTok{*}\NormalTok{( education }\OperatorTok{-}\StringTok{ }\NormalTok{xbar ) ,}
\NormalTok{  a }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{( }\DecValTok{300}\NormalTok{ , }\DecValTok{20}\NormalTok{ ) ,}
\NormalTok{  b }\OperatorTok{~}\StringTok{ }\KeywordTok{dlnorm}\NormalTok{( }\DecValTok{0}\NormalTok{ , }\DecValTok{2}\NormalTok{) ,}
\NormalTok{  sigma }\OperatorTok{~}\StringTok{ }\KeywordTok{dexp}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{) , }\DataTypeTok{data=}\NormalTok{wage_educ_afam )}
\KeywordTok{precis}\NormalTok{( fit_wage_educ)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        mean    sd  5.5% 94.5%
## a     346.4 3.177 341.3 351.4
## b      19.5 1.499  17.1  21.9
## sigma  42.4 0.371  41.9  43.0
\end{verbatim}

Now we extract and examine some samples.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{var_cov <-}\StringTok{ }\KeywordTok{vcov}\NormalTok{(fit_wage_educ)}
\CommentTok{#}
\NormalTok{post_samples <-}\StringTok{ }\KeywordTok{extract.samples}\NormalTok{(fit_wage_educ)}
\KeywordTok{head}\NormalTok{( post_samples )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     a    b sigma
## 1 341 20.6  43.1
## 2 350 19.2  42.3
## 3 347 19.5  42.4
## 4 341 18.9  42.5
## 5 347 18.9  42.7
## 6 345 20.1  42.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{precis}\NormalTok{( post_samples )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        mean    sd  5.5% 94.5%       histogram
## a     346.4 3.183 341.3 351.4   â–â–â–â–‚â–ƒâ–‡â–‡â–…â–‚â–â–â–â–
## b      19.5 1.502  17.1  21.9    â–â–â–â–â–ƒâ–‡â–‡â–‡â–ƒâ–â–â–
## sigma  42.4 0.368  41.9  43.0 â–â–â–â–â–‚â–…â–‡â–‡â–‡â–ƒâ–‚â–â–â–â–
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{post }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as_tibble}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ value)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{size =} \DecValTok{0}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_y_continuous}\NormalTok{(}\OtherTok{NULL}\NormalTok{, }\DataTypeTok{breaks =} \OtherTok{NULL}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\OtherTok{NULL}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{panel.grid =} \KeywordTok{element_blank}\NormalTok{()) }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\NormalTok{key, }\DataTypeTok{scales =} \StringTok{"free"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-91-1.pdf}

Next We use the \textbf{tidybayes} package to compute posterior modes and 95\% HPDIs (we recall our move from mere phenomena to \emph{episteme} or knowledge). We again impose a probability structure on the areas under the posterior densities.

Again this is also where the long format table generated by \texttt{gather()} earns its keep. The \texttt{key} allows us to \texttt{group()} the two estimates in \texttt{value} with all of the grid probabilities matched to the two parameters. Reusing the code from the mean - standard deviation model of wages, let's see if this works yet again.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidybayes)}
\NormalTok{post_samples }\OperatorTok{%>%}\StringTok{ }\KeywordTok{tibble}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{gather}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ key, }\DataTypeTok{value =}\NormalTok{ value) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(key) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mode_hdi}\NormalTok{(value)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 7
##   key   value .lower .upper .width .point .interval
##   <chr> <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    
## 1 a     347.   340.   352.    0.95 mode   hdi      
## 2 b      19.5   16.6   22.4   0.95 mode   hdi      
## 3 sigma  42.5   41.7   43.2   0.95 mode   hdi
\end{verbatim}

Even the volatility of our statistical relationship has a credible range. Whether we have a good model or bad still remains to be seen, and thought through, for implications for decision makers. Our rubric here is that \emph{seeing (touching, feeling, smelling, etc.) is not knowing} .

\hypertarget{adding-some-depth}{%
\section{Adding some depth}\label{adding-some-depth}}

\hypertarget{linear-really}{%
\subsection{Linear, really?}\label{linear-really}}

Any additive model is linear according to the algebraists. Let's add a quadratic term, and another possible explanatory variable to the model. But is it really an explanatory variable? What a quadratic term really does is help express the shape of the relationship, here between wages and education. We make a parabola. There is not necessarily anything causative in the decision maker's mind. An economist might echo that sentiment, and add, of course, that there might be decreasing returns to wages by adding more years of education. Adding more years also can mean a person trades off income production for more education. For those who work full-time and pursue education part-time, partial years of education are accumulated over several years.

Let's first pull in a quadratic term. Let's first look at the scatter of wage versus both a nomial of order 1, \(X_i = (x_i - \overline{x})^1\), and another nomial of order 2 \(X_i^2 = (x_i - \overline{x})^2\)

\begin{align}
w_i &\sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i &\sim \beta_0 + \beta_1 X_i + \beta_2 X_i^2 \\
\beta_0 &\sim \operatorname{Normal}(300, 20) \\
\beta_1 &\sim \operatorname{Log-Normal}(0, 1) \\
\beta_1 &\sim \operatorname{Log-Normal}(0, 1) \\
\sigma &\sim \operatorname{Uniform}(0, 400)
\end{align}

\hypertarget{enough-is-never-enough}{%
\section{Enough is never enough}\label{enough-is-never-enough}}

We now pull \texttt{experience} into the mix to view, phenomenologically, and maybe epistemically if we have an idea of the relationship in mind, the impacts on wages.

Was that enough? For now at least. We accomplished a lot. But perhaps the biggest takeaway is our new found ability to deploy simulation which generates the probability of the compatibility of hypotheses with data. All this allows us to arrive at learning about the data in a principled manner. We learned because we inferred.

Up next, we gather our wits together to rebuild the workhorse model, the linear regression, in the image and likeness of a probabilistic simulator.

\hypertarget{session-information}{%
\section*{Session Information}\label{session-information}}
\addcontentsline{toc}{section}{Session Information}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sessionInfo}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## R version 4.4.1 (2024-06-14 ucrt)
## Platform: x86_64-w64-mingw32/x64
## Running under: Windows 11 x64 (build 26100)
## 
## Matrix products: default
## 
## 
## locale:
## [1] LC_COLLATE=English_United States.utf8 
## [2] LC_CTYPE=English_United States.utf8   
## [3] LC_MONETARY=English_United States.utf8
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.utf8    
## 
## time zone: America/New_York
## tzcode source: internal
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods  
## [8] base     
## 
## other attached packages:
##  [1] MASS_7.3-60.2     AER_1.2-15        survival_3.6-4    sandwich_3.1-1   
##  [5] lmtest_0.9-40     zoo_1.8-14        car_3.1-3         carData_3.0-5    
##  [9] coda_0.19-4.1     mvtnorm_1.3-3     tidybayes_3.0.7   dagitty_0.3-4    
## [13] moments_0.14.1    visNetwork_2.1.2  ggthemes_5.1.0    gifski_1.32.0-2  
## [17] hershey_0.1.0     ggforce_0.5.0     gganimate_1.0.10  GGally_2.3.0     
## [21] rethinking_2.40   posterior_1.6.1   cmdstanr_0.9.0    plotly_4.11.0    
## [25] d3Network_0.5.2.1 lubridate_1.9.4   forcats_1.0.0     stringr_1.5.1    
## [29] dplyr_1.1.4       purrr_1.1.0       readr_2.1.5       tidyr_1.3.1      
## [33] tibble_3.3.0      ggplot2_3.5.2     tidyverse_2.0.0   knitr_1.50       
## [37] kableExtra_1.4.0 
## 
## loaded via a namespace (and not attached):
##  [1] rlang_1.1.4          magrittr_2.0.3       matrixStats_1.5.0   
##  [4] compiler_4.4.1       loo_2.8.0            systemfonts_1.2.3   
##  [7] vctrs_0.6.5          pkgconfig_2.0.3      shape_1.4.6.1       
## [10] arrayhelpers_1.1-0   crayon_1.5.3         fastmap_1.2.0       
## [13] backports_1.5.0      labeling_0.4.3       utf8_1.2.6          
## [16] rmarkdown_2.29       tzdb_0.5.0           ps_1.9.1            
## [19] bit_4.6.0            xfun_0.52            jsonlite_2.0.0      
## [22] progress_1.2.3       tweenr_2.0.3         prettyunits_1.2.0   
## [25] R6_2.6.1             stringi_1.8.7        RColorBrewer_1.1-3  
## [28] boot_1.3-30          Rcpp_1.1.0           bookdown_0.43       
## [31] Matrix_1.7-0         splines_4.4.1        timechange_0.3.0    
## [34] tidyselect_1.2.1     rstudioapi_0.17.1    dichromat_2.0-0.1   
## [37] abind_1.4-8          yaml_2.3.10          curl_6.4.0          
## [40] processx_3.8.6       lattice_0.22-6       plyr_1.8.9          
## [43] withr_3.0.2          S7_0.2.0             evaluate_1.0.4      
## [46] isoband_0.2.7        ggstats_0.10.0       polyclip_1.10-7     
## [49] ggdist_3.3.3         xml2_1.3.8           pillar_1.11.0       
## [52] tensorA_0.36.2.1     whisker_0.4.1        checkmate_2.3.2     
## [55] distributional_0.5.0 generics_0.1.4       vroom_1.6.5         
## [58] hms_1.1.3            scales_1.4.0         glue_1.8.0          
## [61] lazyeval_0.2.2       tools_4.4.1          data.table_1.17.8   
## [64] grid_4.4.1           Formula_1.2-5        cli_3.6.5           
## [67] textshaping_1.0.1    svUnit_1.0.6         viridisLite_0.4.2   
## [70] svglite_2.2.1        V8_6.0.5             gtable_0.3.6        
## [73] digest_0.6.36        rjson_0.2.23         htmlwidgets_1.6.4   
## [76] farver_2.1.2         htmltools_0.5.8.1    lifecycle_1.0.4     
## [79] httr_1.4.7           bit64_4.6.0-1
\end{verbatim}

\hypertarget{waic-up-going-beyond-the-f-test}{%
\chapter{WAIC up! Going beyond the F-test}\label{waic-up-going-beyond-the-f-test}}

\hypertarget{deviance}{%
\section{Deviance}\label{deviance}}

\emph{\emph{WAIC}} stands for \_\_\_\_ \_\_\_\_ ? Yes, it an information criterion (IC) and dutifully computes a statistic that tests the ability of a model to predict. Log odds ratios are involved, as well as the number of parameters, and thus rudimentary measure of the complexity of a model. This criterion approximates the out-of-sample deviance that converges to the cross-validation approximation in a large sample.

A little bit of background is in order. The \emph{W} and the \emph{A} in \(WAIC\) can easily refer to \citet{WatanabeAsymptoticEquivalenceBayes2010} and \citet{Akaike1998}, as well as the more colloquial \emph{Widely Available Information Criteria}.
\footnote{waic-names: Also we should be sure to distinguish singular from regular learning models as in \citet{WatanabeAlgebraicGeometryStatisticalLearning2009}. The rendering of WAIC here follows very closely, sometimes exactly, \citet{McelreathStatisticalRethinkingBayesian2020}'s excellent examples and exposition.}

The cross-validation approximation, especially using the \(N\)-fold \emph{\emph{leave-one-out (LOO)}} strategy results in the \emph{\emph{Pareto-smoothed Importance Sampling (PSIS)}} cross-validation approach. Both WAIC and PSIS go beyond simple goodness of fit, such as, \(R^2\) and ANOVA with its \(F\) distribution. The measure attempts to weigh observations by their importance to the predictive accuracy of the model.

WAIC is the \emph{\emph{log-posterior-predictive density}} (\(lppd\), that is, the Bayesian deviance also used in the PSIS cross-validation approach) and a \emph{\emph{penalty}} proportional to the variance in posterior predictions. The penalty might be analogous to a ridge regression parameter. We are still only concerned now with predictive accuracy, not at all about confounding variables and causal inference, respectively, a local variate and global modeling issues.

\begin{equation}
WAIC(y, \Theta) = âˆ’2(lppd âˆ’ \underbrace{\Sigma_i var_{\theta}\,log  \,\,p(y_i|\theta))}_{penalty}
\end{equation}

The \(lppd\) comes from the notion of \textbf{Kullback-Leibler} model divergence as the additional uncertainty induced by using probabilities from one distribution to describe another distribution. This divergence is often measured by the Kullback-Leibler divergence, KL divergence. The divergence is the distance in log odds ratios between two models. Our two models are everything except the one observation left out, and everything left in. Lots of computing will make our day now.

\footnote{Again we refer to \citet{WatanabeAlgebraicGeometryStatisticalLearning2009} for the details around KL, especially when the Fisher Information Matrix itself is not invertible and therefore only semi-positive definite and thus the underlying observational model is locally singular. Singularity here implies that there is somewhere in the unobserved data space of so-called free parameters where inference using the Fisher Information Matrix is not valid, certainly not computable.}

The Bayesian version of the log-probability score is called the log-pointwise-predictive-density for some data \(y_i\) and posterior distribution \(\theta\). With \(N\) observations and fitting the model \(N\) times, dropping a single observation \(y_i\) each time, then the out-of-sample lppd is the sum of the average accuracy for each omitted y\_i.

\begin{equation}
lppd(y, \Theta) = \Sigma_i \, log \frac{1}{S} \Sigma_s p(y_i | \Theta_s)
\end{equation}

where \(S\) is the number of samples and is the \(s\)-th set of sampled parameter values in the posterior distribution with values \(q_i\).

\begin{equation}
S(q) = \Sigma_i log(q_i)
\end{equation}

Whenever we see logs we know, or at least suspect, that we have scores.

The penalty measure in WAIC is part and parcel of a correction in non-Bayesian models called a \emph{\emph{ridge}} regression. The \emph{ridge} is a parameter that tunes the fit in a regularizing way, much like choosing a fairly tight prior to squeeze only enough information of data and deposit only enough of the likelihood mass into the posterior probability distribution. We can shorten the penalty into an \emph{\emph{effective number of parameters}} statistic \(p_{WAIC}\), much like how the measurement of full-time employed relates to head count.

\hypertarget{taxes-anyone}{%
\section{Taxes anyone?}\label{taxes-anyone}}

Let's use the \texttt{Laffer} dataset loaded with the \texttt{rethinking} package to illustrate what we have so far. We begin a regularization process by standardizing the variables.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rethinking)}
\KeywordTok{data}\NormalTok{(Laffer)}
\KeywordTok{summary}\NormalTok{(Laffer)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     tax_rate     tax_revenue   
##  Min.   : 0.1   Min.   :-0.06  
##  1st Qu.:22.4   1st Qu.: 2.24  
##  Median :28.4   Median : 3.07  
##  Mean   :26.4   Mean   : 3.31  
##  3rd Qu.:33.2   3rd Qu.: 3.58  
##  Max.   :35.4   Max.   :10.02
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d <-}\StringTok{ }\NormalTok{Laffer}
\NormalTok{d}\OperatorTok{$}\NormalTok{T <-}\StringTok{ }\NormalTok{(d}\OperatorTok{$}\NormalTok{tax_rate }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(d}\OperatorTok{$}\NormalTok{tax_rate) ) }\OperatorTok{/}\StringTok{ }\KeywordTok{sd}\NormalTok{(d}\OperatorTok{$}\NormalTok{tax_rate)}
\NormalTok{d}\OperatorTok{$}\NormalTok{R <-}\StringTok{ }\NormalTok{(d}\OperatorTok{$}\NormalTok{tax_revenue }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(d}\OperatorTok{$}\NormalTok{tax_revenue) ) }\OperatorTok{/}\StringTok{ }\KeywordTok{sd}\NormalTok{(d}\OperatorTok{$}\NormalTok{tax_revenue)}
\NormalTok{m <-}\StringTok{ }\KeywordTok{quap}\NormalTok{(}
\KeywordTok{alist}\NormalTok{(}
\NormalTok{  R }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(mu,sigma),}
\NormalTok{  mu <-}\StringTok{ }\NormalTok{a }\OperatorTok{+}\StringTok{ }\NormalTok{b}\OperatorTok{*}\NormalTok{T,}
\NormalTok{  a }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.2}\NormalTok{), }\CommentTok{# regularized priors}
\NormalTok{  b }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.5}\NormalTok{),}
\NormalTok{  sigma }\OperatorTok{~}\StringTok{ }\KeywordTok{dexp}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{) , }\DataTypeTok{data =}\NormalTok{ d )}
\KeywordTok{precis}\NormalTok{( m )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                  mean    sd    5.5% 94.5%
## a     -0.000000000138 0.130 -0.2072 0.207
## b      0.285437294680 0.164  0.0234 0.547
## sigma  0.917242492366 0.118  0.7290 1.106
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{post <-}\StringTok{ }\KeywordTok{extract.samples}\NormalTok{(m,}\DataTypeTok{n=}\DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We compare this with the OLS regression.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m_OLS <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(R }\OperatorTok{~}\StringTok{ }\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{T, }\DataTypeTok{data =}\NormalTok{ d)}
\KeywordTok{summary}\NormalTok{(m_OLS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = R ~ 1 + T, data = d)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -0.958 -0.553 -0.203  0.171  3.625 
## 
## Coefficients:
##                           Estimate             Std. Error t value Pr(>|t|)  
## (Intercept) -0.0000000000000000791  0.1791757159473203465    0.00    1.000  
## T            0.3197441602259718518  0.1823472137720283104    1.75    0.091 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.965 on 27 degrees of freedom
## Multiple R-squared:  0.102,  Adjusted R-squared:  0.069 
## F-statistic: 3.07 on 1 and 27 DF,  p-value: 0.0909
\end{verbatim}

Much the same story emerges here. The F-statistic helps us reject the alternative hypothesis that \textbf{both} the intercept \emph{and} the slope are \emph{not} zero at a stargazingly low degree of plausibility. Ahh, but this routine does not at all help us identify those observations which might more, or less, influence our over-fitting issues. The two models do seem to be in some sort of agreement on the size and direction of the point estimates.

What about our \texttt{quap} model? First, we compute the log-likelihood of each observation \(i\) at each sample \(s\) from the posterior:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n_samples <-}\StringTok{ }\DecValTok{1000}
\NormalTok{log_prob <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{( }\DecValTok{1}\OperatorTok{:}\NormalTok{n_samples ,}
\ControlFlowTok{function}\NormalTok{(s) \{}
\NormalTok{  mu <-}\StringTok{ }\NormalTok{post}\OperatorTok{$}\NormalTok{a[s] }\OperatorTok{+}\StringTok{ }\NormalTok{post}\OperatorTok{$}\NormalTok{b[s]}\OperatorTok{*}\NormalTok{d}\OperatorTok{$}\NormalTok{T}
  \KeywordTok{dnorm}\NormalTok{( d}\OperatorTok{$}\NormalTok{R , mu , post}\OperatorTok{$}\NormalTok{sigma[s] , }\DataTypeTok{log=}\OtherTok{TRUE}\NormalTok{ )}
\NormalTok{\} )}
\KeywordTok{str}\NormalTok{(log_prob)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  num [1:29, 1:1000] -1.066 -0.885 -1.085 -0.907 -0.916 ...
\end{verbatim}

The \texttt{str()} view shows a 1000 samples column matrix with scores for each observation in each row.

\hypertarget{computing-waic}{%
\section{Computing WAIC}\label{computing-waic}}

With all of this derived data available, we now compute \(lppd\), the Bayesian deviance, by averaging the samples in each row, taking the log, and adding all of the logs together (the sum of logs are like the products of probabilities, all logical \emph{both-and}s. We have to use the function log\_sum\_exp to compute the log of a sum of exponentiated terms, else we might be as precise as we need to be, let alone experience under and overflow arithmetical headaches. After all of that we subtract the log of the number of samples, and thus the log of the average.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n_obs <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{( d )}
\NormalTok{lppd <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{( }\DecValTok{1}\OperatorTok{:}\NormalTok{n_obs , }\ControlFlowTok{function}\NormalTok{(i) }\KeywordTok{log_sum_exp}\NormalTok{(log_prob[i,]) }\OperatorTok{-}\StringTok{ }\KeywordTok{log}\NormalTok{(n_samples) )}
\KeywordTok{sum}\NormalTok{(lppd)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -38.4
\end{verbatim}

We will need \texttt{sum(lppd)} for the total \texttt{lppd} in the WAIC formula. The penalty term p\_\{WAIC\} adds up the variance across samples for each and every observation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pWAIC <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{( }\DecValTok{1}\OperatorTok{:}\NormalTok{n_obs , }\ControlFlowTok{function}\NormalTok{(i) }\KeywordTok{var}\NormalTok{(log_prob[i,]) )}
\KeywordTok{sum}\NormalTok{(pWAIC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 7.38
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(pWAIC)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    0.02    0.02    0.02    0.25    0.05    5.92
\end{verbatim}

Again \texttt{sum(pWAIC)} returns total \texttt{pWAIC} in the formula. At last we compute WAIC like this.

\begin{Shaded}
\begin{Highlighting}[]
 \DecValTok{-2}\OperatorTok{*}\NormalTok{( }\KeywordTok{sum}\NormalTok{(lppd) }\OperatorTok{-}\StringTok{ }\KeywordTok{sum}\NormalTok{(pWAIC) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 91.5
\end{verbatim}

We can compute the standard error of WAIC through the square root of number of cases multiplied by the variance over the individual observation terms in WAIC. Why the -2? We get a positive statistic. The 2 apocryphally came from the base 2 number system used by Shannon in developing information theory. The traditional reason is that the sum of lppd and the sum of pWAIC act like a likelihood ratio, and in logs a difference, thus deviance. The numerator and denominators are chi-squared distributed with so many degrees of freedom in parameters and data. Scaling by 2 helped with the construction of likelihood ratio distribution tables. So says the received tradition.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{waic_vec <-}\StringTok{ }\DecValTok{-2}\OperatorTok{*}\NormalTok{( lppd }\OperatorTok{-}\StringTok{ }\NormalTok{pWAIC )}
\KeywordTok{sqrt}\NormalTok{( n_obs}\OperatorTok{*}\KeywordTok{var}\NormalTok{(waic_vec) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 25.2
\end{verbatim}

\hypertarget{influence}{%
\section{Influence}\label{influence}}

Since each observation has a penalty in the \(p_WAIC\) vector, we attempt to identify those observations than contribute to overfitting using the \texttt{pointwise=TRUE} feature in the \texttt{rethinking::WAIC()} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{WAIC}\NormalTok{(m, }\DataTypeTok{pointwise =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     WAIC   lppd penalty std_err
## 1   3.80 -1.422  0.4763    23.9
## 2   1.84 -0.897  0.0247    23.9
## 3   2.25 -1.072  0.0527    23.9
## 4   1.84 -0.896  0.0233    23.9
## 5   1.78 -0.870  0.0196    23.9
## 6   2.05 -0.999  0.0246    23.9
## 7   2.70 -1.299  0.0501    23.9
## 8   2.83 -1.391  0.0265    23.9
## 9   1.73 -0.850  0.0170    23.9
## 10  1.74 -0.854  0.0178    23.9
## 11  4.57 -2.131  0.1545    23.9
## 12 26.36 -7.575  5.6032    23.9
## 13  3.58 -1.714  0.0773    23.9
## 14  2.46 -1.208  0.0207    23.9
## 15  1.70 -0.832  0.0178    23.9
## 16  1.72 -0.841  0.0179    23.9
## 17  1.71 -0.835  0.0178    23.9
## 18  1.73 -0.846  0.0177    23.9
## 19  2.98 -1.430  0.0614    23.9
## 20  1.75 -0.857  0.0178    23.9
## 21  1.85 -0.909  0.0177    23.9
## 22  1.73 -0.849  0.0182    23.9
## 23  1.80 -0.881  0.0201    23.9
## 24  1.93 -0.942  0.0231    23.9
## 25  2.06 -1.007  0.0236    23.9
## 26  2.55 -1.235  0.0387    23.9
## 27  2.66 -1.280  0.0500    23.9
## 28  2.82 -1.342  0.0677    23.9
## 29  2.16 -1.049  0.0327    23.9
\end{verbatim}

Yes, the 12th observation has the highest deviance (WAIC version thereof) and the highest variance of the posterior distribution, that is, the highest penalty. Influential? Informative? Surprising? Again, yes, yes and yes.

\hypertarget{another-model}{%
\section{Another model?}\label{another-model}}

Of course! Let's estimate then compare this model with the basic Laffer model above. Let's compare the Gaussian model with a so-called robust model using the Student's-t distribution with somewhat thicker tails. The 2 degrees of freedom will definitely thicken those tails.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# linear model with student-t}
\NormalTok{m_t <-}\StringTok{ }\KeywordTok{quap}\NormalTok{( }\KeywordTok{alist}\NormalTok{(}
\NormalTok{  R }\OperatorTok{~}\StringTok{ }\KeywordTok{dstudent}\NormalTok{( }\DecValTok{2}\NormalTok{ , mu , sigma ),}
\NormalTok{  mu <-}\StringTok{ }\NormalTok{a }\OperatorTok{+}\StringTok{ }\NormalTok{b}\OperatorTok{*}\NormalTok{T,}
\NormalTok{  a }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{( }\DecValTok{0}\NormalTok{ , }\FloatTok{0.2}\NormalTok{ ), }\CommentTok{# regularized priors}
\NormalTok{  b }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{( }\DecValTok{0}\NormalTok{ , }\FloatTok{0.5}\NormalTok{ ),}
\NormalTok{  sigma }\OperatorTok{~}\StringTok{ }\KeywordTok{dexp}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{) , }\DataTypeTok{data=}\NormalTok{d )}
\KeywordTok{precis}\NormalTok{( m_t )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         mean     sd     5.5%   94.5%
## a     -0.172 0.0954 -0.32446 -0.0196
## b      0.192 0.1234 -0.00513  0.3893
## sigma  0.444 0.0948  0.29282  0.5959
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{WAIC}\NormalTok{( m_t )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   WAIC  lppd penalty std_err
## 1 74.6 -33.4    3.89    13.7
\end{verbatim}

Now let's compare the two models.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# the default comparison is our new friend WAIC}
\KeywordTok{compare}\NormalTok{( m, m_t )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     WAIC   SE dWAIC  dSE pWAIC   weight
## m_t 75.0 13.5   0.0   NA  4.07 0.999205
## m   89.3 22.7  14.3 14.8  6.17 0.000795
\end{verbatim}

Which would you choose? Here is where we can stand:

\begin{itemize}
\item
  Goodness of fit always favor more complex models
\item
  Information divergence is the right measure of model accuracy, but it too leads to more complex models
\item
  Regularizing priors skeptical of extreme parameter values will begin to trade off complexity with accuracy and thus reduce overfitting
\item
  Use of multiple criteria: cross-validation (LOO), PSIS, WAIC will complement regularizing priors.
\end{itemize}

So onward to model selection. The first right thing we did was contrive two different models. One will deliver mesokurtic tails, the other leptokurtic tails. So which one? The one with the lower of the criterion values because these are deviance, distance, divergence measures. Less divergence means better predictive capability. Less deviance from the data means more plausible hypotheses that are compatible with, consistent with, the data. On this basis we choose the robust Student's-t model. But that will this help us determine the best predictive model?

Anticipating the best predictive model we can compare the two as the difference in \texttt{WAIC} scores.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{waic1 <-}\StringTok{ }\KeywordTok{WAIC}\NormalTok{( m , }\DataTypeTok{pointwise=}\OtherTok{TRUE}\NormalTok{ )}
\NormalTok{waic2 <-}\StringTok{ }\KeywordTok{WAIC}\NormalTok{( m_t , }\DataTypeTok{pointwise=}\OtherTok{TRUE}\NormalTok{ )}
\NormalTok{dwaic <-}\StringTok{ }\NormalTok{waic1 }\OperatorTok{-}\StringTok{ }\NormalTok{waic2}
\NormalTok{plt_title <-}\StringTok{ "Taxes: model comparison"}
\NormalTok{waic_effort <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{( }\DataTypeTok{dwaic =}\NormalTok{ dwaic}\OperatorTok{$}\NormalTok{WAIC, }\DataTypeTok{T =}\NormalTok{ d}\OperatorTok{$}\NormalTok{T, }\DataTypeTok{tid =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"tax id "}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{29}\NormalTok{) )}
\NormalTok{plt <-}\StringTok{ }\NormalTok{waic_effort }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ dwaic, }\DataTypeTok{y =}\NormalTok{ T, }\DataTypeTok{label =}\NormalTok{ tid)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_vline}\NormalTok{( }\DataTypeTok{xintercept =} \DecValTok{0}\NormalTok{, }\DataTypeTok{color =} \StringTok{"red"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{color =} \StringTok{"blue"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_text}\NormalTok{(}\KeywordTok{aes}\NormalTok{( }\DataTypeTok{label =} \KeywordTok{ifelse}\NormalTok{(dwaic }\OperatorTok{>}\StringTok{ }\DecValTok{10}\NormalTok{, }\KeywordTok{as.character}\NormalTok{(tid),}\StringTok{''}\NormalTok{)), }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{, }\DataTypeTok{vjust =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"WAIC difference: the tails have it"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ylab}\NormalTok{(}\StringTok{"tax rate"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ggtitle}\NormalTok{( plt_title )}
\NormalTok{plt}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-105-1.pdf}

We might investigate tax id number 12. We also notice that the rest of the observations fall close to zero difference in their WAIC's. Perhaps the two models are not that different after all?

\hypertarget{the-prediction-causality-tradeoff}{%
\section{The prediction-causality tradeoff}\label{the-prediction-causality-tradeoff}}

What about statistical causality and inference? And how do we get to causality? Another day and time will help point the way. In the meantime, it is important to realize that causally incorrect models often, in practice, produce seemingly accurate predictions. For accurate causal inference we really need a higher viewpoint, that of the heuristic structure of the problem we are trying to solve. What does that all mean? Statistical knowledge is simply inferior to, subsumed by, immanent in a higher integration of statistical knowledge with the knowledge which comes from development (think history and genetics) and dialectic (think alternative world views and horizons).

Models can be constructed and taken to data, but they still might not provide better predictive power, that is, lower deviance or DKL or WAIC or log likelihood, or PSIS. We are now into analyst horizons, DAGs, constructive models of behavior that prescind from the data, but will ultimately be applied to the data for at least predictive validation.

{[}Insert PSIS - volatility graph and ggridge impact graphs a la Rjournal submission{]}-

\hypertarget{case-language-and-food}{%
\chapter{Case: Language and Food}\label{case-language-and-food}}

Daniel Nettle is a behavioral scientist who specializes in linguistic aspects of human development. His 1998 paper on language diversity, \href{https://www.danielnettle.org.uk/download/009.pdf}{which we can access here} poses this question.

\begin{quote}
This paper, then, asks the question of, what, in general, determines the size of language communities found in a human population.
\end{quote}

Language is a mask for regional integration or disintegration of societies. Powerful forces can unite, or divide, whole groups of people, often identifiable by the language of their region of origin. Food security is one of several fundamental sources of movements of people from one region to another. Ecological risk, as measured by the length of the mean growing season in a region, correlates with food security. Prominently for the time, Nettle published his paper with the original data he used to conduct the analysis.

The values in \texttt{data(nettle)} are data on language diversity in 74 nations. The meaning of each column is given below.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  country: Name of the country
\item
  num.lang: Number of recognized languages spoken
\item
  area: Area in square kilometers
\item
  k.pop: Population, in thousands
\item
  num.stations: Number of weather stations that provided data for the next two columns
\item
  mean.growing.season: Average length of growing season, in months
\item
  sd.growing.season: Standard deviation of length of growing season, in months
\end{enumerate}

Use these data to evaluate the hypothesis that language diversity is partly a product of food security. The notion is that, in productive ecologies, people don't need large social networks to buffer them against a risk of food shortfalls. This means cultural groups can be smaller and more self-sufficient,leading to more languages per capita. In this way we use the number of languages per capita as the outcome.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d}\OperatorTok{$}\NormalTok{lang.per.cap <-}\StringTok{ }\NormalTok{d}\OperatorTok{$}\NormalTok{num.lang }\OperatorTok{/}\StringTok{ }\NormalTok{d}\OperatorTok{$}\NormalTok{k.pop}
\end{Highlighting}
\end{Shaded}

Use the logarithm of this new variable as your regression outcome. We can continue this model using counts and the enhanced Poisson distribution. For now we use the similarly convergent Gaussian distribution.

We will evaluate the effects of both \texttt{mean.growing.season} and \texttt{sd.growing.season}, as well as their two-way interaction.

Here are three parts to help.

\begin{itemize}
\item
  First, we examine the hypothesis that language diversity, as measured by \texttt{log(lang.per.cap)}, is positively associated with the average length of the growing season, \texttt{mean.growing.season}. Consider \texttt{log(area)} in the models(s) as a covariate (not an interaction).
\item
  Next we consider the hypothesis that language diversity is negatively associated with the standard deviation of length of growing season, \texttt{sd.growing.season}. This hypothesis follows from uncertainty in harvest favoring social insurance through larger social networks and therefore fewer languages. Again, consider \texttt{log(area)} as a covariate (not an interaction). Interpret your results.
\item
  Finally, we assess the hypothesis that \texttt{mean.growing.season} and
  \texttt{sd.growing.season} interact to synergistically reduce language diversity. The notion is that, in nations with longer average growing seasons, high variance makes storage and redistribution even more important than it would be otherwise. That way, people can cooperate to preserve and protect harvest windfalls to be used during the droughts.
\end{itemize}

\hypertarget{a-first-model-set}{%
\subsection{A first model set}\label{a-first-model-set}}

We fit three models to begin to analyze the hypothesis

\begin{quote}
Language diversity, as measured by \texttt{log(lang.per.cap)}, is positively associated with the average length of the growing season, \texttt{mean.growing.season}.
\end{quote}

The models:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We attempt to predict \texttt{log\_lang\_per\_capita} (\(L\)) naively with a constant and no other predictors.
\item
  We build on the naive model with only mean growing season (\(M\)) as a predictor.
\item
  We can expand on this model by including \texttt{log(area)} (\(A\)) to mediate growing season or to independently influence language diversity.
\end{enumerate}

The mediation model has this formulation.

\[
\begin{align}
L & \sim \operatorname{Normal}(\mu_L, \sigma_L) \\
\mu_L & = \alpha_L + \beta_{LM} M + \beta_{LA} A\\
\alpha_L & \sim \operatorname{Normal}(median(L),2) \\
\beta_{LM} & \sim \operatorname{Normal}(0,2) \\
\beta_{LA} & \sim \operatorname{Normal}(0,2) \\
\sigma_L & \sim \operatorname{Exponential}(1) \\
M & \sim \operatorname{Normal}(\mu_M, \sigma_M) \\
\mu_M & = \alpha_M + \beta_{M} A\\
\alpha_M & \sim \operatorname{Normal}(median(M),2) \\
\beta_M & \sim \operatorname{Normal}(0,2) \\
\sigma_M & \sim \operatorname{Exponential}(1) \\
\end{align}
\]

In this model we literally run a regression (\(\mu_M = \alpha_M + \beta_{M} A\)) to help with an other regression (\(\mu_L = \alpha_L + \beta_{LM} M + \beta_{LA} A\)) where \(\mu_M, \sigma_M\) mediate movements in the main attraction \(\mu_L, \sigma_L\).

All of this will deserve a comparison. We will use WAIC. A lower WAIC in one model indicates a lower loss of information measured as deviancy relative to other models. WAIC, similar to the likelihood ratio, the F-test, and even \(R^2\), is a predictive statistical inference of a sort. Adding more variates increases the complexity of the models and likely will decrease information loss.

A second model set would involve the standard deviation of the growing season. A third model set would divide the world into meaningful regions to examine the differences and similarities of language diversity globally.

We begin with the data.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rethinking)}
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{data}\NormalTok{(nettle)}
\NormalTok{d <-}\StringTok{ }\NormalTok{nettle}
\KeywordTok{summary}\NormalTok{( d )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        country      num.lang        area             k.pop       
##  Algeria   : 1   Min.   :  1   Min.   :  12189   Min.   :   102  
##  Angola    : 1   1st Qu.: 17   1st Qu.: 167708   1st Qu.:  3829  
##  Australia : 1   Median : 40   Median : 434796   Median :  9487  
##  Bangladesh: 1   Mean   : 90   Mean   : 880698   Mean   : 33574  
##  Benin     : 1   3rd Qu.: 94   3rd Qu.:1080316   3rd Qu.: 24744  
##  Bolivia   : 1   Max.   :862   Max.   :8511965   Max.   :849638  
##  (Other)   :68                                                   
##   num.stations   mean.growing.season sd.growing.season
##  Min.   :  1.0   Min.   : 0.00       Min.   :0.00     
##  1st Qu.: 10.0   1st Qu.: 5.35       1st Qu.:0.94     
##  Median : 20.5   Median : 7.36       Median :1.69     
##  Mean   : 37.9   Mean   : 7.04       Mean   :1.70     
##  3rd Qu.: 44.8   3rd Qu.: 9.28       3rd Qu.:2.11     
##  Max.   :272.0   Max.   :12.00       Max.   :5.87     
## 
\end{verbatim}

We note:

\begin{itemize}
\item
  There are 68 countries represented.
\item
  The sample exhibits widely varying language diversity, area, and population.
\item
  We note the extreme distances between the 75th quantiles and the maximum. Scaling will be important in the regularization of the model.
\end{itemize}

\hypertarget{models}{%
\subsection{Models}\label{models}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d}\OperatorTok{$}\NormalTok{L <-}\StringTok{ }\KeywordTok{log}\NormalTok{( d}\OperatorTok{$}\NormalTok{num.lang }\OperatorTok{/}\StringTok{ }\NormalTok{d}\OperatorTok{$}\NormalTok{k.pop )}
\NormalTok{d}\OperatorTok{$}\NormalTok{A <-}\StringTok{ }\KeywordTok{log}\NormalTok{(d}\OperatorTok{$}\NormalTok{area) }
\NormalTok{d}\OperatorTok{$}\NormalTok{M <-}\StringTok{ }\NormalTok{d}\OperatorTok{$}\NormalTok{mean.growing.season}
\CommentTok{#}
\CommentTok{# m-models for the mean.growing.season}
\NormalTok{m0 <-}\StringTok{ }\KeywordTok{quap}\NormalTok{(}
  \KeywordTok{alist}\NormalTok{(}
\NormalTok{    L }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(mu,sigma),}
\NormalTok{    mu <-}\StringTok{ }\NormalTok{aL }\OperatorTok{+}\StringTok{ }\DecValTok{0}\NormalTok{,}
\NormalTok{    aL }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(}\DecValTok{5}\NormalTok{,}\FloatTok{1.5}\NormalTok{),}
\NormalTok{    sigma }\OperatorTok{~}\StringTok{ }\KeywordTok{dexp}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{  ) , }\DataTypeTok{data=}\NormalTok{d )}
\CommentTok{# Only M}
\NormalTok{m1 <-}\StringTok{ }\KeywordTok{quap}\NormalTok{(}
  \KeywordTok{alist}\NormalTok{(}
\NormalTok{    L }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(mu,sigma),}
\NormalTok{    mu <-}\StringTok{ }\NormalTok{aL }\OperatorTok{+}\StringTok{ }\NormalTok{bLM}\OperatorTok{*}\NormalTok{M,}
\NormalTok{    aL }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(}\OperatorTok{-}\DecValTok{5}\NormalTok{,}\FloatTok{1.5}\NormalTok{),}
\NormalTok{    bLM }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.5}\NormalTok{),}
\NormalTok{    sigma }\OperatorTok{~}\StringTok{ }\KeywordTok{dexp}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{  ) , }\DataTypeTok{data=}\NormalTok{d )}
\CommentTok{#}
\CommentTok{# Independent M and A}
\CommentTok{# M->L<-A}
\NormalTok{m2 <-}\StringTok{ }\KeywordTok{quap}\NormalTok{(}
  \KeywordTok{alist}\NormalTok{(}
\NormalTok{    L }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(mu,sigma),}
\NormalTok{    mu <-}\StringTok{ }\NormalTok{aL }\OperatorTok{+}\StringTok{ }\NormalTok{bLM}\OperatorTok{*}\NormalTok{M }\OperatorTok{+}\StringTok{ }\NormalTok{bLA}\OperatorTok{*}\NormalTok{A,}
\NormalTok{    aL }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(}\OperatorTok{-}\DecValTok{5}\NormalTok{,}\FloatTok{1.5}\NormalTok{),}
    \KeywordTok{c}\NormalTok{(bLM,bLA) }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.5}\NormalTok{),}
\NormalTok{    sigma }\OperatorTok{~}\StringTok{ }\KeywordTok{dexp}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{  ) , }\DataTypeTok{data=}\NormalTok{d )}
\CommentTok{#}
\CommentTok{# A mediating M}
\CommentTok{# A->G->L}
\CommentTok{# }
\NormalTok{m3 <-}\StringTok{ }\KeywordTok{quap}\NormalTok{(}
  \KeywordTok{alist}\NormalTok{(}
\NormalTok{    L }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(mu,sigma),}
\NormalTok{    mu <-}\StringTok{ }\NormalTok{aL }\OperatorTok{+}\StringTok{ }\NormalTok{bLM}\OperatorTok{*}\NormalTok{M }\OperatorTok{+}\StringTok{ }\NormalTok{bLA}\OperatorTok{*}\NormalTok{A,}
\NormalTok{    aL }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(}\OperatorTok{-}\DecValTok{5}\NormalTok{,}\FloatTok{1.5}\NormalTok{),}
    \KeywordTok{c}\NormalTok{(bLM,bLA) }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.5}\NormalTok{),}
\NormalTok{    sigma }\OperatorTok{~}\StringTok{ }\KeywordTok{dexp}\NormalTok{(}\DecValTok{1}\NormalTok{),}
\NormalTok{    M }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(muM, sigmaM),}
\NormalTok{    muM <-}\StringTok{ }\NormalTok{aM }\OperatorTok{+}\StringTok{ }\NormalTok{bM}\OperatorTok{*}\NormalTok{A,}
\NormalTok{    aM }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(}\FloatTok{7.4}\NormalTok{,}\DecValTok{2}\NormalTok{),}
\NormalTok{    bM }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.5}\NormalTok{),}
\NormalTok{    sigmaM }\OperatorTok{~}\StringTok{ }\KeywordTok{dexp}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{  ) , }\DataTypeTok{data=}\NormalTok{d,  )}
\CommentTok{#}
\KeywordTok{precis}\NormalTok{( m0, }\DataTypeTok{digits =} \DecValTok{4}\NormalTok{, }\DataTypeTok{depth =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        mean    sd  5.5% 94.5%
## aL    -5.32 0.175 -5.60 -5.04
## sigma  1.50 0.123  1.31  1.70
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{precis}\NormalTok{( m1, }\DataTypeTok{digits =} \DecValTok{4}\NormalTok{, }\DataTypeTok{depth =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         mean     sd    5.5% 94.5%
## aL    -6.558 0.3859 -7.1750 -5.94
## bLM    0.159 0.0503  0.0786  0.24
## sigma  1.397 0.1134  1.2159  1.58
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{precis}\NormalTok{( m2, }\DataTypeTok{digits =} \DecValTok{4}\NormalTok{, }\DataTypeTok{depth =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         mean     sd    5.5%   94.5%
## aL    -4.620 1.1731 -6.4950 -2.7452
## bLM    0.153 0.0498  0.0738  0.2328
## bLA   -0.148 0.0849 -0.2841 -0.0128
## sigma  1.378 0.1118  1.1993  1.5567
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{precis}\NormalTok{( m3, }\DataTypeTok{digits =} \DecValTok{4}\NormalTok{, }\DataTypeTok{depth =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          mean     sd    5.5%   94.5%
## aL     -4.620 1.1731 -6.4947 -2.7450
## bLM     0.153 0.0498  0.0738  0.2328
## bLA    -0.148 0.0849 -0.2841 -0.0129
## sigma   1.378 0.1118  1.1993  1.5566
## aM      9.943 1.7148  7.2024 12.6836
## bM     -0.230 0.1336 -0.4437 -0.0166
## sigmaM  2.969 0.2414  2.5833  3.3550
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{compare}\NormalTok{( m0, m1, m2, m3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    WAIC   SE dWAIC   dSE pWAIC  weight
## m3  267 15.7 0.000    NA  4.31 0.42935
## m2  268 15.9 0.454 0.252  4.55 0.34215
## m1  268 15.7 1.289 2.954  3.87 0.22542
## m0  277 17.0 9.873 7.211  2.92 0.00308
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{( }\KeywordTok{coeftab}\NormalTok{( m0, m1, m2, m3 ) )}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-108-1.pdf}

More covariates usually results in less deviancy and more predictivity. But very much more?

\begin{itemize}
\item
  The naive model, \texttt{m0}, is least predictive.
\item
  The two complicated models, \texttt{m2} and \texttt{m3}, are plausibly tied for second place.
\item
  The mid-complex model, \texttt{m1}, is the least deviant from an information loss point of view.
\end{itemize}

Here we visualize the predictions with WAIC's cousin PSIS. Both use the Bayesian predictive likelihood. PSIS here will also provide cross-validation with the leave-one-out procedure. The horizontal axis measures the degree of unpredictability using the Pareto power \(k\) parameter. The vertical axis measures the volatility of observations using the penalty component of the PSIS calculation.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{4284}\NormalTok{)}
\NormalTok{m <-}\StringTok{ }\NormalTok{m0}
\NormalTok{model_name <-}\StringTok{ }\KeywordTok{format}\NormalTok{( m}\OperatorTok{@}\NormalTok{formula[[}\DecValTok{2}\NormalTok{]])}
\NormalTok{PSIS_m <-}\StringTok{ }\KeywordTok{PSIS}\NormalTok{( m, }\DataTypeTok{pointwise=}\OtherTok{TRUE}\NormalTok{ )}
\NormalTok{PSIS_m <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{( PSIS_m, }
                 \DataTypeTok{country =}\NormalTok{ d}\OperatorTok{$}\NormalTok{country}
\NormalTok{                 )}
\NormalTok{p0_psis <-}\StringTok{ }\NormalTok{PSIS_m }\OperatorTok{|}\ErrorTok{>}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{( }\KeywordTok{aes}\NormalTok{( }\DataTypeTok{x=}\NormalTok{k, }\DataTypeTok{y=}\NormalTok{penalty ) ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{( }\DataTypeTok{shape=}\DecValTok{21}\NormalTok{, }\DataTypeTok{color=}\StringTok{"blue"}\NormalTok{ ) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_vline}\NormalTok{( }\DataTypeTok{xintercept =} \FloatTok{0.2}\NormalTok{, }\DataTypeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_text}\NormalTok{(}\KeywordTok{aes}\NormalTok{( }\DataTypeTok{label =} \KeywordTok{ifelse}\NormalTok{((k }\OperatorTok{>}\StringTok{ }\FloatTok{0.2}\NormalTok{)}\OperatorTok{&}\NormalTok{(penalty }\OperatorTok{>}\StringTok{ }\FloatTok{0.2}\NormalTok{), }\KeywordTok{as.character}\NormalTok{(country),}\StringTok{''}\NormalTok{)), }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{, }\DataTypeTok{vjust =} \DecValTok{1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{( }\DataTypeTok{title =} \StringTok{"Bias"}\NormalTok{,}
        \DataTypeTok{subtitle =}\NormalTok{ model_name,}
        \DataTypeTok{x =}\NormalTok{ (}\StringTok{"Uncertainty: PSIS Pareto k"}\NormalTok{),}
        \DataTypeTok{y =}\NormalTok{ (}\StringTok{"Volatility: PSIS penalty"}\NormalTok{))}
\CommentTok{#p1 #ggplotly( p1 )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#}
\NormalTok{m <-}\StringTok{ }\NormalTok{m0}
\NormalTok{model_name <-}\StringTok{ }\KeywordTok{format}\NormalTok{( m}\OperatorTok{@}\NormalTok{formula[[}\DecValTok{2}\NormalTok{]] )}
\NormalTok{post <-}\StringTok{ }\KeywordTok{extract.samples}\NormalTok{( m) }\CommentTok{# pull 3 columns of sampled parms}
\NormalTok{M_seq <-}\StringTok{ }\KeywordTok{seq}\NormalTok{( }\DataTypeTok{from=}\DecValTok{0}\NormalTok{, }\DataTypeTok{to=}\DecValTok{12}\NormalTok{, }\DataTypeTok{length.out=}\KeywordTok{length}\NormalTok{(d}\OperatorTok{$}\NormalTok{M))                                     }\CommentTok{# evenly spaced A and M}
\NormalTok{mu_link <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(M)\{}
  \CommentTok{# revolve around median area}
\NormalTok{  mu <-}\StringTok{ }\NormalTok{post}\OperatorTok{$}\NormalTok{aL }\CommentTok{#+ post$bLM*M + post$bLA*13}
\NormalTok{\}}
\NormalTok{mu <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{( M_seq, mu_link )          }\CommentTok{# calculated line}
\NormalTok{mu_mean <-}\StringTok{ }\KeywordTok{apply}\NormalTok{( mu, }\DecValTok{2}\NormalTok{, mean )         }\CommentTok{# make averages}
\NormalTok{mu_PI <-}\StringTok{ }\KeywordTok{apply}\NormalTok{( mu, }\DecValTok{2}\NormalTok{, PI, }\DataTypeTok{prob=}\FloatTok{0.91}\NormalTok{)   }\CommentTok{# make probability intervals}

\CommentTok{#}
\CommentTok{#summary( mu_CI )}
\NormalTok{post_tbl <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{M_seq =}\NormalTok{ M_seq,}
  \DataTypeTok{lower =} \KeywordTok{t}\NormalTok{(mu_PI)[,}\DecValTok{1}\NormalTok{],}
  \DataTypeTok{upper =} \KeywordTok{t}\NormalTok{(mu_PI)[,}\DecValTok{2}\NormalTok{],}
  \DataTypeTok{mean =}\NormalTok{ mu_mean,}
  \DataTypeTok{L =}\NormalTok{ d}\OperatorTok{$}\NormalTok{L,}
  \DataTypeTok{M =}\NormalTok{ d}\OperatorTok{$}\NormalTok{M,}
  \DataTypeTok{A =} \DecValTok{13}
\NormalTok{)}
\CommentTok{# plot}
\NormalTok{p0_pred <-}\StringTok{ }\NormalTok{post_tbl }\OperatorTok{|}\ErrorTok{>}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{( }\KeywordTok{aes}\NormalTok{( }\DataTypeTok{x=}\NormalTok{M_seq, }\DataTypeTok{y=}\NormalTok{L ) )}\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{( }\KeywordTok{aes}\NormalTok{( }\DataTypeTok{x=}\NormalTok{M_seq, }\DataTypeTok{y=}\NormalTok{mean ), }
             \DataTypeTok{color=}\StringTok{"red"}\NormalTok{ ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_ribbon}\NormalTok{( }\KeywordTok{aes}\NormalTok{( }\DataTypeTok{ymin=}\NormalTok{lower, }\DataTypeTok{ymax=}\NormalTok{upper ), }
               \DataTypeTok{alpha=}\FloatTok{0.1}\NormalTok{, }\DataTypeTok{fill =} \StringTok{"blue"}\NormalTok{,  }
               \DataTypeTok{color =} \StringTok{"red"}\NormalTok{, }
               \DataTypeTok{linetype =} \StringTok{"dotted"}\NormalTok{ ) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{( }\DataTypeTok{x =} \StringTok{"Mean growing season"}\NormalTok{,}
        \DataTypeTok{y =} \StringTok{"Number of languages"}\NormalTok{,}
        \DataTypeTok{title =} \StringTok{"91% Predictive Intervals"}\NormalTok{,}
        \DataTypeTok{subtitle =}\NormalTok{ model_name}
\NormalTok{        )}
\CommentTok{# p0_pred}
\end{Highlighting}
\end{Shaded}

\emph{Commentary}

(Note structural and statistical differences from the other models.)

Model: m1

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Run Model m1 PSIS here}
\CommentTok{# }
\CommentTok{# }
\CommentTok{# p1_psis}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Run Model m1  Predictive Probability Intervals here}
\CommentTok{# }
\CommentTok{# }
\CommentTok{# p1_psis}
\end{Highlighting}
\end{Shaded}

\emph{Commentary}

(Note structural and statistical differences from the other models.)

\hypertarget{model-m2}{%
\section{Model: m2}\label{model-m2}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Run Model m1 PSIS here}
\CommentTok{# }
\CommentTok{# }
\CommentTok{# p2_psis}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Run Model m1  Predictive Probability Intervals here}
\CommentTok{# }
\CommentTok{# }
\CommentTok{# p2_pred}
\end{Highlighting}
\end{Shaded}

\emph{Commentary}

(Note structural and statistical differences from the other models.)

\hypertarget{model-m3}{%
\section{Model: m3}\label{model-m3}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Run Model m3 PSIS here}
\CommentTok{# }
\CommentTok{# }
\CommentTok{# p3_psis}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Run Model m3  Predictive Probability Intervals here}
\CommentTok{# }
\CommentTok{# }
\CommentTok{# p3_pred}
\end{Highlighting}
\end{Shaded}

\emph{Commentary}

(Note structural and statistical differences from the other models.)

\hypertarget{overall-assessment}{%
\section{Overall assessment}\label{overall-assessment}}

\href{https://ggplot2-book.org/arranging-plots.html}{Arrange plots in a grid using the \textbf{patchwork} package for ease of comparison.}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(patchwork)}
\NormalTok{p0_psis }\OperatorTok{|}\StringTok{ }\NormalTok{p0_pred}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-117-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# for example with both models 0 and 1}
\CommentTok{# (p0_psis / p1_psis) | (p0_pred / p1_pred) }
\end{Highlighting}
\end{Shaded}

\hypertarget{appendix-a.-using-r}{%
\chapter*{Appendix A. Using R}\label{appendix-a.-using-r}}
\addcontentsline{toc}{chapter}{Appendix A. Using R}

\begin{itemize}
\item
  R and RStudio set up
\item
  R markdown and latex for text rendering
\item
  Base R data and calculations
\item
  Tidyverse wrangling and visualization
\end{itemize}

\hypertarget{warming-up-to-r-and-more}{%
\chapter*{Warming up to R and more}\label{warming-up-to-r-and-more}}
\addcontentsline{toc}{chapter}{Warming up to R and more}

\hypertarget{overview}{%
\section{Overview}\label{overview}}

In this chapter we will

\begin{itemize}
\item
  Discuss R for analytics
\item
  Install R, RStudio, and Latex
\item
  Install R markdown and run a document
\item
  Have some fun with STAN
\end{itemize}

\hypertarget{what-is-r}{%
\section{\texorpdfstring{What is \texttt{R}?}{What is R?}}\label{what-is-r}}

\texttt{R} is software for interacting with data along a variety of user generated paths. With \texttt{R} you can create sophisticated (even interactive) graphs, you can carry out statistical and operational research analyses, and you can create and run simulations. \texttt{R} is also a programming language with an extensive set of built-in functions. With increasing experience, you can extend the language and write your own code to build your own financial analytical tools. Advanced users can even incorporate functions written in other languages, such as C, C++, and Fortran.

The current version of \texttt{R} derives from the \texttt{S} language. \texttt{S} has been around for more than twenty years and has been with extrensive use in statistics and finance, first as \texttt{S} and then as the commercially available \texttt{S-PLUS}. \texttt{R} is an open source implementation of the \texttt{S} language that is now a viable alternative to \texttt{S-PLUS}. A core team of statisticians and many other contributors work to update and improve \texttt{R} and to make versions that run well under all of the most popular operating systems. Importantly, \texttt{R} is a free, high-quality statistical software that will be useful as you learn financial analytics even though it is also a first-rate tool for professional statisticians, operational researchers, and financial analysts and engineers. \footnote{But see this post on a truly big data language APL: \url{https://scottlocklin.wordpress.com/2013/07/28/ruins-of-forgotten-empires-apl-languages/}}

\hypertarget{r-for-analytics}{%
\section{\texorpdfstring{\texttt{R} for analytics}{R for analytics}}\label{r-for-analytics}}

There are several reasons that make \texttt{R} an excellent choice of software for an analytics course. Some benefits of using \texttt{R} include:

\begin{itemize}
\item
  \texttt{R} is free and available online. \texttt{R} is open-source and runs on \texttt{UNIX}, \texttt{Windows}, and \texttt{Macintosh} operating systems.
\item
  \texttt{R} has a well-documented, context-based, help system enhanced by a wide, and deep, ranging user community globally and across several disciplines.
\item
  \texttt{R} has excellent native static graphing capabilities. Interactive dynamic graphics are evolving along with the ability to embed analytics into online applications. With \texttt{R} you can build dashboards and websites to communicate results dynamically with consumers of the analytics you generate.
\item
  \texttt{R}'s language has a powerful, easy-to-learn syntax with many built-in statistical and operational research functions. Just as important are the extensive web-scraping, text structuring, object class construction, and the extensible functional programming aspects of the language. A formal language definition is being developed. This will yield more standardization and better control of the language in future versions.
\item
  \texttt{R} is a computer programming language. For programmers it will feel more familiar than for others, for example Excel users. \texttt{R} requires array thinking and object relationships that are not necessarily native, but indeed are possible, in an Excel spreadsheet environment. In many ways, the Excel style and \texttt{R} style of environments complement one another.
\item
  Even though it is not necessarily the simplest software to use, the basics are easy enough to master, so that learning to use \texttt{R} need not interfere with learning the statistical, operational research, data, and domain-specific concepts encountered in an analytics-focused course. \^{}{[}The attribute \emph{simple} is relative. For example, this Excel formula with nested if statements is far from \emph{simple} to understand, document, and implement: `=if(x3 = 1, 2, if(y2=0, ``42''\&cell(3, 2), if(left(a\(4\), find(b\(4\), ``=42''))))).
\item
  Doing statistics in a spreadsheet (e.g., Microsoft Excel) is generally a bad idea. While hundreds of millions of Excel users might disagree with this proposition learn the lesson of the 415 Report spreadsheet model to manage risks at JP Morgan Chase. Although many people are likely feel more familiar with them, spreadsheets are very limited in terms of what analyses they allow you do. If you get into the habit of trying to do your real life data analysis using spreadsheets, then you've dug yourself into a very deep hole.
\item
  Proprietary software is expensive, not very extensible, and has many routines that are often opaque to users. When you have a chance look up the cost of a Matlab or SAS single user license. The tools you will need will be provided a la carte at a price for licensing fees.
\item
  \texttt{R} is highly extensible. When you download and install R, you get all the basic ``packages'', and those are very powerful on their own. However, because R is so open and so widely used, it's become something of a standard tool in statistics, and so lots of people write their own packages that extend the system -- all freely available.
\item
  R is a real programming language. As you get better at using R for data analysis, you're also learning to program. When you program you must think through the question you are posing, the data you are collecting, the analytical techniques you will deploy, the visualization of your results. This workflow is commonly called the \emph{\textbf{software development lifecycle}}. If you don't already know how to program, then learning how to do statistics using \texttt{R} is a good way to begin.
\end{itemize}

There is at least one drawback.

\begin{itemize}
\tightlist
\item
  The primary hurdle to using \texttt{R} is that most existing documentation and plethora of packages are written for an audience that is knowledgable about statistics and operational research and has experience with other statistical computing programs. In contrast, this course intends to make \texttt{R} accessible to you, especially those who are new to both statistical concepts and statistical computing.
\end{itemize}

\hypertarget{hot-and-cold-running-resources}{%
\section{Hot and cold running resources}\label{hot-and-cold-running-resources}}

Much is available in books, e-books, and online for free. This is an extensive online community that links expert and novice modelers globally.

The standard start-up is at CRAN \url{http://cran.r-project.org/manuals.html}. A script in the appendix can be dropped into a workspace and played with easily. You can easily skip the rest of this expose on isntalling R, the integrated development environment RStudio, and some generous quips about computing, by simply going to the excellent on-line (and free) resource by James D. Long and Paul Teetor. 2019. \emph{R Cookbook 2nd Edition}. O'Reilley: Sebastopol, CA. A version of this resource is \href{https://rc2e.com/}{\underline{accessible here}} with extensive R and RStudio installation instructions.

Other resources include:

\begin{itemize}
\tightlist
\item
  Julian Faraway's \url{https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf} complete course on regression where you can imbibe deeply of the many ways to use \texttt{R} in statistics.
\item
  Along econometrics lines is Grant Farnsworth's \url{https://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf}.
\item
  Winston Chang's \url{http://www.cookbook-r.com/} and Hadley Wickham's example at \url{http://ggplot2.org/} are terrific online graphics resources.
\item
  Psychologist Danielle Navarro's Libretext \url{https://learningstatisticswithr.com} is a great companion to any statistical reasoning we might perform.
\end{itemize}

\hypertarget{gettingR}{%
\section{Installing R}\label{gettingR}}

R needs to be installed on your computer. R is freely distributed online, and you can download it from the R homepage, which is:

\begin{quote}
\url{http://cran.r-project.org/}
\end{quote}

At the top of the page -- under the heading ``Download and Install R'' -- you'll see separate links for Windows users, Mac users, and Linux users. If you follow the relevant link, you'll see that the online instructions are pretty self-explanatory.

R updates every six months or so, but don't worry, the now current version will suffice.

\hypertarget{installing-r-on-a-windows-computer}{%
\subsection{Installing R on a Windows computer}\label{installing-r-on-a-windows-computer}}

The CRAN homepage changes from time to time, and it's very old-school web design, but you can usually find what you are looking for. In general you'll find a link at the top of the page with the text ``Download R for Windows''. If you click on that, it will take you to a page that offers you a few options. Again, at the very top of the page you'll be told to click on a link that says to click here if you're installing R for the first time. That's probably what you want. This will take you to a page that has a prominent link at the top called ``Download R 4.0.2 for Windows''. That's the one you want. Click on that and your browser should start downloading a file called \texttt{R-4.x.x-win.exe}, or whatever the equivalent version number is by the time you read this.

The file for version 4.2.1 is about 84MB in size, so it may take some time depending on how fast your connection is. Once you've downloaded the file, double click to install it. As with any software you download online, Windows will ask you some questions about whether you trust the file and if it can alter your system. Say yes.

After you click through those, it'll ask you where you want to install it, and what components you want to install. Your PC should be a 64 bit machine to run the routines for the rest of this book. The default values should be fine for most people, so again, just click through. Once all that is done, you should have R installed on your system. You can access it from the Start menu, or from the desktop if you asked it to add a shortcut there.

While it may be some fun to open up R through the desktop shortcut, I suggest is that instead of doing that you should now install RStudio (see Section \ref{installingrstudio} for instructions).

One other thing (always one more thing!): to run the tools in this book, you must install \texttt{RTools42} for Windows based systems. This will allow you to use the \texttt{Rcpp} C++ compiler that is under the hood of the \texttt{rstan}, \texttt{rethinking}, \texttt{brms}, and \texttt{tidybayes} packages. For more details visit \url{https://rstan.org}.

\hypertarget{installing-r-on-a-mac}{%
\subsection{Installing R on a Mac}\label{installing-r-on-a-mac}}

When you click on the Mac OS X link, you should find yourself on a page with the title ``R for Mac OS X''. The vast majority of Mac users will have a fairly recent version of the operating system: as long as you run macOS 10.13 (High Sierra), then you'll be fine. \footnote{If you're running an older version of the Mac OS, then you need to follow the link to the ``old'' page (\url{http://cran.r-project.org/bin/macosx/old/}). You should be able to find the installer file that you need at the bottom of the page.}

There's a fairly prominent link on the page called ``R-4.0.2.pkg'', which is the one you want. Click on that link and you'll start downloading the installer file, which is \texttt{R-4.0.2.pkg}. It's about 84MB in size.

Once you've downloaded \texttt{R-4.0.2.pkg}, all you need to do is open it by double clicking on the package file. The installation should go smoothly from there: just follow all the instructions just like you usually do when you install something. Once it's finished, you'll find a file called \texttt{R.app} in the Applications folder. You can now open up R in the usual way \footnote{Tip for advanced Mac users. You can run R from the terminal if you want to. The command is just ``R''. It behaves like the normal desktop version, except that help documentation behaves like a ``man'' page instead of opening in a new window.} if you want to, but what I suggest is that instead of doing that you should now install RStudio (see Section \ref{installingrstudio} for instructions).

\hypertarget{installing-r-on-a-linux-computer}{%
\subsection{Installing R on a Linux computer}\label{installing-r-on-a-linux-computer}}

If you run a Linux box, regardless of what distribution, then you should find the instructions on the website easy enough. You can compile R from source yourself if you want, or install it through your package management system, which will probably have R in it.

Alternatively, the CRAN site has precompiled binaries for Debian, Red Hat, Suse and Ubuntu and has separate instructions for each. Once you've got R installed, you can run it from the command line just by typing \texttt{R}. However, if you're feeling envious of Windows and Mac users for their fancy GUIs, you can download RStudio too (see Section \ref{installingrstudio} for instructions).

\hypertarget{installingrstudio}{%
\subsection{Downloading and installing RStudio}\label{installingrstudio}}

When you install R initially, it comes with one application that lets you do run R natively in an R GUI: it's the R.exe application on a Windows machine, and the R.app application on a Mac. There are many integrated development environments (IDE) that will look for R and display the R console, while also allowing you to manage files, projects, look at your coding history, and review a variety of other objects. \texttt{Jupyter} is one such system that runs Julia, Python, and R interoperatively. The one I use exclusively, mainly for its one-stop-shop philosophy of data management, analysis cycles, and production of results in reports, books (like this one), and presentations, is \texttt{RStudio}. You can download the free personal version of \texttt{RStudio} here:

\begin{quote}
\url{http://www.RStudio.org/}
\end{quote}

When you visit the RStudio website, you'll see a new school user interface, much easier to navigate and simpler than the CRAN website, \footnote{This is probably no coincidence: the people who design and distribute the core R language itself are focused on technical aspects and issues. And sometimes they almost seem to forget that there's an actual human user at the end. The people who design and distribute RStudio are focused on the user. Their goal is to make R as available, usable, auditable as possible. Both are indispensable to the computing community.}

Just click the \texttt{Download} button and follow the directions to the desktop version for your system.

After it's finished installing, you start R by opening RStudio. You don't need to open R.app or R.exe in order to access R. RStudio will take care of that for you. In this screenshot you can see four panels. One of them is the R console.

\begin{figure}
\centering
\includegraphics{img/rstudio-shot.jpg}
\caption{Rstudio Screen Shot}
\end{figure}

The midnight blue background helps my eyesight and perhaps yours as well. Your very first Rstudio job is to set up a new Project. This will create a \texttt{*.Rproj} file in a working directory. It is in that working directory that you will perform all of your computing. I typically use one for each week of instruction, for each paper or presentation I am writing, for this book too. Your programming life will thank you for this habit.

RStudio has extensive guidance on everything from \href{https://support.rstudio.com/hc/en-us/articles/200486468-Authoring-R-Presentations}{authoring slides} to tutorials and numerous cheatsheets that will help us navigate packages. The tutorials are accessed through a tab in the environment pane of Rstudio.

The only shortcoming with RStudio is that it's a perpetual work in progress: they keep improving it! Having said that I have found that updating RStudio is easy, and does not interfere with my workflows, file directories, R installation, packages, running C++ libraries, and so forth.

\hypertarget{first-day-at-school}{%
\section{First day at school}\label{first-day-at-school}}

Always with some fear and trepidation we enter a new phase of learning. Let's dive right into the deep end of this pool.

\hypertarget{install-r-markdown}{%
\subsection{\texorpdfstring{Install \texttt{R\ Markdown}}{Install R Markdown}}\label{install-r-markdown}}

Click on \texttt{RStudio} in your tray or start up menu. Be sure you are connected to the Internet. A console panel will appear. At the console prompt \texttt{\textgreater{}} type

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"rmarkdown"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  This action will install the \texttt{rmarkdown} package.
\item
  This package will enable you to construct documentation for your work as well as actually run the code you build.
\end{itemize}

If you have several packages to attach to your workspace you can use this function to check if the packages are installed, attach installed packages, and install packages not yet installed. You can copy and paste the \texttt{is\_installed()} function (yes, your first function) and the \texttt{pkg} vector into the console. Then we call the function with the \texttt{pkg} argument.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{is_installed <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) \{}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in}\NormalTok{ x) \{}
        \CommentTok{# require will return TRUE}
        \CommentTok{# (1) 'invisibly' if it was}
        \CommentTok{# able to load package}
        \ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{require}\NormalTok{(i, }\DataTypeTok{character.only =} \OtherTok{TRUE}\NormalTok{)) \{}
            \CommentTok{# If, for any reason,}
            \CommentTok{# the package was not}
            \CommentTok{# able to be loaded}
            \CommentTok{# then re-install and}
            \CommentTok{# expect a RStudio}
            \CommentTok{# message}
            \KeywordTok{install.packages}\NormalTok{(i, }\DataTypeTok{dependencies =} \OtherTok{TRUE}\NormalTok{)}
            \CommentTok{# Attach the package}
            \CommentTok{# after installing}
            \KeywordTok{require}\NormalTok{(i, }\DataTypeTok{character.only =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{        \}}
\NormalTok{    \}}
\NormalTok{\}}
\CommentTok{# for example here is a vector of}
\CommentTok{# concatenated character strings}
\CommentTok{# assigned to the obj `pkg`}
\NormalTok{pkg <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"rmarkdown"}\NormalTok{, }\StringTok{"shiny"}\NormalTok{, }\StringTok{"psych"}\NormalTok{,}
    \StringTok{"knitr"}\NormalTok{, }\StringTok{"tidyverse"}\NormalTok{, }\StringTok{"ggthemes"}\NormalTok{,}
    \StringTok{"plotly"}\NormalTok{, }\StringTok{"moments"}\NormalTok{, }\StringTok{"flexdashboard"}\NormalTok{,}
    \StringTok{"GGally"}\NormalTok{)}
\CommentTok{# use `pkg` as the 'argument' of}
\CommentTok{# the function `is_installed()`}
\KeywordTok{is_installed}\NormalTok{(pkg)}
\end{Highlighting}
\end{Shaded}

Voila and we have packages in our library. We will add more packages as we go along. But these will suffice for now.

\begin{itemize}
\tightlist
\item
  This extremely helpful web page, \url{http://rmarkdown.rstudio.com/gallery.html}, is a portal to several examples of \texttt{R\ Markdown} source files that can be loaded into \texttt{RStudio}, modified, and used with other content for your own work.
\end{itemize}

\hypertarget{install-latex}{%
\section{Install LaTex}\label{install-latex}}

Even though matheatics is kept to a low roar in this book, we can profitably use a comprehensive text rendering system for our documents. \texttt{R\ Markdown} uses a text rendering system called \texttt{LaTeX} to render text, including mathematical and graphical content. We can install the\texttt{tinytex}, or \texttt{MikTeX} document rendering system for \texttt{Windows} or \texttt{MacTeX} document rendering system for \texttt{Mac\ OS\ X}. This book is rendered with \texttt{tinytex}.

\begin{itemize}
\tightlist
\item
  For \texttt{tinytex}just follow Yihui's instructions:
\end{itemize}

\begin{verbatim}
Installing and maintaining TinyTeX is easy for R users, since the R package tinytex has provided wrapper functions (N.B. the lowercase and bold tinytex means the R package, and the camel-case TinyTeX means the LaTeX distribution). You can use tinytex to install TinyTeX:

install.packages('tinytex')
tinytex::install_tinytex()
# to uninstall TinyTeX, run tinytex::uninstall_tinytex() 
\end{verbatim}

Yes, that simple.

\begin{itemize}
\item
  For \texttt{Windows}, navigate to the \url{https://miktex.org/download} page and go to the \textbf{64- or 32- bit} installer. Click on the appropriate \texttt{Download} button and follow the directions. \textbf{Be very sure you select the \emph{COMPLETE} installation}. Frequently Asked Questions (FAQ) can be found at \url{https://docs.miktex.org/faq/}. If you have \texttt{RStudio} already running, you will have to restart your session.
\item
  For \texttt{MAC\ OS\ X}, navigate to the \url{http://www.tug.org/mactex/} page and download the \texttt{MacTeX} system and follow the directions. This distribution requires Mac OS 10.5 Leopard or higher and runs on Intel or PowerPC processors. \textbf{Be very sure you select the \emph{FULL} installation}. Frequently Asked Questions (FAQ) can be found at \url{https://docs.miktex.org/faq/}. If you have \texttt{RStudio} already running, you will have to restart your session. FAQ can be found at \url{http://www.tug.org/mactex/faq/index.html}.
\end{itemize}

\hypertarget{our-first-file}{%
\subsection{Our first file}\label{our-first-file}}

Open \texttt{RStudio} and see something like this screenshot\ldots{}

\begin{figure}
\centering
\includegraphics{img/rstudio-shot.jpg}
\caption{Rstudio screenshot}
\end{figure}

\begin{itemize}
\item
  You can modify the position and content of the four panes by selecting \texttt{View\ \textgreater{}\ Panes\ \textgreater{}\ Pane\ Options}.
\item
  If you haven't already, definitely \texttt{install.packages("rmarkdown")}. Then in the console again enter \texttt{library(rmarkdown)}. Under \texttt{File\ \textgreater{}\ New\ File\ \textgreater{}\ Rmarkdown} a dialog box invites you to open document, presentation, Shiny, and other files. Upon choosing \texttt{documents} you may open up a new file. Under \texttt{File\ \textgreater{}\ Save\ As} save the untitle file in an appropriate directory. The \texttt{R\ Markdown} file extension \texttt{Rmd} will appear in the file name in your directory.
\item
  When creating a new \texttt{Rmarkdown} file, \texttt{RStudio} deposits a template that shows you how to use the markdown approach. You can generate a document by clicking on \texttt{knit} in the icon ribbon attached to the file name tab in the script pane. If you do not see \texttt{knit}, then you might need to install and load the \texttt{knitr} package with the following statements in the \texttt{R} console. You might need also to restart your RStudio session.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"knitr"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(knitr)  }\CommentTok{# comments always appreciated!}
\end{Highlighting}
\end{Shaded}

The \texttt{Rmd} file contains three types of content:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  An (optional) \textbf{YAML header} surrounded by \texttt{-\/-\/-} on the top and the bottom of \texttt{YAML} statements. \texttt{YAML} is ``Yet Another Markdown (or up) Language''. Here is an example from this document:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{---}
\FunctionTok{title:}\AttributeTok{ }\StringTok{"Setting Up R for Analytics"}
\FunctionTok{author:}\AttributeTok{ }\StringTok{"Bill Foote"}
\FunctionTok{date:}\AttributeTok{ }\StringTok{"November 11, 2016"}
\FunctionTok{output:}\AttributeTok{ pdf_document}
\OtherTok{---}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Chunks} of R code surrounded by ``` (find this key usually as the lowercase of the \texttt{\textasciitilde{}} symbol).
\item
  Text mixed with text formatting like \texttt{\#\ heading} and \texttt{\_italics\_} and mathematical formulae like \texttt{\$z\ =\ \textbackslash{}frac\{(\textbackslash{}bar\ x-\textbackslash{}mu\_0)\}\{s/\textbackslash{}sqrt\{n\}\}\$} which will render
  \begin{equation}
  z = \frac{(\bar x-\mu_0)}{s/\sqrt{n}}.
  \end{equation}
\end{enumerate}

When you open an \texttt{.Rmd} file, \texttt{RStudio} provides an interface where code, code output, and text documentation are interleaved. You can run each code chunk by clicking the \texttt{Run} icon (it looks like a play button at the top of the chunk), or by pressing \texttt{Cmd/Ctrl\ +\ Shift\ +\ Enter}. \texttt{RStudio} executes the code and displays the results in the console with the code.

You can write mathematical formulae in an \texttt{R\ Markdown} document as well. For example, here is a formula for net present value.

\begin{Shaded}
\begin{Highlighting}[]
\SpecialStringTok{$$}
\SpecialStringTok{NPV = }\SpecialCharTok{\textbackslash{}sum}\SpecialStringTok{_\{t=0\}^\{T\} }\SpecialCharTok{\textbackslash{}frac}\SpecialStringTok{\{NCF_t\}\{(1+WACC)^t\}}
\SpecialStringTok{$$}
\end{Highlighting}
\end{Shaded}

This script will render

\begin{equation}
NPV = \sum_{t=0}^{T} \frac{NCF_t}{(1+WACC)^t}
\end{equation}

\begin{itemize}
\tightlist
\item
  Here are examples of common in file text formatting in \texttt{R\ Markdown}.
\end{itemize}

\begin{verbatim}
Text formatting 
------------------------------------------------------------

*italic*  or _italic_
**bold**   __bold__
_**bold and italic**_
`code`
superscript^2 and subscript_2

Headings
------------------------------------------------------------

# 1st Level Header

## 2nd Level Header

### 3rd Level Header

Lists
------------------------------------------------------------

*   Bulleted list item 1

*   Item 2

    * Item 2a

    * Item 2b

1.  Numbered list item 1

1.  Item 2. The numbers are incremented automatically in the output.

Links and images
------------------------------------------------------------

<http://example.com>

[linked phrase](http://example.com)

![optional caption text](path/to/img.jpg)

Tables 
------------------------------------------------------------

First Header  | Second Header
------------- | -------------
Content Cell  | Content Cell
Content Cell  | Content Cell

Math
------------------------------------------------------------

$\frac{\mu}{\sigma^2}$

\[\frac{\mu}{\sigma^2}]

$$
\frac{\mu}{\sigma^2}
$$
\end{verbatim}

More information can be found \href{https://cran.r-project.org/web/packages/rmarkdown/rmarkdown.pdf}{here with CRAN documentation} and \href{https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf}{here at RStudio} as well.

\hypertarget{some-fun-with-stan}{%
\section{Some fun with Stan}\label{some-fun-with-stan}}

We will use the \texttt{rethinking} package for our work in this book. They depend on the \texttt{rstan} package which is an interface to the C++ \texttt{STAN} library for probabilistic computation. So our first step is to navigate to the \href{https://mc-stan.org}{STAN site} and to the \href{https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started}{rstan github site} to install the \texttt{rstan} interface to \texttt{Stan}.

\begin{quote}
Follow the directions \emph{exactly} and \emph{in order}.
\end{quote}

Failure to do so will cause much heart-ache, ulcers and other physical, mental, and emotional disorders!

Because Stan models and any package that depends on Stan are compiled in C++ it is critically important to check the C++ tool chain in your system. If you have a windows OS then you must install \texttt{Rtools42} as mentioned above in the R installation notes. The \texttt{Rcpp} package runs the interface to the C++ compiler.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Follow the \href{https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started}{getting started} directions in the order presented and closely.
\item
  Remove the \texttt{-march=native} flag in the \texttt{Documents/.R/Makevars.win} file if on a Windows OS.
\item
  If you get a \texttt{Error:\ \textquotesingle{}makevars\_user\textquotesingle{}\ is\ not\ an\ exported\ object\ from\ \textquotesingle{}namespace:withr\textquotesingle{}} or similar error then remove the \texttt{withr} and \texttt{rstan} packages and re-install \texttt{withr} first, then \texttt{rstan}.
\item
  In Rstudio you can check the syntax of the \texttt{YOURMODELFILE.stan} using \texttt{rstan:::rstudio\_stanc("MODELFILE.stan")} to ensure that all the code is correct. Also you must put a blank line after the last line of code.
\item
  When running \texttt{stan()} we might see an error like \texttt{\textquotesingle{}-E\textquotesingle{}\ not\ found} -- ignore it as it seems to come from the use of \texttt{g++} compiler.
\item
  Speaking of the C++ compiler, on windows especially, install \texttt{Rtools40}. Be sure to look up where the \texttt{g++.exe} file is and modify the \texttt{.Renviron} wherever your \texttt{R} system is located, usually in the \texttt{Documents} file.
\item
  STAN runs through rstan on Rcpp. We need to be sure that Rcpp works properly. Test this idea with the simple program
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(Rcpp)}
\KeywordTok{evalCpp}\NormalTok{(}\StringTok{"1+5"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6
\end{verbatim}

If this simple program does not work, check PATH and BINPREF and your \texttt{Rtools40} implementation.

When following the instructions on \href{https://github.com/rmcelreath/rethinking}{the \texttt{rethinking} site} don't forget the double quotes below. The \texttt{::} operator allows us to use a function from a package, in this case \texttt{install\_github} from the \texttt{devtools} package.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"devtools"}\NormalTok{, }\StringTok{"coda"}\NormalTok{,}
    \StringTok{"mvtnorm"}\NormalTok{, }\StringTok{"devtools"}\NormalTok{, }\StringTok{"loo"}\NormalTok{, }\StringTok{"dagitty"}\NormalTok{))}
\NormalTok{devtools}\OperatorTok{::}\KeywordTok{install_github}\NormalTok{(}\StringTok{"rmcelreath/rethinking"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can copy this code into Rstudio to test whether any of this is working.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rethinking)}
\CommentTok{# model}
\NormalTok{f <-}\StringTok{ }\KeywordTok{alist}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\KeywordTok{dnorm}\NormalTok{(mu, sigma), mu }\OperatorTok{~}
\StringTok{    }\KeywordTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{), sigma }\OperatorTok{~}\StringTok{ }\KeywordTok{dexp}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\CommentTok{# quadratic approximation}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{quap}\NormalTok{(f, }\DataTypeTok{data =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{y =} \KeywordTok{c}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{,}
    \DecValTok{1}\NormalTok{)), }\DataTypeTok{start =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{mu =} \DecValTok{0}\NormalTok{, }\DataTypeTok{sigma =} \DecValTok{1}\NormalTok{))}

\KeywordTok{summary}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        mean    sd   5.5% 94.5%
## mu    0.000 0.592 -0.947 0.947
## sigma 0.839 0.329  0.314 1.365
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# precis( fit ) yields the same}
\CommentTok{# output as summary()}
\end{Highlighting}
\end{Shaded}

McElreath loves his 89\% probability intervals! 89 is afterall a prime number.

\hypertarget{github}{%
\section{Github}\label{github}}

\href{www.gihub.com}{Get a github account to store your work.}. This is but a first step in the transparency needed to support solid data analytics solutions as effectively software development. You can also track your work, collaborate, manage versions and issues. This book is served from github pages linked to a repository.

\hypertarget{jargon}{%
\section{jaRgon}\label{jargon}}

(Mostly directly copied from \href{http://www.burns-stat.com/documents/tutorials/impatient-r/jargon/}{Patrick Burns}, and annotated a bit, for educational use only.)

\begin{quote}
atomic vector
\end{quote}

An object that contains only one form of data. The atomic modes are: \emph{logical}, \emph{numeric}, \emph{complex} and \emph{character}.

\begin{quote}
attach
\end{quote}

The act of adding an item to the search list. You usually attach a package with the \texttt{require} function, you attach saved files and objects with the \texttt{attach} function.

\begin{quote}
data frame
\end{quote}

A rectangular data object where each column may be a different type of data. Conceptually a generalization of a matrix, but implemented entirely differently. This is a \texttt{tibble()} in the \texttt{tidyverse}.

\begin{quote}
factor
\end{quote}

A data object that represents categorical data. It is possible (and often unfortunate) to confuse a factor with a character vector.

\begin{quote}
global environment
\end{quote}

The first location on the search list, and the place where objects that you create reside. See \textbf{search list}.

\begin{quote}
list
\end{quote}

A type of object with possibly multiple components where each component may be an arbitrary object, including a list. Each object can can different dimensions and data types.

\begin{quote}
matrix
\end{quote}

A rectangular data object where all cells have the same data type. Conceptually a specialization of a data frame, but implemented entirely differently. This object has rows and columns.

\begin{quote}
package
\end{quote}

A collection of \texttt{R} objects in a special format that includes help files, functions, examples, data, and source code. Most packages primarily or exclusively contain functions, but some packages exclusively contain datasets. \textbf{Packages} are attached to workspaces using the \texttt{library()} function. We use the \texttt{require()} function only to test if we have a package since this function returns a logical value (\texttt{TRUE} or \texttt{FALSE}).

\begin{quote}
search list
\end{quote}

The collection of locations that \texttt{R} searches for objects when it is evaluating a command.

Start to add your own entries as you expand your research capabilities.

\hypertarget{tickling-the-ivories}{%
\chapter*{Tickling the Ivories}\label{tickling-the-ivories}}
\addcontentsline{toc}{chapter}{Tickling the Ivories}

In this chapter we will use the R console in RStudio to run through the basic syntax of R. But that's not all: we will also use R's innate ability to implement linear algebra to build the basic statistics of ordinary least squares (OLS) regression.

\hypertarget{start-to-tickle}{%
\section{Start to tickle}\label{start-to-tickle}}

Or if you paint and draw, the 2-minute pose will warm you up. In the \texttt{RStudio} console panel (in the NE pane of my IDE) play with these by typing these statements at the \texttt{\textgreater{}} symbol:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2 3 4 5 6
\end{verbatim}

This will produce a vector from 2 to 6.

We can use \texttt{alt-} (hold alt and hyphen keys down simultaneously) to produce \texttt{\textless{}-}, and assign data to an new object. This is a from \texttt{R}'s predecessor James Chamber's \texttt{S} (ATT Bell Labs) that was ported from the single keystroke \(\leftarrow\) in Ken Iverson's \texttt{APL} (IBM), where it is reserved as a binary logical operator. We can now also use \texttt{=} to assign variables in \texttt{R}. But, also a holdover from \texttt{APL}, we will continue to use \texttt{=} only for assignments within functions. Glad we got that over!

Now let's try these expressions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)}
\KeywordTok{sum}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 20
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{prod}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 720
\end{verbatim}

These actions assign the results of a calculation to a variable \texttt{x} and then sum and multiply the elements. \texttt{x} is stored in the active workspace. You can verify that by typing \texttt{ls()} in the console to list the objects in the workspace. Type in these statements as well.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ls}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   [1] "a"                     "alpha"                 "ann.line"             
##   [4] "ann.text"              "ans"                   "b"                    
##   [7] "caption"               "check"                 "clt_sim"              
##  [10] "conjectures"           "CPS1988"               "d"                    
##  [13] "data"                  "data_D"                "data_grid"            
##  [16] "data_M"                "data_moments"          "data_null"            
##  [19] "date_time"             "df.sample"             "dpareto"              
##  [22] "dt_limit"              "dwaic"                 "education"            
##  [25] "ethnicity"             "eucdf"                 "exD"                  
##  [28] "f"                     "fit"                   "fit_wage_educ"        
##  [31] "fte"                   "grid_D"                "grid_function"        
##  [34] "grid_M"                "growth"                "guess_d"              
##  [37] "hands"                 "header"                "hours"                
##  [40] "hypotheses"            "i"                     "inits"                
##  [43] "IRR"                   "k.est"                 "Laffer"               
##  [46] "limitRange"            "line.size"             "LL1"                  
##  [49] "LL2"                   "log_prob"              "loss"                 
##  [52] "loss_quad"             "lppd"                  "m"                    
##  [55] "m_min"                 "m_OLS"                 "M_seq"                
##  [58] "m_t"                   "m0"                    "m1"                   
##  [61] "m2"                    "m3"                    "main.title"           
##  [64] "make_loss"             "make_quad_loss"        "margin"               
##  [67] "max.a"                 "mean_sim"              "mean_width"           
##  [70] "min.a"                 "min_label"             "min_loss"             
##  [73] "model"                 "model_name"            "mu"                   
##  [76] "mu_b"                  "mu_link"               "mu_mean"              
##  [79] "mu_PI"                 "n"                     "N"                    
##  [82] "n.sample"              "n_bins"                "n_obs"                
##  [85] "n_samples"             "n_success"             "n_trials"             
##  [88] "nettle"                "nodes"                 "null"                 
##  [91] "null_table"            "opt_plot"              "p"                    
##  [94] "p_grid"                "p_MAP"                 "p0_pred"              
##  [97] "p0_psis"               "p1"                    "par_mean"             
## [100] "par_varcov"            "pareto.fit"            "pareto.loglike"       
## [103] "pareto.tail.ks.test"   "phd1"                  "phd2"                 
## [106] "phdD1"                 "phdD2"                 "plot.eucdf.loglog"    
## [109] "plot.survival.loglog"  "plt"                   "plt_title"            
## [112] "post"                  "post_calc"             "post_samples"         
## [115] "post_table"            "post_tbl"              "posterior_hands"      
## [118] "posterior_hands_raw"   "posterior_shovels"     "posterior_shovels_raw"
## [121] "ppareto"               "pr_gt_null"            "price"                
## [124] "prior"                 "priors"                "probability"          
## [127] "prod1"                 "prod2"                 "PSIS_m"               
## [130] "pWAIC"                 "q"                     "qpareto"              
## [133] "random_datetime"       "row_1"                 "row_2"                
## [136] "rpareto"               "s_xbar"                "samples"              
## [139] "samples_mcmc"          "shovels"               "sig"                  
## [142] "sig_b"                 "sim_df"                "SSE"                  
## [145] "SSE_min"               "SSE_min_index"         "start"                
## [148] "sum_column"            "sum_row"               "sumM"                 
## [151] "table"                 "table_posteriors"      "table_priors"         
## [154] "test_time"             "tests"                 "title"                
## [157] "title_raw"             "title_samples"         "var_cov"              
## [160] "wage"                  "wage_afam_fit"         "wage_all"             
## [163] "wage_educ"             "wage_educ_afam"        "wage_grid"            
## [166] "wage_grid_samples"     "waic_effort"           "waic_vec"             
## [169] "waic1"                 "waic2"                 "x"                    
## [172] "X"                     "x.title"               "x_label"              
## [175] "xbar"                  "XY"                    "y"                    
## [178] "Y"                     "y.title"               "y_label"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(x)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2 3 4 5 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\DecValTok{6}\OperatorTok{:}\DecValTok{8}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] NA NA NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\DecValTok{6}\OperatorTok{:}\DecValTok{8}\NormalTok{] <-}\StringTok{ }\DecValTok{7}\OperatorTok{:}\DecValTok{9}
\NormalTok{x}\OperatorTok{/}\DecValTok{0}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] Inf Inf Inf Inf Inf Inf Inf Inf
\end{verbatim}

\texttt{x} has length of 5 and we use that to index all of the current elements of \texttt{x}. Trying to access elements 6 to 8 produces \texttt{na} because they do not exist yet. Appending 7 to 9 will fill the spaces. Dividing by 0 produces \texttt{inf}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(x1 <-}\StringTok{ }\NormalTok{x }\OperatorTok{-}\StringTok{ }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0 1 2 3 4 5 6 7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0 1 2 3 4 5 6 7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}\OperatorTok{/}\NormalTok{x1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  Inf 3.00 2.00 1.67 1.50 1.40 1.33 1.29
\end{verbatim}

Putting parentheses around an expression is the same as printing out the result of the expression. Element-wise division (multiplication, addition, subtraction) produces \texttt{inf} as the first element.

\hypertarget{try-this-exercise}{%
\section{Try this exercise}\label{try-this-exercise}}

Suppose we have a gargleblaster machine that produces free cash flows of \$10 million each year for 8 years. The machine will be scrapped and currently you believe you can get \$5 million at the end of year 8 as salvage value. The forward curve of interest rates for the next 1 to 8 years is 0.06, 0.07, 0.05, 0.09, 0.09, 0.08, 0.08, 0.08.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the value of \$1 received at the end of each of the next 8 years? Use this script to begin the modeling process. Describe each calculation.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rates <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.06}\NormalTok{, }\FloatTok{0.07}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.09}\NormalTok{, }\FloatTok{0.09}\NormalTok{,}
    \FloatTok{0.08}\NormalTok{, }\FloatTok{0.08}\NormalTok{, }\FloatTok{0.08}\NormalTok{)}
\NormalTok{t <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{8}\NormalTok{)}
\NormalTok{(pv}\FloatTok{.1}\NormalTok{ <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{rates)}\OperatorTok{^}\NormalTok{t))}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  What is the present value of salvage? Salvage would be at element 8 of an 8-element cash flow vector, and thus would use the eighth forward rate, \texttt{rate{[}8{]}}, and \texttt{t} would be 8 as well. Eliminate the sum in the above script. Make a variable called \texttt{salvage} and assign salvage value to this variable. Use this variable in place of the \texttt{1} in the above script for \texttt{pv.1}. Call the new present value \texttt{pv.salvage}.
\item
  What is the present value of the gargleblaster machine? Type in these statements. The \texttt{rep} function makes an \texttt{8} element cash flow vector. We change the value of the 8th element of the cash flow vector to include salvage. Now use the \texttt{pv.1} statement above and substitute \texttt{cashflow} for \texttt{1}. You will have your result.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cashflow <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{8}\NormalTok{)}
\NormalTok{cashflow[}\DecValTok{8}\NormalTok{] <-}\StringTok{ }\NormalTok{cashflow[}\DecValTok{8}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{salvage}
\end{Highlighting}
\end{Shaded}

Some results follow. The present value of \$1 is
The present value of a \$1 is this mathemetical formula.

\begin{equation}
PV = \sum_{t=1}^{8}\frac{1}{(1+r)^t}
\end{equation}

We can translate this mathematical expression into \texttt{R} this way: \footnote{Bond traders: please ignore the lack of rigorous detail surrounding an arbitrage-free term structure here! An 8\% rate 6 years out means earning a return of 8\% per annum on a 6 year strip.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rates <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.06}\NormalTok{, }\FloatTok{0.07}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.09}\NormalTok{, }\FloatTok{0.09}\NormalTok{,}
    \FloatTok{0.08}\NormalTok{, }\FloatTok{0.08}\NormalTok{, }\FloatTok{0.08}\NormalTok{)}
\NormalTok{t <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{8}\NormalTok{)}
\NormalTok{(}\DecValTok{1}\OperatorTok{/}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{rates)}\OperatorTok{^}\NormalTok{t)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.943 0.873 0.864 0.708 0.650 0.630 0.583 0.540
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(pv}\FloatTok{.1}\NormalTok{ <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{rates)}\OperatorTok{^}\NormalTok{t))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.79
\end{verbatim}

We define \texttt{rates} as a vector using the \texttt{c()} concatenation function. We then define a sequence of 8 time indices \texttt{t} starting with 1. The present value of a \$1 is sum of the vector element-by-element calculation of the date by date discounts \(1/(1+r)^t\).

The present value of salvage is the discounted salvage that is expected to occur at, and in this illustration only at, year 8.

\begin{equation}
PV_{salvage} = \frac{salvage}{(1+r)^8}
\end{equation}

Translated into \texttt{R} we have

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{salvage <-}\StringTok{ }\DecValTok{5}
\NormalTok{(pv.salvage <-}\StringTok{ }\NormalTok{salvage}\OperatorTok{/}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{rates[}\DecValTok{8}\NormalTok{])}\OperatorTok{^}\DecValTok{8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.7
\end{verbatim}

The present value of the gargleblaster machine is the present value of cashflows from operations from year 1 to year 8 plus the present value of salvage received in year 8. Salvage by definition is realized at the of the life of the operational cashflows upon disposition of the asset, here at year 8.

\begin{equation}
PV_{total} = \sum_{t=1}^{8}\frac{cashflow_t}{(1+r)^t} + \frac{salvage}{(1+r)^8}
\end{equation}

This expression translates into \texttt{R} this way:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cashflow <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{8}\NormalTok{)}
\NormalTok{cashflow[}\DecValTok{8}\NormalTok{] <-}\StringTok{ }\NormalTok{cashflow[}\DecValTok{8}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{salvage}
\NormalTok{(pv.machine <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(cashflow}\OperatorTok{/}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{rates)}\OperatorTok{^}\NormalTok{t))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 60.6
\end{verbatim}

The \texttt{rep} or ``repeat'' function creates cash flows of \$10 for each of 8 years. We adjust the year 8 cash flow to reflect salvage so that \(cashflow_8 = 10 + salvage\). The \texttt{{[}8{]}} indexes the eighth element of the \texttt{cashflow} vector.

\hypertarget{building-some-character}{%
\section{Building Some Character}\label{building-some-character}}

Let's type these expressions into the console at the \texttt{\textgreater{}} prompt:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\KeywordTok{length}\NormalTok{(x) }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{] <-}\StringTok{ "end"}
\NormalTok{x[}\KeywordTok{length}\NormalTok{(x) }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{] <-}\StringTok{ "end"}
\NormalTok{x.char <-}\StringTok{ }\NormalTok{x[}\OperatorTok{-}\KeywordTok{length}\NormalTok{(x)]}
\NormalTok{x <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(x.char[}\OperatorTok{-}\KeywordTok{length}\NormalTok{(x.char)])}
\KeywordTok{str}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  num [1:8] 2 3 4 5 6 7 8 9
\end{verbatim}

We have appended the string ``end'' to the end of x, twice.

\begin{itemize}
\tightlist
\item
  We use the \texttt{-} negative operator to eliminate it.
\item
  By inserting a string of characters into a numeric vector we have forced \texttt{R} to transform all numerical values to characters.
\item
  To keep things straight we called the character version \texttt{x.char}.
\item
  In the end we convert \texttt{x.char} back to numbers that we check with the \texttt{str}(ucture) function.
\end{itemize}

We will use this procedure to build data tables (we will call these ``data frames'') when comparing distributions of variables such as stock returns.

Here's a useful set of statements for coding and classifying variables. Type these statements into the console.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1016}\NormalTok{)}
\NormalTok{n.sim <-}\StringTok{ }\DecValTok{10}
\NormalTok{x <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n.sim)}
\NormalTok{y <-}\StringTok{ }\NormalTok{x}\OperatorTok{/}\NormalTok{(}\KeywordTok{rchisq}\NormalTok{(x}\OperatorTok{^}\DecValTok{2}\NormalTok{, }\DataTypeTok{df =} \DecValTok{3}\NormalTok{))}\OperatorTok{^}\FloatTok{0.5}
\end{Highlighting}
\end{Shaded}

We did a lot of \texttt{R} here. First, we set a random seed to reproduce the same results every time we run this simulaton. Then, we store the number of simulations in \texttt{n.sim} and produced two new variables with normal and a weirder looking distribution (a Student's t distribution?). Invoking \texttt{help} will display help with distributions in the console pane of the \texttt{RStudio} IDE.

Now let's try to display some of this interesting, and if surprising, information. The next code block will set up some presentation layer data for a plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z <-}\StringTok{ }\KeywordTok{c}\NormalTok{(x, y)}
\NormalTok{indicator <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"normal"}\NormalTok{, }\StringTok{"abnormal"}\NormalTok{),}
    \DataTypeTok{each =} \KeywordTok{length}\NormalTok{(x))}
\NormalTok{xy_df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{Variates =}\NormalTok{ z, }\DataTypeTok{Distributions =}\NormalTok{ indicator)}
\end{Highlighting}
\end{Shaded}

We concatenate the two variables into a new variable \texttt{z}. We built into the variable \texttt{indicator} the classifier to indicate which is \texttt{x} and which is \texttt{y}. But let's visualize what we want. (Paint in words here.) We want a column the first \texttt{n.sim} elements of which are \texttt{x} and the second are \texttt{y}. We then want a column the first \texttt{n.sim} elements of which are indicated by the character string ``normal'', and the second \texttt{n.sim} elements by ``abnormal''.

The \texttt{rep} function replicates the concatenation of ``normal'' and ``abnormal'' 10 times (the \texttt{length(x)}). The \texttt{each} feature concatenates 10 replications of ``normal'' to 10 replications of ``abnormal''. We concatenate the variates into \texttt{xy} with the \texttt{c()} function.

Data frames are just column and row tables. Enter \texttt{str(xy\_df)} to see what the structure of the \texttt{xy\_df} data frame contains. In later work we will use a streamlined version of the data frame called a \texttt{tibble} in the \texttt{tidyverse} ecosystem.

We can see the first 5 components of the data frame components using the \texttt{\$} subsetting notation as below.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(xy_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    20 obs. of  2 variables:
##  $ Variates     : num  0.777 1.373 1.303 0.148 -1.825 ...
##  $ Distributions: chr  "normal" "normal" "normal" "normal" ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(xy_df}\OperatorTok{$}\NormalTok{Variates, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  0.777  1.373  1.303  0.148 -1.825
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(xy_df}\OperatorTok{$}\NormalTok{Distributions, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "normal" "normal" "normal" "normal" "normal"
\end{verbatim}

The \texttt{str} call returns the two vectors inside of \texttt{xy}. One is numeric and the other is a ``factor'' with two levels. \texttt{R} and many of the routines in \texttt{R} will interpret these as zeros and ones in developing indicator and dummy variables for regressions and filtering. The \texttt{head()} (and there is a \texttt{tail()} too) function displays as many components as you wish to see starting with row 1.

\hypertarget{the-plot-thickens}{%
\section{The plot thickens}\label{the-plot-thickens}}

We will want to see our handiwork, so load the \texttt{ggplot2} library using \texttt{install.packages("ggplot2")}.\footnote{Visit Hadley Wickham's examples at \url{http://ggplot2.org/}.)}

This plotting package requires data frames. A ``data frame'' simply put is a list of vectors and arrays with names. An example of a data frame in Excel is just the worksheet. There are columns with names in the first row, followed by several rows of data in each column. If you were to install the \texttt{tidyverse} package and load the package with \texttt{library(tidyverse)} you could use \texttt{tibble()} instead of the bas R \texttt{data.frame()}.

Here we have defined a data frame \texttt{xy\_df}. All of the \texttt{x} and \texttt{y} variates are put into one part of the frame, and the distribution indicator into another. For all of this to work in a plot the two arrays must be of the same length. Thus we use the common \texttt{n.sim} and \texttt{length(x)} to insure this when we computed the series. We always examine the data, here using the \texttt{head} and \texttt{tail} functions.

Type \texttt{help(ggplot)} into the console for details. The \texttt{ggplot2} graphics package embodies Hadley Wickham's ``grammar of graphics'' we can review at \url{http://ggplot2.org}. Hadley Wickham has a very useful presentation with numerous examples at \url{http://ggplot2.org/resources/2007-past-present-future.pdf}.

As mentioned above, the package uses data frames to process graphics. A lot of packages other than \texttt{ggplot2}, including the base \texttt{stats} package, require data
frames.

We load the library first. The next statement sets up the blank but all too ready canvas (it will be empty!) on which a density plot can be rendered.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{ggplot}\NormalTok{(xy_df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Variates, }\DataTypeTok{fill =}\NormalTok{ Distributions))}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-136-1.pdf}

The data frame name \texttt{xy\_df} is first followed by the aesthetics mapping of data. The next statement inserts a geometrical element, here a density curve, which has a transparency parameter aesthetic \texttt{alpha}.

\includegraphics{_main_files/figure-latex/unnamed-chunk-137-1.pdf}

\hypertarget{try-this-example}{%
\subsection{Try this example}\label{try-this-example}}

Zoom in with \texttt{xlim} and lower x-axis and upper x-axis limits using the following statement:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(xy_df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Variates, }\DataTypeTok{fill =}\NormalTok{ Distributions)) }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_density}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.3}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{xlim}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{,}
    \DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-138-1.pdf}

Now we are getting to extreme value statistics by visualizing the tail of this distribution.

\hypertarget{arrays-and-you}{%
\section{Arrays and You}\label{arrays-and-you}}

Arrays have rows and columns and are akin to tables. All of Excel's worksheets are organized into cells that are tables with columns and rows. Data frames are more akin to tables in data bases. Here are some simple matrix arrays and functions. We start by making a mistake:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(A.error <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{11}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4]
## [1,]    1    4    7   10
## [2,]    2    5    8   11
## [3,]    3    6    9    1
\end{verbatim}

The \texttt{matrix()} function takes as input here the sequence of numbers from 1 to 11. It then tries to put these 11 elements into a 4 column array with 3 rows. It is missing a number as the error points out. To make a 4 column array out of 11 numbers it needs a twelth number to complete the third row. We then type in these statements

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(A_row <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{12}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4]
## [1,]    1    4    7   10
## [2,]    2    5    8   11
## [3,]    3    6    9   12
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(A_col <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{12}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{4}\NormalTok{, }\DataTypeTok{byrow =} \OtherTok{FALSE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4]
## [1,]    1    4    7   10
## [2,]    2    5    8   11
## [3,]    3    6    9   12
\end{verbatim}

In \texttt{A} we take 12 integers in a row and specify they be organized into 4 columns, and in \texttt{R} this is by row. In the next statement we see that \texttt{A\_col} and column binding \texttt{cbind()} are equivalent.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(R <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{, }\DecValTok{5}\OperatorTok{:}\DecValTok{8}\NormalTok{, }\DecValTok{9}\OperatorTok{:}\DecValTok{12}\NormalTok{))  }\CommentTok{# Concatenate rows}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4]
## [1,]    1    2    3    4
## [2,]    5    6    7    8
## [3,]    9   10   11   12
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(C <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\DecValTok{4}\OperatorTok{:}\DecValTok{6}\NormalTok{, }\DecValTok{7}\OperatorTok{:}\DecValTok{9}\NormalTok{, }\DecValTok{10}\OperatorTok{:}\DecValTok{12}\NormalTok{))  }\CommentTok{# concatenate columns}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4]
## [1,]    1    4    7   10
## [2,]    2    5    8   11
## [3,]    3    6    9   12
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A_col }\OperatorTok{==}\StringTok{ }\NormalTok{C}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4]
## [1,] TRUE TRUE TRUE TRUE
## [2,] TRUE TRUE TRUE TRUE
## [3,] TRUE TRUE TRUE TRUE
\end{verbatim}

Using the \texttt{outer} product allows us to operate on matrix elements, first picking the minimum, then the maximum of each row. The \texttt{pmin} and \texttt{pmax} compare rows element by element. If you used \texttt{min} and \texttt{max} you would get the minimum and maximum of the whole matrix.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(A_min <-}\StringTok{ }\KeywordTok{outer}\NormalTok{(}\DecValTok{3}\OperatorTok{:}\DecValTok{6}\OperatorTok{/}\DecValTok{4}\NormalTok{, }\DecValTok{3}\OperatorTok{:}\DecValTok{6}\OperatorTok{/}\DecValTok{4}\NormalTok{, }\DataTypeTok{FUN =}\NormalTok{ pmin))  }\CommentTok{#}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4]
## [1,] 0.75 0.75 0.75 0.75
## [2,] 0.75 1.00 1.00 1.00
## [3,] 0.75 1.00 1.25 1.25
## [4,] 0.75 1.00 1.25 1.50
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(A_max <-}\StringTok{ }\KeywordTok{outer}\NormalTok{(}\DecValTok{3}\OperatorTok{:}\DecValTok{6}\OperatorTok{/}\DecValTok{4}\NormalTok{, }\DecValTok{3}\OperatorTok{:}\DecValTok{6}\OperatorTok{/}\DecValTok{4}\NormalTok{, }\DataTypeTok{FUN =}\NormalTok{ pmax))  }\CommentTok{#}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4]
## [1,] 0.75 1.00 1.25  1.5
## [2,] 1.00 1.00 1.25  1.5
## [3,] 1.25 1.25 1.25  1.5
## [4,] 1.50 1.50 1.50  1.5
\end{verbatim}

We build a symmetrical matrix and replace the diagonal with 1. \texttt{A\_sym} looks like a correlation matrix. Here all we were doing is playing with shaping data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(A_sym <-}\StringTok{ }\NormalTok{A_max }\OperatorTok{-}\StringTok{ }\NormalTok{A_min }\OperatorTok{-}\StringTok{ }\FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       [,1]  [,2]  [,3]  [,4]
## [1,] -0.50 -0.25  0.00  0.25
## [2,] -0.25 -0.50 -0.25  0.00
## [3,]  0.00 -0.25 -0.50 -0.25
## [4,]  0.25  0.00 -0.25 -0.50
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{diag}\NormalTok{(A_sym) <-}\StringTok{ }\DecValTok{1}
\NormalTok{A_sym}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       [,1]  [,2]  [,3]  [,4]
## [1,]  1.00 -0.25  0.00  0.25
## [2,] -0.25  1.00 -0.25  0.00
## [3,]  0.00 -0.25  1.00 -0.25
## [4,]  0.25  0.00 -0.25  1.00
\end{verbatim}

\hypertarget{try-this-exercise-1}{%
\subsection{Try this exercise}\label{try-this-exercise-1}}

The \texttt{inner} product \texttt{\%*\%} cross-multiplies successive elements of a row with the successive elements of a column. If there are two rows with 5 columns, there must be a matrix at least with 1 column that has 5 rows in it.

Let's run these statements.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n_sim <-}\StringTok{ }\DecValTok{100}
\NormalTok{x_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{rgamma}\NormalTok{(n_sim, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.2}\NormalTok{)}
\NormalTok{x_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{rlnorm}\NormalTok{(n_sim, }\FloatTok{0.15}\NormalTok{, }\FloatTok{0.25}\NormalTok{)}
\KeywordTok{hist}\NormalTok{(x_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-144-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(x_}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-144-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(x_}\DecValTok{1}\NormalTok{, x_}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\texttt{rgamma} allows us to generate \texttt{n\_sim} versions of the gamma distribution with scale parameter \texttt{0.5} and shape parameter \texttt{0.2}. \texttt{rlnorm} is a popular financial return distribution with mean \texttt{0.15} and standard deviation \texttt{0.25}. We can call up \texttt{??distributions} to get detailed information. Let's plot the histograms of each simulated random variate using \texttt{hist()}.

The \texttt{cbind} function binds into matrix columns the row arrays \texttt{x\_1} and \texttt{x\_2}. These might be simulations of operational and financial losses. The \texttt{X} matrix could look like the \emph{\emph{design matrix}} for a regression.

Let's simulate a response vector, say equity, and call it \texttt{y} and look at its histogram.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\FloatTok{1.5} \OperatorTok{*}\StringTok{ }\NormalTok{x_}\DecValTok{1} \OperatorTok{+}\StringTok{ }\FloatTok{0.8} \OperatorTok{*}\StringTok{ }\NormalTok{x_}\DecValTok{2} \OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n_sim,}
    \FloatTok{4.2}\NormalTok{, }\FloatTok{5.03}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we have a frequentist statistical model for \(y\):

\[
y = X \beta + \varepsilon
\]

where \(y\) is a 100 \(\times\) 1 (rows \(\times\) columns) vector, \(X\) is a 100 \(\times\) 2 matrix, \(\beta\) is a 2 \(\times\) 1 vector, and \(\epsilon\) is a 100 \(\times\) 1 vector of disturbances (a.k.a., ``errors'').

Multiplying out the matrix term \(X \beta\) we have

\[
y = \beta_1 x_1 + \beta_2 x_2 + \varepsilon
\]

where \(y\), \(x_1\), \(x_2\), and \(\varepsilon\) are all vectors with 100 rows for simulated observations.

If we look for \(\beta\) to minimize the sum of squared \(\varepsilon\) we would find that the solution is

\[
\hat{\beta} = (X^T X)^{-1} X^{T} y.
\]

Where \(\hat{\beta}\) is read as ``beta hat''.

The result \(y\) with its \texttt{hist()} is

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-146-1.pdf}

The rubber meets the road here as we compute \(\hat{\beta}\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(x_}\DecValTok{1}\NormalTok{, x_}\DecValTok{2}\NormalTok{)}
\NormalTok{XTX_inverse <-}\StringTok{ }\KeywordTok{solve}\NormalTok{(}\KeywordTok{t}\NormalTok{(X) }\OperatorTok{%*%}\StringTok{ }\NormalTok{X)}
\NormalTok{(beta_hat <-}\StringTok{ }\NormalTok{XTX_inverse }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(X) }\OperatorTok{%*%}
\StringTok{    }\NormalTok{y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     [,1]
## x_1 1.61
## x_2 4.04
\end{verbatim}

The \texttt{beta\_hat} coefficients are much different than our model for \texttt{y}. Why? Because of the innovation, error, disturbance term \texttt{rnorm(n\_sim,\ 1,\ 2)} we added to the \texttt{1.5*x\_1\ \ +\ 0.8\ *\ x\_2} terms.

Now for the estimated \(\varepsilon\) where we use the matrix inner product \texttt{\%*\%}. We need to be sure to \emph{pre}-multiply \texttt{beta\_hat} with \texttt{X}!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{e <-}\StringTok{ }\NormalTok{y }\OperatorTok{-}\StringTok{ }\NormalTok{X }\OperatorTok{%*%}\StringTok{ }\NormalTok{beta_hat}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{hist}\NormalTok{(e)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-149-1.pdf}

We see that the ``residuals'' are almost centered at \texttt{0}.

\hypertarget{more-about-residuals}{%
\subsection{More about residuals}\label{more-about-residuals}}

For no charge at all let's calculate the sum of squared errors in matrix talk, along with the number of obervations \texttt{n} and degrees of freedom \texttt{n\ -\ k}, all to get the standard error of the regression \texttt{e\_se}. Mathematically we are computing

\[
\sigma_{\varepsilon} = \sqrt{\sum_{i=1}^N \frac{\varepsilon_i^2}{n-k}}
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(e_sse <-}\StringTok{ }\KeywordTok{t}\NormalTok{(e) }\OperatorTok{%*%}\StringTok{ }\NormalTok{e)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]
## [1,] 3021
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(n <-}\StringTok{ }\KeywordTok{dim}\NormalTok{(X)[}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 100
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(k <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(beta_hat))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(e_se <-}\StringTok{ }\NormalTok{(e_sse}\OperatorTok{/}\NormalTok{(n }\OperatorTok{-}\StringTok{ }\NormalTok{k))}\OperatorTok{^}\FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]
## [1,] 5.55
\end{verbatim}

The statement \texttt{dim(X){[}1{]}} returns the first of two dimensions of the matrix \texttt{X}. The R system has a built in OLS model called \texttt{lm()} (for linear model). Here is that model and a \texttt{summary()} of results.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\NormalTok{data_xy <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{x_1 =}\NormalTok{ x_}\DecValTok{1}\NormalTok{, }\DataTypeTok{x_2 =}\NormalTok{ x_}\DecValTok{2}\NormalTok{,}
    \DataTypeTok{y =}\NormalTok{ y)}
\NormalTok{fit_}\DecValTok{0}\NormalTok{ <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x_}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{x_}\DecValTok{2} \OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ data_xy)}
\KeywordTok{summary}\NormalTok{(fit_}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = y ~ x_1 + x_2 - 1, data = data_xy)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -12.015  -3.200   0.366   4.333  14.347 
## 
## Coefficients:
##     Estimate Std. Error t value             Pr(>|t|)    
## x_1    1.607      0.161    9.97 < 0.0000000000000002 ***
## x_2    4.042      0.616    6.56         0.0000000026 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 5.55 on 98 degrees of freedom
## Multiple R-squared:  0.799,  Adjusted R-squared:  0.795 
## F-statistic:  195 on 2 and 98 DF,  p-value: <0.0000000000000002
\end{verbatim}

First we load the \texttt{tidyverse} package and make a tibble data set. Then we use the statistical model \texttt{y\ \textasciitilde{}\ x\_1\ +\ x\_2\ -\ 1} The \texttt{lm()} function parses this formula into the dependent variable \texttt{y}, the two independent variables \texttt{x\_1} and \texttt{x\_2}, and \texttt{-1} for a zero intercept all referring to the components of the data set in the \texttt{lm()} function. We then can view the statistics of the regression using the \texttt{summary()} method associated with \texttt{lm()}. All results are in the object \texttt{fit\_0}.

Of course we must try our model with an intercept, the default in \texttt{lm()}. I usually put the \texttt{1\ +} into the formula as a reminder.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{x_}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{x_}\DecValTok{2}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ data_xy)}
\KeywordTok{summary}\NormalTok{(fit_}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = y ~ 1 + x_1 + x_2, data = data_xy)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -12.768  -3.148  -0.674   3.700  13.362 
## 
## Coefficients:
##             Estimate Std. Error t value            Pr(>|t|)    
## (Intercept)    5.313      2.221    2.39               0.019 *  
## x_1            1.559      0.159    9.82 0.00000000000000033 ***
## x_2           -0.200      1.873   -0.11               0.915    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 5.42 on 97 degrees of freedom
## Multiple R-squared:   0.5,   Adjusted R-squared:  0.489 
## F-statistic: 48.5 on 2 and 97 DF,  p-value: 0.00000000000000257
\end{verbatim}

We see a very different story where the coefficient of the \texttt{x\_2} variable changes sign. In future sagas we will bury \(R^2\).

The \texttt{lm()} function is the workhorse of R regression in the frequentist statistical tradition of Joseph Venn and R.A. \citet{Fisher_1925}. Fisher was a geneticist who espoused this approach which really did apply quite well to many aspects of his field. These aspects include highly mechanistic biological processes.

In contrast, our field is that of human behavior in the business domain. Fisher's approach might not always work for us there. He assumes effectively that the prior belief is uniformly distributed. More than that, his approach is to calculate averages, aggregations, of series of data, deviations (squared), of data, and so one. Then given the aggregation, simulate the aggregator. Our probabilistic approach does not assume a prior belief. It goes on to simulate the range of possible, and logical, ways (hypotheses) that data can condition hypotheses about a parameter, and elicit a posterior response given a belief about the hypothesis. This approach solves for the parameter across a range of hypotheses conditional on beliefs and data. If our questions do not require this extent of analysis, then, so be it, we use Fisher's ellipsis of an approach.

We also notice the layout of the writing of the function and its arguments. This tremendously aids comprehension as well as shooting trouble when running the functions.

Finally, again for no charge at all, lets load library \texttt{psych} (use \texttt{install.packages("psych")} as needed). We will use \texttt{pairs.panels()} for a pretty picture of our work in this try out. First column bind \texttt{cbind()} the \texttt{y}, \texttt{X}, and \texttt{e} arrays to create a data frame for \texttt{pairs.panel()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(psych)}
\NormalTok{all <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(y, X, e)}
\end{Highlighting}
\end{Shaded}

We then invoke the \texttt{pairs.panels()} function using the \texttt{all} array we just created. The result is a scatterplot matrix with histograms of each variate down the diagonal. The lower triangle of the matrix is populated with scatterplots. The upper triangle of the matrix has correlations depicted with increasing font sizes for higher correlations.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{pairs.panels}\NormalTok{(all)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-153-1.pdf}

We will use this sort of tool again and again to explore the multivariate relationships among our data. Even better displays are available with the

\hypertarget{more-array-work}{%
\section{More Array Work}\label{more-array-work}}

We show off some more array operations in the following statements.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{nrow}\NormalTok{(A_min)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ncol}\NormalTok{(A_min)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(A_min)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4 4
\end{verbatim}

We calculate the number of rows and columns first. We then see that these exactly correspond to the two element vector produced by \texttt{dim}. Next we enter these statements into the console.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{rowSums}\NormalTok{(A_min)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.00 3.75 4.25 4.50
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{colSums}\NormalTok{(A_min)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.00 3.75 4.25 4.50
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(A_min, }\DecValTok{1}\NormalTok{, sum)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.00 3.75 4.25 4.50
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(A_min, }\DecValTok{2}\NormalTok{, sum)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.00 3.75 4.25 4.50
\end{verbatim}

We also calculate the sums of each row and each column. Alternatively we can use the \texttt{apply} function on the first dimension (rows) and then on the second dimension (columns) of the matrix. Some matrix multiplications follow below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(A_inner <-}\StringTok{ }\NormalTok{A_sym }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(A_min[, }\DecValTok{1}\OperatorTok{:}\KeywordTok{dim}\NormalTok{(A_min)[}\DecValTok{2}\NormalTok{]]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       [,1]  [,2]  [,3]  [,4]
## [1,] 0.750 0.750 0.812 0.875
## [2,] 0.375 0.562 0.500 0.500
## [3,] 0.375 0.500 0.688 0.625
## [4,] 0.750 0.938 1.125 1.375
\end{verbatim}

Starting from the inner circle of embedded parentheses we pull every row (the \texttt{{[},col{]}} piece) for columns from the first to the second dimension of the \texttt{dim()} of \texttt{A\_min}. We then transpose (row for column) the elements of \texttt{A\_min} and cross left multiply in an inner product this transposed matrix with \texttt{A\_sym}.
We have already deployed very useful matrix operation, the inverse. The \texttt{R} function \texttt{solve()} provides the answer to the question: what two matrices, when multiplied by one another, produces the identity matrix? The identity matrix is a matrix of all ones down the diagonal and zeros elsewhere.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(A_inner_invert <-}\StringTok{ }\KeywordTok{solve}\NormalTok{(A_inner))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                       [,1]  [,2]  [,3]  [,4]
## [1,]  4.952380952380952550 -3.05 -1.14 -1.52
## [2,] -2.285714285714285587  6.86 -2.29  0.00
## [3,]  0.000000000000000127 -2.29  6.86 -2.29
## [4,] -1.142857142857143016 -1.14 -3.43  3.43
\end{verbatim}

Now we use our inverse with the original matrix we inverted.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(A_result <-}\StringTok{ }\NormalTok{A_inner }\OperatorTok{%*%}\StringTok{ }\NormalTok{A_inner_invert)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                      [,1]                  [,2]                  [,3]
## [1,] 0.999999999999999778  0.000000000000000000  0.000000000000000000
## [2,] 0.000000000000000222  1.000000000000000222  0.000000000000000000
## [3,] 0.000000000000000111  0.000000000000000111  1.000000000000000000
## [4,] 0.000000000000000222 -0.000000000000000444 -0.000000000000000888
##                      [,4]
## [1,] 0.000000000000000000
## [2,] 0.000000000000000000
## [3,] 0.000000000000000444
## [4,] 1.000000000000000000
\end{verbatim}

When we cross multiply \texttt{A\_inner} with its inverse, we should, and do, get the identity matrix that is a matrix of ones in the diagonal and zeros in the off-diagonal elements.

\hypertarget{summary}{%
\section{Summary}\label{summary}}

We covered very general data manipulations in \texttt{R} including arithmetical operations, vectors and matrices, their formation and operations, and data frames. We used data frames as inputs to plotting functions. We also built a matrix-based linear regression model and a present value calculator. This will be nearly the last time in this book we will employ frequentist statistical calculation. The next chapter will bring us to the brink of the statistical reasoning often called Bayesian. It is also nearly the last time we will seriously use the nouns and verbs of base R data wrangling. We will start next chapter to use modern data management nouns and verbs with the \texttt{tidyverse} package and ecosystem.

\hypertarget{further-reading}{%
\section{Further Reading}\label{further-reading}}

This introductory chapter covers material from Teetor, chapters 1, 2, 5, 6. Present value, salvage, and other valuation topics can be found in Brealey et al.~under \texttt{present\ value} in the index of any of several editions.

\hypertarget{exercises-2}{%
\section{Exercises}\label{exercises-2}}

These practice sets will repeat various \texttt{R} features from this chapter. Specifically we will practice defining vectors, matrices (arrays), and data frames and their use in present value, growth, future value calculations, We will build on this basic practice with the computation of ordinary lease squares coefficients and plots using \texttt{ggplot2}. We will summarize our findings in debrief documented with an \texttt{R\ markdown} file and output.

\hypertarget{r-markdown-set-up}{%
\subsection{\texorpdfstring{\texttt{R\ Markdown} set up}{R Markdown set up}}\label{r-markdown-set-up}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Open a new \texttt{R\ Markdown} html document file and save it with file name \texttt{MYName-FIN654-PS01} to your working directory. The \texttt{Rmd} file extension will automatically be appended to the file name. Create a new folder called \texttt{data} in this working directory and deposit the \texttt{.csv} file for practice set \#2 to this directory.
\item
  Modify the \texttt{YAML} header in the \texttt{Rmd} file to reflect the name of this practice set, your name, and date.
\item
  Replace the \texttt{R\ Markdown} example in the new file with the following script.
\end{enumerate}

\begin{verbatim}
## Practice set 1: present value
(INSERT results here)
## Practice set 2: regression
(Insert results here)
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Click \texttt{knit} in the \texttt{Rstudio} command bar to produce the \texttt{html} document.
\end{enumerate}

\hypertarget{mutual-fund-simulation}{%
\subsection{Mutual Fund simulation}\label{mutual-fund-simulation}}

\hypertarget{problem}{%
\subsubsection{Problem}\label{problem}}

We work for a mutual fund that is legally required to fair value the stock of unlisted companies it owns. Your fund is about to purchase shares of InUrCorner, a U.S. based company, that provides internet-of-things legal services.

\begin{itemize}
\item
  We sampled several companies with business plans similar to InUrCorner and find that the average weighted average cost of capital is 18\%.
\item
  InUrCorner sales is \$80 million and projected to growth at 50\% per year for the next 3 years and 15\% per year thereafter.
\item
  Cost of services provided as a percent of sales is currently 75\% and projected to be flat for the foreseeable future.
\item
  Depreciation is also constant at 5\% of net fixed assets (gross fixed asset minus accumulated depreciation), as are taxes (all-in) at 35\% of taxable profits.
\item
  Discussions with InUrCorner management indicate that the company will need an increase in working capital at the rate of 15\% each year and an increase in fixed assets at the rate of 10\% of sales each year. Currently working capital is \$10, net fixed assets is \$90, and accumulated depreciation is \$15.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Let's project \texttt{sales}, \texttt{cost}, increments to net fixed assets \texttt{NFA}, increments to working capital \texttt{WC}, \texttt{depreciation}, \texttt{tax}, and free cash flow \texttt{FCF} for the next 4 years. We will use a table to report the projection.
\end{enumerate}

Let's use this code to build and display a table.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Form table of results}
\NormalTok{table.names <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Sales"}\NormalTok{, }\StringTok{"Cost"}\NormalTok{, }\StringTok{"Working Capital (incr.)"}\NormalTok{,}
    \StringTok{"Net Fixed Assets (incr.)"}\NormalTok{, }\StringTok{"Free                             Cash Flow"}\NormalTok{)}
\CommentTok{# Assign projection labels}
\NormalTok{table.year <-}\StringTok{ }\NormalTok{year  }\CommentTok{# Assign projection years}
\NormalTok{table.data <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(sales, cost, WC.incr,}
\NormalTok{    NFA.incr, FCF)  }\CommentTok{# Layer projections}
\KeywordTok{rownames}\NormalTok{(table.data) <-}\StringTok{ }\NormalTok{table.names  }\CommentTok{# Replace rows with projection labels}
\KeywordTok{colnames}\NormalTok{(table.data) <-}\StringTok{ }\NormalTok{table.year  }\CommentTok{# Replace columns with projection years}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(table.data)  }\CommentTok{# Display a readable table}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Modify the assumptions by +/- 10\% and report the results.
\end{enumerate}

\hypertarget{healthcare-provider-admission-rates}{%
\subsection{Healthcare provider admission rates}\label{healthcare-provider-admission-rates}}

We work for a healthcare insurer and our management is interested in understanding the relationship between input admission and outpatient rates as drivers of expenses, payroll, and employment. We gathered a sample of 200 hospitals in a test market in this data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x_data <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/hospitals.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Build a table that explores this data set variable by variable and relationships among variables.
\item
  Investigate the influence of admission and outpatient rates on expenses and payroll. First, form these arrays.
\end{enumerate}

Next, compute the regression coefficients.

Finally, compute the regression statistics.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Use this code to investigate further the relationship among predicted expenses and the drivers, admissions and outpatients.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(reshape2)}
\KeywordTok{library}\NormalTok{(ggplot2)}
\NormalTok{actual <-}\StringTok{ }\NormalTok{y}
\NormalTok{predicted <-}\StringTok{ }\NormalTok{X }\OperatorTok{%*%}\StringTok{ }\NormalTok{beta_hat}
\NormalTok{residual <-}\StringTok{ }\NormalTok{actual }\OperatorTok{-}\StringTok{ }\NormalTok{predicted}
\NormalTok{results <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{actual =}\NormalTok{ actual,}
    \DataTypeTok{predicted =}\NormalTok{ predicted, }\DataTypeTok{residual =}\NormalTok{ residual)}
\CommentTok{# Insert comment here}
\NormalTok{min_xy <-}\StringTok{ }\KeywordTok{min}\NormalTok{(}\KeywordTok{min}\NormalTok{(results}\OperatorTok{$}\NormalTok{actual), }\KeywordTok{min}\NormalTok{(results}\OperatorTok{$}\NormalTok{predicted))}
\NormalTok{max_xy <-}\StringTok{ }\KeywordTok{max}\NormalTok{(}\KeywordTok{max}\NormalTok{(results}\OperatorTok{$}\NormalTok{actual), }\KeywordTok{max}\NormalTok{(results}\OperatorTok{$}\NormalTok{predicted))}
\CommentTok{# Insert comment here}
\NormalTok{plot_melt <-}\StringTok{ }\KeywordTok{melt}\NormalTok{(results, }\DataTypeTok{id.vars =} \StringTok{"predicted"}\NormalTok{)}
\CommentTok{# Insert comment here}
\NormalTok{plot_data <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(plot_melt, }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{predicted =} \KeywordTok{c}\NormalTok{(min_xy,}
\NormalTok{    max_xy), }\DataTypeTok{variable =} \KeywordTok{c}\NormalTok{(}\StringTok{"actual"}\NormalTok{, }\StringTok{"actual"}\NormalTok{),}
    \DataTypeTok{value =} \KeywordTok{c}\NormalTok{(max_xy, min_xy)))}
\CommentTok{# Insert comment here}
\NormalTok{p <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(plot, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ predicted,}
    \DataTypeTok{y =}\NormalTok{ value)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \FloatTok{2.5}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme_bw}\NormalTok{()}
\NormalTok{p <-}\StringTok{ }\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\NormalTok{variable, }\DataTypeTok{scales =} \StringTok{"free"}\NormalTok{)}
\NormalTok{p}
\end{Highlighting}
\end{Shaded}

\hypertarget{practice-set-debrief}{%
\subsection{Practice Set Debrief}\label{practice-set-debrief}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  List the R skills needed to complete these practice labs.
\item
  What are the packages used to compute and graph results. Explain each of them.
\item
  How well did the results begin to answer the business questions posed at the beginning of each practice lab?
\end{enumerate}

\hypertarget{project-captive-financing}{%
\section{Project: Captive Financing}\label{project-captive-financing}}

This project will allow us to practice various \texttt{R} features using live data to support a decision regarding the provision of captive financing to customers at the beginning of this chapter. We will focus on translating regression statistics into \texttt{R}, plotting results, and interpreting ordinary least squares regression outcomes.

\hypertarget{problem-1}{%
\subsection{Problem}\label{problem-1}}

As we researched how to provide captive financing and insurance for our customers we found that we needed to understand the relationships among lending rates and various terms and conditions of typical equipment financing contracts.

We will focus on one question:

\begin{quote}
\emph{What is the influence of terms and conditions on the lending rate of fully committed commercial loans with maturities greater than one year?}
\end{quote}

\hypertarget{data-1}{%
\subsection{Data}\label{data-1}}

The data set \texttt{commloan.csv} contains data from the St.~Louis Federal Reserve Bank's \href{https://fred.stlouisfed.org/categories/32407}{FRED} website we will use to get some high level insights. The quarterly data extends from the first quarter of 2003 to the second quarter of 2016 and aggregates a survey administered by the St.~Louis Fed. There are several time series included. Each is by the time that pricing terms Were set and by commitment, with maturities more than 365 Days from a survey of all commercial banks. Here are the definitions.

\begin{center}
\begin{tabular}{| l | p{4cm} | l | }
  \hline
  Variable & Description & Units of Measure \\
  \hline
  rate & Weighted-Average Effective Loan Rate & percent \\ \hline
  prepay & Percent of Value of Loans Subject to Prepayment Penalty & percent \\ \hline
  maturity & Weighted-Average Maturity/Repricing Interval in Days & days \\ \hline
  size & Average Loan Size & thousands USD \\ \hline
  volume & Total Value of Loans & millions USD \\
  \hline
\end{tabular}
\end{center}

\hypertarget{work-flow}{%
\subsection{Work Flow}\label{work-flow}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Prepare the data.
\end{enumerate}

\begin{itemize}
\item
  Visit the \href{https://fred.stlouisfed.org/categories/32407}{FRED} website. Include any information on the site to enhance the interpretation of results.
\item
  Use \texttt{read.csv} to read the data into \texttt{R}. Be sure to set the project's working directory where the data directory resides. Use \texttt{na.omit()} to clean the data.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# setwd('C:/Users/Bill}
\CommentTok{# Foote/bookdown/bookdown-demo-master')}
\CommentTok{# the project directory}
\NormalTok{x_data <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"data/commloans.csv"}\NormalTok{)}
\NormalTok{x_data <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(x_data)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  Assign the data to a variable called \texttt{x\_data}. Examine the first and last 5 entries. Run a \texttt{summary} of the data set.
\item
  What anomalies appear based on these procedures?
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Explore the data.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Let's plot the time series data using this code:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(reshape2)}
\CommentTok{# Use melt() from reshape2 to build}
\CommentTok{# data frame with data as id and}
\CommentTok{# values of variables}
\NormalTok{x_melted <-}\StringTok{ }\KeywordTok{melt}\NormalTok{(x_data[, }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{)], }\DataTypeTok{id =} \StringTok{"date"}\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ x_melted, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ date,}
    \DataTypeTok{y =}\NormalTok{ value)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\NormalTok{variable,}
    \DataTypeTok{scales =} \StringTok{"free_x"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  Describe the data frame that \texttt{melt()} produces.
\item
  Let's load the \texttt{psych} library and produce a scatterplot matrix. Interpret this exploration.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Analyze the data.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Let's regress \texttt{rate} on the rest of the variables in \texttt{x\_data}. To do this we form a matrix of independent variables (predictor or explanatory variables) in the matrix \texttt{X} and a separate vector \texttt{y} for the dependent (response) variable \texttt{rate}. We recall that the \texttt{1} vector will produce a constant intercept in the regression model.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\KeywordTok{as.vector}\NormalTok{(x_data[, }\StringTok{"rate"}\NormalTok{])}
\NormalTok{X <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\DecValTok{1}\NormalTok{, x_data[, }\KeywordTok{c}\NormalTok{(}\StringTok{"prepaypenalty"}\NormalTok{,}
    \StringTok{"maturity"}\NormalTok{, }\StringTok{"size"}\NormalTok{, }\StringTok{"volume"}\NormalTok{)]))}
\KeywordTok{head}\NormalTok{(y)}
\KeywordTok{head}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  Explain the code used to form \texttt{y} and \texttt{X}.
\item
  Calculate the \(\hat{\beta}\) coefficients and interpret their meaning.
\item
  Calculate actual and predicted \texttt{rates} and plot using this code.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Insert comment here}
\KeywordTok{library}\NormalTok{(reshape2)}
\KeywordTok{library}\NormalTok{(ggplot2)}
\NormalTok{actual <-}\StringTok{ }\NormalTok{y}
\NormalTok{predicted <-}\StringTok{ }\NormalTok{X }\OperatorTok{%*%}\StringTok{ }\NormalTok{beta.hat}
\NormalTok{residual <-}\StringTok{ }\NormalTok{actual }\OperatorTok{-}\StringTok{ }\NormalTok{predicted}
\NormalTok{results <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{actual =}\NormalTok{ actual,}
    \DataTypeTok{predicted =}\NormalTok{ predicted, }\DataTypeTok{residual =}\NormalTok{ residual)}
\CommentTok{# Insert comment here}
\NormalTok{min_xy <-}\StringTok{ }\KeywordTok{min}\NormalTok{(}\KeywordTok{min}\NormalTok{(results}\OperatorTok{$}\NormalTok{actual), }\KeywordTok{min}\NormalTok{(results}\OperatorTok{$}\NormalTok{predicted))}
\NormalTok{max_xy <-}\StringTok{ }\KeywordTok{max}\NormalTok{(}\KeywordTok{max}\NormalTok{(results}\OperatorTok{$}\NormalTok{actual), }\KeywordTok{max}\NormalTok{(results}\OperatorTok{$}\NormalTok{predicted))}
\CommentTok{# Insert comment here}
\NormalTok{plot_melt <-}\StringTok{ }\KeywordTok{melt}\NormalTok{(results, }\DataTypeTok{id.vars =} \StringTok{"predicted"}\NormalTok{)}
\CommentTok{# Insert comment here}
\NormalTok{plot_data <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(plot_melt, }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{predicted =} \KeywordTok{c}\NormalTok{(min_xy,}
\NormalTok{    max_xy), }\DataTypeTok{variable =} \KeywordTok{c}\NormalTok{(}\StringTok{"actual"}\NormalTok{, }\StringTok{"actual"}\NormalTok{),}
    \DataTypeTok{value =} \KeywordTok{c}\NormalTok{(max_xy, min_xy)))}
\CommentTok{# Insert comment here}
\NormalTok{p <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(plot, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ predicted,}
    \DataTypeTok{y =}\NormalTok{ value)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size =} \FloatTok{2.5}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme_bw}\NormalTok{()}
\NormalTok{p <-}\StringTok{ }\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\NormalTok{variable, }\DataTypeTok{scales =} \StringTok{"free"}\NormalTok{)}
\NormalTok{p}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  Insert explanatory comments into the code chunk to document the work flow for this plot.
\item
  Interpret the graphs of actual and residual versus predicted values of \texttt{rate}.
\item
  Calculate the standard error of the residuals, Interpret its meaning.
\end{itemize}

\hypertarget{appendix-b---canons-of-empirical-method}{%
\chapter*{Appendix B - Canons of Empirical Method}\label{appendix-b---canons-of-empirical-method}}
\addcontentsline{toc}{chapter}{Appendix B - Canons of Empirical Method}

\hypertarget{canons-of-empirical-method}{%
\section{Canons of Empirical Method}\label{canons-of-empirical-method}}

Our approach to decision making is not ethereal, it is concrete, yes, but supplemented with insight, so much so we feel compelled to decide on something different. Our horizons will remain static and decay; or they will expand and grow. We might ponder this question using Bernard \citet{lonergan_insight} \textbf{Canons of Empirical Method}. Here is a synopsis of these guidelines. There are six: selection, operations, relevance, followed by complete explanation, parsimony, and statistical residues.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Selection.}. This canon would restrict us to sensible data, data we can observe as existing outside of our mental images of the data in our minds.
\end{enumerate}

\begin{itemize}
\item
  We can only use propositions against which we can apply the restricted sensible data.
\item
  If we cannot do this, we cannot possibly search for intelligible anything in objective reality, namely the pattern in the data susceptible to description, explanation, and perhaps prediction. Operations is a many-headed hydra.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Operations.} This canon is responsible for expanding our consciousness about data using the cumulative font of previously verified, and unverified, work. This means we use prior research to inform answers to current questions, but it must be relevant. Operations also helps us construct mentally, for all analysis is a figment of the imagination and the intellect, but applied to the ever expanding font of previously verified work.
\end{enumerate}

\begin{itemize}
\item
  Constructions are cumulative and so too are they verified. We keep what works and throw the rest into a bin for reuse, if only to remember what does not work. In this way the accumulation of works are also systematic in that different pieces of constructions provide integrity, harmony, and even clarity to any analysis. And if they do not, then they go into the bin for reuse, repurposing. With operational constructions we understand and have a history of previous endeavors, how well they did or did not work, what we can use or discard from previous analyses.
\item
  In a word or two, operations allow us to transcend our current state of operations and envision a different, hopefully better by some rubric, way to proceed. All of this is the answer to a question about the shape or form of the analysis. It is ultimately the work of re-selection and the harbinger of relevance.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Relevance.} Relevant data, propositions, criteria for choosing one intelligible pattern rather than another, one decision alternative than another, all presuppose that data can be applied to different sets of hypotheses, inform different species of decisions, each require different sets of criteria. Data are raw and that flexible. Using statistics as derived data can be even more problematic. Averages of transactions can wash out very high and very low levels of measures, both informative in their extremes. Perhaps they might be the only relevant part of a data set, whereas the averages tell us nothing much interesting about the pattern of the data we call intelligibility. And thus when applied to a decision the analysis is irrelevant, plausibly.
\end{enumerate}

\begin{itemize}
\item
  A most important part of the Canon of Relevance is that it views data as emanating from beings as they relate to one another, not to our measurements, the instruments by which we measure and record, even to the observers.
\item
  This canon is a first step to a principled notion of objectivity a built on the immediately restrictive selection of terms, propositions, arguments, and verified operational explanations. There are immanent patterns latent in all data which has nothing to do with how the analysis is run, why we are analyzing something in the first case, even what are the materials and resources involved in the analysis. These immanent patterns in sensible data we are collecting are part and parcel of the formal causality of the analysis. Nascent, inchoate somewhat, yes, but immanently and thus relevantly critical to our analysis mission.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  \textbf{Complete Explanation.} The end of all inquiry is to gain as complete an explanation of observed data given our various conjectures, theories, and hypotheses about the data. But the data is not only how many widgets we make a minute in several facilities linked by rail, it is also the data of what guides our priorities in deciding what widgets, facilities, linkages are more important than others. We thus have observed data, so-called hard data, and more latent preferences based on norms, guidelines, conscience, in a word, values. This is the bedrock of making good decisions. The necessary triad of selection, operations, and relevance then erupts into the best of all possible current worlds, including our bias, of a way forward.
\item
  \textbf{Parsimony.} Apparently William of Ockham never had the proverbial razor. What he actually stated was ``Numquam pluralitas non est ponenda sine necessitate.'' (roughly now: any plurality is not to be posited without necessity.) This principle is a component of the Canon of Parsimony, wherein any scientist must eliminate any statement which is not verifiable and include only those statements which, currently, are. There are two sides to the parsimony coin.
\end{enumerate}

\begin{itemize}
\item
  We have just seen that the Canon of Relevance also requires the parsimonious inclusion of any insights which add to the raw data. While laws of nature, including human behavior in managerial decisions, might in a particular context seem to fail, repeated experiments, contexts, situations should be able to come to a statistical law which can be affirmed as verified. The key is to understand the difference between a law and an event.
\item
  Events can deviate from laws (trends, hypotheses, models), but to be verifiable they cannot deviate in any systematic way. If they do the other side of parsimony directs us to consider including the perhaps newly discovered deviation into a new formulation of the law, trend, or model. \^{}{[}This is an often used principle of logic and selection of the minimal set of hypotheses needed to answer a question. St.~Thomas Aquinas uses this principle to set up an objection to the existence of God, which he roundly refutes by the superiority of causal over simple explanations of anything. \citet{aquinas_st}, \emph{Summa Theologiae}, q.3, a.3, ad 2.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  \textbf{Statistical Residues.} Yes, outliers finally. These are particular, concrete situations not contemplated by all of the systematic analysis we try to rely upon. Yet, we make the decision, not knowing exactly how the decision will play out. We reserve for failures, and often omit the asteroid-event we missed along the way. We are not only imperfect reasoners, we are also imperfect, incomplete decision makers.
\end{enumerate}

\begin{itemize}
\item
  Our brilliant ideas are abstractions from practical reality. Thus they can only be further explained and used to predict when yoked with the concrete circumstances of new events as they unfold, thus the need for a Canon of Operations. Thus also more work by the imperfect human reasoner is always needed to determine further insights mashed together with concrete situations. Abstraction cannot simply be the impoverished replicas of observed data. They must exceed, transcend the data, by operating on the data with new insights.
\item
  As if this is not enough, if a new situation arises, then unsystematic deviations might need either to be incorporated into new models or else discarded as residues, at least relative to the abstraction called a model. But one last point is that if the abstract understanding called a model begins to degenerate, there is the ``inverse insight'' that this degeneration is also intelligible and must be reported as a claim to be acted upon in a decision. Thus this canon seems to point to another application of the Canon of Relevance to focus our attention on what claims, insights, theories, add to the data and its (relatively) complete explanation.
\end{itemize}

\hypertarget{rules-to-think-by}{%
\section{Rules to think by}\label{rules-to-think-by}}

\citet{Zellner1971} espouses a Bayesian analysis of inferential deductive logic for hypotheses as well as for decisions. He follows \citet{Jeffreys1966} and his nine rules of inductive inference. We can restate them here using the epistemological language of \citet{lonergan_insight} and his canons of general empirical method.\footnote{The canons are listed, related to one another, and explicated in \emph{Insight}, chapter 3. Therein is a thematic approach to the inference rules set out by Jeffreys, Jaynes and Zellner.}

\textbf{Rule 1.} ``All hypotheses used may be explicitly stated, and the conclusions must follow from the hypotheses.'' (\citet{Jeffreys1966}, p.~8) A complete set of hypotheses, decision alternatives, must be explicit to the analyst and the consumer of the decision maker. But further, these hypotheses must deductively result in a ``complete explanation'' (\citet{lonergan_insight}, p.~107-109) in the sense that for a universe of discourse carved out by the analyst and decision maker, the hypotheses must be able to result in a comprehensive set of conclusions. One imagines the wrangle over the logically inconsistent use of \emph{modus tollens} to base inferences with the Null Hypothesis Significance Test framework. If \emph{modus tollens} is \(\text{IF A, THEN B, AND NOT B, THEN NOT A}\) then our deduction that \(\text{NOT A}\) is valid. However if we deny the antecedent and argue \(\text{IF A, THEN B, AND NOT A, THEN NOT B}\) we deduce inconsistently by denying the antecedent.\footnote{See this note in the \emph{Oxford Reference} site: \url{https://www.oxfordreference.com/display/10.1093/oi/authority.20110803095711627}}

\begin{example}
\protect\hypertarget{exm:unnamed-chunk-176}{}\label{exm:unnamed-chunk-176}\leavevmode

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{If burglars, who do not have a key, entered by the front door (\(A\) the antecedent), then they must have forced the lock, which requires a key (\(B\) the consequent).}

Now suppose we have data which indicates that \textbf{the lock was not forced, \(\sim B\), denying the consequent}. We can deduce from this data, namely, that the burglars did not force the lock, \(\sim B\), that thus they also did not enter by the front door, \(\sim A\). This is \emph{\textbf{modus tollens}} or \textbf{denying the consequent}.

If the data we have is that the burglars did not enter by the front door, \(\sim A\) denying the antecedent, then it is illogical to deduce that they did not force the lock. \(\sim B\). This is the fallacy of \textbf{denying the antecedent}. They might have forced the lock, was startled by the owners' attack cat, and decided to enter the house by another route.
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

\end{example}

\textbf{Rule 2.} ``The theory must be self-consistent; that is, it must not be possible to derive contradictory conclusions from the postulates and any given set of observational data.'' (\citet{Jeffreys1966}, p.~5) This rule works in tandem with the next rule.

\textbf{Rule 3.} ``Any rule must be applicable in practice. A definition is useless unless the thing defined can be recognized in terms of the definition when it occurs. The existence of a thing or the estimate of a quantity must not involve an impossible experiment.'' (\citet{Jeffreys1966}, p.~5) Here are \citet{lonergan_insight}'s Canons of Selection, Relevance and Operations at work. We can unpack this rule by a three-way division of labor. But we will only stop by the Canon of Relevance here. We defer to Rule 5 for Canons of Selection and Operations, not that they do not apply here! There is strong connection between Rules 3 and 5 bridged by Operations and Selection within the context of Operations. Both of these point to Relevance, here. In fact it seems that Relevance and Operations are flip-sides of the same coin.

Relevant data, propositions, criteria for choosing one intelligible pattern rather than another, one decision alternative than another, all presuppose that data can be applied to different sets of hypotheses, inform different species of decisions, each require different sets of criteria. Data are raw and that flexible. Using statistics as derived data can be even more problematic. Averages of transactions can wash out very high and very low levels of measures, both informative in their extremes. Perhaps they might be the only relevant part of a data set, whereas the averages tell us nothing much interesting about the pattern of the data we call intelligibility. And thus when applied to a decision the analysis is irrelevant, plausibly. A most important part of the Canon of Relevance is that it views data as emanating from beings as they relate to one another, not to our measurements, the instruments by which we measure and record, even to the observers. This canon is a first step to a principled notion of objectivity. There is are immanent patterns latent in all data which has nothing to do with how the analysis is run, why we are analyzing something in the first case, even what are the materials and resources involved in the analysis. These immanent patterns in sensible data we are collecting are part and parcel of the formal causality of the analysis. Nascent, inchoate somewhat, yes, but immanently and thus relevantly critical to our analysis mission.

\textbf{Rule 4.} ``The theory must provide explicitly for the possibility that inference made by it may turn out to be wrong.'' (\citet{Jeffreys1966}, p.~6) Jeffreys goes on to state, categorically, we might be wrong do to error, incomplete and evolving data, and an attitude of never allowing for revision. On the same page he goes on to say ``\ldots{} {[}W{]}e have a certain amount of confidence that it will be right in any particular case, though this confidence does not amount to logical certainty.'' ( \emph{ibid.}) \citet{lonergan_insight} agrees and uses this notion to buttress an uncertainty principle in our analytical work. ``{[}While i{]}t is true enough that data are hazy, that measurements are not perfectly accurate, that measuring can distort the measured object \ldots{} {[}, yet one{]} can affirm them {[}and{]} continue to misconceive classical laws{[}, such falling bodies.{]}'' (p.~125) How do we characterize the ways of affirming the indeterminant nature of laws in the concrete? Not imaginatively, but as the ``indeterminancy of the abstract.'' (p.~125)

\textbf{Rule 5.} ``The theory must not deny any empirical proposition \emph{a priori}; any precisely stated empirical proposition must be formally capable of being accepted, in the sense of the last rule {[}4{]}, given a moderate amount of relevant evidence.'' (\citet{Jeffreys1966}, p.~6) Yes, only relevant propositions are allowed. The only relevant propositions are those that have some sort of chance of surviving the rigors of the Canon of Selection (\citet{lonergan_insight}, p.~94-97) and the Canon of Operations (p.~97-99) implicit in our formal approach to finding patterns, intelligibility in otherwise unintelligible data. Selection would restrict us to sensible data, data we can observe as existing outside of our mental images of the data in our minds. This canon and here Rule 5 restrict us to propositions against which we can apply the restricted sensible data. If we cannot do this, we cannot possibly search for intelligible anything in objective reality, namely the pattern in the data susceptible to description, explanation, and perhaps prediction. Operations is a many-headed hydra.

Operations would help us expand our consciousness about data using the cumulative font of previous work. This means we use prior research to inform answers to current questions, but it must be relevant. Operations also helps us construct mentally, for all analysis is a figment of the imagination and the intellect. Constructions are cumulative and so are they verified. We keep what works and throw the rest into a bin for reuse. In this way they are also systematic in that different pieces of constructions provide integrity, harmony, and even clarity to any analysis. And if they do not, then they go into the bin for reuse, repurposing. With operational constructions we understand and have a history of previous endeavors, how well they did or did not work, what we can use or discard from previous analyses. In a word or two, operations allow us to transcend our current state of operations and envision a different, hopefully better by some rubric, way to proceed. All of this is the answer to a question about the shape or form of the analysis.

These five rules are considered by many, including Jeffreys and Zellner, to be \emph{essential}. Jeffreys also indicates three more useful rules.

\textbf{Rule 6.} ``The number of postulates should be reduced to a minimum.'' (\citet{Jeffreys1966}, p.~6) This is often stated as a variant of William of Occam's Razor. What William actually stated was apparently ``Numquam pluralitas non est ponenda sine necessitate.'' (Any plurality is not to be posited without necessity.) This principle is a component of the Canon of Parsimony for \citet{lonergan_insight}, p.102, wherein any scientist must eliminate any statement which is not verifiable and include only those statements which, currently, are. On the other hand, Lonergan's Canon of Relevance (p.~103) also requires the albeit parsimonious inclusion of any insights which add to the raw data. While laws of nature might in a particular experiment seem to fail, repeated experiments should be able to come to a statistical law which can be affirmed as verified. The key is to understand the difference between a law and an event. Events can deviate from laws (trends, hypotheses, models), but to be verifable they cannot deviate in any systematic way. If they do the other side of parsimony directs us to consider including the perhaps newly discovered deviatoin into a new formulation of the law, trend, or model. \footnote{This is an often used principle of logic and selection of the minimal set of hypotheses needed to answer a question. St.~Thomas Aquinas uses this principle to set up an objection to the existence of God, which he roundly refutes by the superiority of causal over simple explanations of anything. \emph{Summa Theologiae}, q.3, a.3, ad 2. }

\textbf{Rule 7}. ``While we do not regard the human mind as a perfect reasoner, we must accept it as the only one available. The theory need not represent actual thought processes in detail, but should agree with them in outline.''" (\citet{Jeffreys1966}, p.~9) As with \citet{lonergan_insight}, p.~124-125, our brilliant ideas are abstractions from practical reality. Thus they can only be further explained and used to predict when yoked with the concrete circumstances of new events as they unfold. Thus also more work by Jeffreys' imperfect human reasoner is always needed to determine further insights mashed together with concrete situations. As if this is not enough, Lonergan goes on to imply if a new situation arises, then unsystematic deviations might need either to be incorporated into new models or else discarded as residues, at least relative to the abstraction called a model. But one last point is that if the abstract understanding called a model begins to degenerate, there is the ``inverse insight'' (p.~125) that this degeneration is also intelligible and must be reported as a claim. Thus this rule seems to be a species of Lonergans Canon of Statistical Residues. But it also seems to require another Canon of Relevance to ``fix our attention on what insight adds to data.'' (p.~125)

Something must be said about the imperfect human reasoner. And this will not be a foray as much as into psychology as it will be into epistemology, the study of how it is we know anything at all. The object is all being, asking the question \emph{what is it?} presupposing perhaps but at the least alongside the more basic question \emph{is it?} and all of this incompletely (we do not know all the rules of this road) and imperfectly (we will at times erroneous apply whatever rules we seem to know). The first question is \emph{formal causality}, what is the nature of, blueprint for, scaffolding, framework, approach we will use for our analysis. The second question is about the existence of the object of our various desires to understand. But it raises a question of \emph{final causality} called \emph{why bother?} or \emph{what's the purpose?} the end we would like to achieve on this mission, should we accept it. Both of these raise two more questions of \emph{material causality}, the what's in the process, the people, technology platforms, resources, materials, inputs, and \emph{efficient causality}, the how the process of inputs, activities, technology, know-how, conventional wisdom, yielding desired outputs by decision makers. Let's apply this causal heuristic to get a thumbnail sketch of the imperfect human reasoner.

\begin{itemize}
\item
  The reasoner, or community of reasoners for that matter, are \emph{aware} (not not, remember the adjective imperfect) of raw data, use operations to observe and record the data, or at least somehow remember the data.
\item
  The reasoner then \emph{understands} the data by throwing provisional hypotheses like darts onto a known dart board. There is a center on this dart board, the goal, the finality of the understanding. Some darts stick (they are consistent with the data), others eventually will fall off to the floor. Whatever is left is the pattern of data we might call the beginnings of an understanding.
\item
  While there are several pointed projectiles on the dartboard only those close enough to the center are very plausible, others not so. We can even measure the distances of darts to the center to quantify our analysis. We might say at this point that we can make a first judgment based on the \emph{is it?} question of \emph{yes it is}.
\end{itemize}

There is a pattern in the data. This is what we will mean by \emph{systematic}. It might also answer something about the nature of the data, the \emph{what is it?}. Attributes like long-lived, very complicated, and it is blue by the way can be verified on the dartboard. We have verified in data, using a reasonable approach to belief in plausibility, called a distance. We can even add up all of the distances to normalize the distance in a metric we call a probability. Quite a leap and all part of the answer to the question of \emph{what is it?} and the blueprint we are using to answer questions at all.

\textbf{Rule 8.} ``In view of the great complexity of induction, we cannot help to develop it more thoroughly than deduction.'' (\citet{Jeffreys1966}, p.~10) And thus the role of the use of the logic of pure mathematics. But if we use pure mathematics in this logical way, then the notion of probability is not that of empirical frequencies as we usually view them in elementary statistics courses, rather the ordered degrees of reasonable, rational, belief about a claim. This means also that any inductively constructed claim should emanate from a deductively valid logic. One builds on the other. One verifies the other in concrete events. One provides a deduced ordering of hypotheses when hypotheses are confronted by data. There is thus some sort of \emph{uncertainty principle} lurking here as Lonergan points out: ``For the concrete includes a nonsystematic component, and so the concrete cannot be deduced in its full determinancy from any set of systematic premises.'' (\citet{lonergan_insight}, p.~123)

\hypertarget{appendix-c.-logicks-the-algebra}{%
\chapter*{Appendix C. Logicks, the Algebra}\label{appendix-c.-logicks-the-algebra}}
\addcontentsline{toc}{chapter}{Appendix C. Logicks, the Algebra}

\hypertarget{why-this-essay}{%
\section{Why this essay}\label{why-this-essay}}

The end we seek here is to discover a principled way to determine the validity, coherence, and plausibility of our statements. Our work takes the form of an argument which is just a sequence of statements with premises which offer a theory of the argument and also data to fit to the theory. Premises and data lead to a conclusion which essentially is all about the fit of data to the theory. We will work with basic algebra and arithmetic applied to natural language statements. But in effect, the premises, data, conclusions and the process itself of the argument must make practical sense to the arguer. We will call this requirement a canon of common sense. Theories are usually in some sort of conditional form, if-this, then-that. Observed (or at least stated as so) data relate to terms in the theory. The structure of the argument then infers a conclusion. Any statement here is either true or false. We will assign a 1 if true and a 0 if false to quantify our arguments in the binary integer field.

Why do we need a principled way to determine the validity of sequences of our statements? We will maintain a hypothesis we might call a canon of rationality. This canon will hold throughout all of our proceedings here. In a nutshell we will place the practice and science of logic in the only context we know, namely, ourselves. We are not machines or animals or plants or rocks, although we have aspects and attributes of each. We do however ask questions, rocks, plants, animals, machines (not even Artificial Intelligence) raises questions from experience, understanding, judgment, and decision. Yes, some machines may, if we switch them on, may have been programmed by a human to prompt strings of morphemes, terms, propositions, even arguments which have the same attributes as questions a human would ask. But of themselves, machines do not, on their own volition, prompt anything. Machines do not have a will. Machines can also be programmed to store information, instructions, and adapt responses to evolving instructions, They can also in that capacity write their own instructions, build and deploy, if programmed to, their own data bases and applications, and with great agility, even exceeding their human makers. Even a hammer exceeds human ability to break rocks which would otherwise break human hands if hands tried to break rocks. A technology, actually a technique, strictly, we will develop is an implementation of logic through the use of the rules of algebra. It is in this context that we need to situate our evolving technology of implementing ways to reason, judge, understand, the three common sense acts of the mind. Each of these acts of consciousness are quite necessarily experienced, observed, perhaps remembered and stored. Thus whenever we inquire about anything in our world, we generate data of what we observe, data of understanding, data of judging, and data of reasoning. We will call this polymorphic (many shaped) consciousness.

What is logic, still and up to this point using the procedure from high school algebra of ``let X (logic) be an unknown variable for which we are seeking a solution''? We can begin through the practice of logic, or what we might consider logical structures, by thinking about and through our reality. Thus using our thinking structures informed by our truly polymorphic human consciousness we can define logic. Through understanding we answer the question ``what is it?''. Within this question are the numerous questions of ``what is it composed of?'', questions of our experience of whatever it about which we inquire. By judgment we determine answers to the question ``whether it is so?''. By reason we mean finding valid conclusions which answer the question ``why is it so?''. Logic is the art and science of answering these questions. To enhance our understanding (``what is logic?'') we can contrast logic with ethics which answers the question ``what should we do about it?''.

We can go one step further. Through each of these questions we can ferret out typical answers. To the questions for understanding, ``what is it?'', we can propose two answers about the clarity of our understanding (going binary on us right now): ``it is clear what it is,'' or ``it is not clear what it is.'' Similarly we can propose two answers to questions of judgment, ``whether it is or not'': ``is it true,'' or ``it is not true.'' Lastly, in the field of inquiry called logic, we can propose two answers about the validity of our reasoning: ``it is valid,'' or ``it is not valid.'' We must immediately realize we have mutually exclusive, and again in the realm of a particular inquiry, completely exhaustive answers, all with exclusive ``or,'' that is, disjunctive. If we add up all the trues and all the falses we get all of the answers to all of the questions for judgment. So we suppose now we need some clarification about what we mean by clear, true, and valid.

We will consider our understanding to be clear when propositions are unambiguous, with terms well defined and not admitting of vague, hard to measure, and if not able to measure, hard to distinguish from other terms, and their role in propositions. Clarity is also achieved when the relationships among propositions are unambiguous.

Our judgments are true is easier if we follow Aristotle's formula in his Metaphysics 1011b25: ``To say of what is that it is not, or of what is not that it is, is false, while to say of what is that it is, and of what is not that it is not, is true.'' This is correspondence theory of truth, the need for which is familiar perhaps to anyone who has dealt with two year olds.

Valid reasoning is even easier as this layer is all about arguments. An argument (terms in propositions, propositions in premises, reasoning in conclusions) is valid only when clear terms, true prepositions in premises, and the conclusion necessarily follows from the premises. As simple as this is, if terms are not well-defined, and/or propositions are not clearly true (or false), then an arguments are not valid. Even if terms are well-defined and unambiguous, propositions are clearly true or false, arguments are not valid if the conclusion follows a line of reason not supported by clear premises and unambiguous terms. Finally, and here we actually get practical, even if the line of reasoning formally is supported by clearly understood premises, an argument is not valid in the field of observable reality if the conclusion makes no sense, common or otherwise.

Let's use the cats and gods from the example in Peter \citet{kreeft2005socratic} (32).

\[
\begin{array}{r | c}
\text{cats and gods}                 &                 \\ \hline  
\text{A: if I am a cat, then B: I am a god} & A \rightarrow B \\
\text{but, A: I am a cat}                    & A               \\ \hline
\text{therefore, B: I am a god}         & \therefore B
\end{array}
\]

This argument makes mathematical sense in a way we will later demonstrate. If the premises are true, then we have a logically valid argument since the conclusion follows necessarily (meaning, always true, happens all the time everywhere) from the premises. And we will say mathematically true since, we will show, symbolically the logic robot we will build will always return a true value for the conclusion. But this is common non-sense! I am not a cat, and even if I were to be a cat with whiskers, talons, teeth, and a pretty furry face, I could not be a god no matter how hard I try! So, the premises are not possibly true in this reality we wake up to each day, and the argument fails, augers into the ground, burns down.

There exists right across from the typewriter producing a this text a cat named Waffles (ginger, about 17 years old, or so we think and he's asleep, again so we think). There exist other animals called dogs, and we know very well they are quite different beasts. Yes, we allow ourselves the luxury of begin able to abstract from individuals to existential groups of similar individuals, a pillar of the scientific method. We will now use this same logical argument, its structure, to inquire about Waffles.

\[
\begin{array}{r | c}
\text{cats and dogs}                 &                 \\ \hline  
\text{A: if Waffles is a cat, then B: Waffles is not a dog} & A \rightarrow B \\
\text{but, A: Waffles is a cat}                    & A               \\ \hline
\text{therefore, B: Waffles is not a dog}         & \therefore B
\end{array}
\]

We have unambiguous terms, Waffles, cat, dog; true propositions; straight line of reasoning from true propositions to a necessary conclusion. We have a logically valid argument, which definitely makes sense in some way to Waffles.

Here is an example of a logically invalid argument.

\[
\begin{array}{r | c}
\text{cats and dogs}                 &                 \\ \hline  
\text{A: if Waffles is a cat, then B: Waffles is not a dog} & A \rightarrow B \\
\text{but, A: Waffles is not a dog}                    & A               \\ \hline
\text{therefore, B: Waffles is a cat}         & \therefore B
\end{array}
\]

What we might not know is that there is also a hamster across the room also named Waffles. Hamster-Waffles is miffed at the suggestion that she is a cat. And now that we have a tad of clarity about validity, we can embark on a principled approach to a logic of probabilistic inference. This entails some further thought about inference in a setting with observed data and (un)observed conjectures about data. Our question is locatrred right there, and if there is a ``right there,'' how might we characterize the truthfulness of arguments in the face of sampling data relevant to the problem at hand. To do this we need to clarify further two approaches to probabilistic inference.

\hypertarget{two-schools-of-thought}{%
\section{Two schools of thought}\label{two-schools-of-thought}}

\citet{cox1946probability} describes two schools of probability. One is the frequency school which would sample events in ensembles indefinitely and across many universes. An ensemble is a combination of possible events. For example, the probability of extra-terrestrial life on one planet would require the counting of every possible combination of planetary existence in our universe and in any other universe as well. This would of course our universe is not a universe, nor are the other universes as well! Be that as it may, this notion of a long-run probability as a frequency requires, technically, an infinite amount of time across an infinite set of possible ensembles of events in a finite universe with finite time and space, let alone the annoying fact of a finite set of human observers with imperfect measurement technologies. The idea is thus a purely mental fabrication to aid comprehension of what is known to be incomplete (information about extra-terrestrial life), corruptible (measurement errors by human and their measurement technologies), and otherwise unimagined by humans who do not experience insight very often.

On the other hand there is the expectation school which thinks about probability as the logical consequence of how relatively likely one event or hypothesis is to another. In \citet{keynes2013treatise} view, the theory of probability is the logic of probable inference. Probability is a syllogistic relation from a hypothesis to data, and on to a conclusion. Each hypothesis is more or less plausible than another as measured by the ways in which data are consistent with a particular hypothesis. Plausibilities are bounded by extremes of impossibility and certainty, Aristotelian deductive logic is a special case of plausibility. This means we need more than binary true-false deduction to make statements about the plausibility of hypotheses in the face of data. We also no longer need the certainty uncomfortably afforded by leaps of imaginative extension of samplings into an infinity of possible combinations of hypotheses and events, namely the count of incidents. Instead we do need the count of ways in which data are consistent with a particular hypothesis. We now start from plausible numbers of ways consistent with data instead of infinite counts of instances of data.

The expectation school's results build on an algebra of the logic of propositions. We will begin with tables of algebraic relationships among propositions. Once we establish these rules we can operate on any logical statement with the power of 9th grade elementary algebra. We need just three algebraic operations to work the algebra for pairs of propositions \(a\) and \(b\): AND (\(a \times b\)), NOT (\(1+a\)), XOR (\(a+b\)). We need to add something else to our analysis: common sense, both espoused by \citet{jaynes2003probability} and \citet{kreeft2005socratic} in natural sciences and philosophy. Just because the math might work out logically (and usually will if we are very careful), does not mean we can interpret our results in a particular and realistic context. This is the problem of \textbf{existential import} which \citet{boole1848calculus} and purely symbolic logicians seem to eschew. They would have us believe that the math is the math, without reference to anything real as surveyed by \citet{wu1969problem}. At one extreme end of this view any \(a+b\), whatever \(a\) and \(b\) might represent objectively, are logically equivalent to any other \(a\) and \(b\), whatever they might represent to the human being attempting to reason. They may indeed not even exist, even hypothetically. We will follow this convention analyzed by \citet{read2015aristotle} regarding \citet{ackrill1975categories} and \citet{avicenna1971avicenna}'s commentaries.

``With an affirmation and negation one will always be false and the other true whether {[}Socrates{]} exists or not. For take `Socrates is sick' and `Socrates is not sick': if he exists it is clear that one or the other of them will be true or false, and equally if he does not; for if he does not exist `he is sick' is false but `he is not sick' true.'' ( \emph{De Interpretatione}, 13b26-31)

Here we will blithely consider propositions and their subject to at least probably exist so as to be able to discuss them with respect to the very real actions we undertake upon deliberation, reflection, and understanding of our conscious experience and held beliefs.

In what follows we develop together an algebraic approach to studying inference introduced by \citet{boole1848calculus} and expanded upon by \citet{peirce1870description}, \citet{keynes2013treatise}, \citet{jaynes2003probability}, and more recently \citet{suleimenov2023improving}. The logical system here relies on four fundamental logical forms developed by \citet{aristotle2009prior} and as explicated in practical logic by \citet{kreeft2005socratic} and \citet{toulmin2003uses}. Far deeper than this discussion, but notable in grounding these techniques is \citet{carnielli2015method}. From there we would move into the realm of non-distributive and still-reflexive logic which extends Boolean and Peircean classical logic to the possibilities of a quantum-infused logic (\citet{heelan1970quantum-logic}), but we will push this extension into an separate discussion.

\hypertarget{rules}{%
\section{Rules}\label{rules}}

We clarify our approach with rules learnt from basic arithmetic. Addition is defined as addition of integers, but restricted to modulo 2, with only two possible values \(\{0,1\}\).\footnote{\citet{burris2021SEP-algebra-logic-tradition} develops a sweeping history of the progress, decline, and revitalization of algebraic logic. \citet{burris2013horn} augment Boole's algebraic logic with a Rule of 0 and 1 founded on Horn sentences. This is significant because of the hermeneutical anti-compositional principle of building up an understanding of reasoned arguments from its propositional elements and in turn from terms. We take into account the argument as a whole and the reciprocal interaction of the whole argument with its components.} These values are the terms and the propositions themselves, not to mention the possible outcomes of sequences of propositions we would call an argument. If we remember our basic maths and modulo arithmetic as a clock, the hands on this clock can sit at 0 or 1. If the hand is at 1 and we add 1 to this setting we end up at, yes, 0. A regular OR in many languages would often mean and/or, an inclusive OR, an either one or the other or both. The XOR excludes any possibility in common between two propositions. Propositions are mutually exclusive of one another. Here is a table to help us.

\[
\begin{array}{c | c c}
+ (XOR)   &  0    &  1 \\ \hline
0         &  0    &  1 \\
1         &  1    &  0 
\end{array}
\]

Addition with boolean values \(\{0,1\}\) is the same as that of the logical exclusive OR operations, XOR. Since each element equals its opposite, subtraction is thus the same operation as addition, so that \(0-0=0\), \(0-1=1\) (remembering the clock with just two settings!), \(1-0=1\), and \(1-1=0\), anyway, all equivalent to the \(+\) rules. In words, it is \textbf{one or the other, not both}.

The multiplication is multiplication modulo 2 (see the table below), and on boolean variables corresponds to the logical AND operation. Here is the \textbf{both} proposition with two terms.

\[
\begin{array}{c | c c}
\times (AND)   &  0    &  1 \\ \hline
0         &  0    &  0 \\
1         &  0    &  1 
\end{array}
\]

The NOT logic simply takes a 0 and flips it to 1, and if 1 flips it to zero.

\[
\begin{array}{c | c}
\neg (NOT)   &  1+x   \\ \hline
0            &  1      \\
1            &  0      
\end{array}
\]

With these three operations we can evaluate any logical statement. And we must remember that NOT is not the subtraction operation we are very familiar with from non-binary algebra. Some folks will write \(\overline x\) instead of \(\neg\).

Here is a table of several useful expressions based on AND, XOR, and NOT. For any two propositions \(x\) and \(y\) whose values can only take on a 0 or 1, so that when \(x=0\), we mean \(x\) is false, otherwise (that is, \$x=1) true.\footnote{Boole has the negation of x as 1-x. We follow \citet{carnielli2015method} 1+x, given that all subtractions end up being addition in mod 2 arithmetic.}

\[
\begin{array}{c c | c c c c c}
  &   & AND & XOR & NOT & y \mid x & SUB \\ \hline
x & y & xy & x+y & 1+x & 1+x+xy    & x-y=x+y\\ \hline
0 & 0 & 0 & 0 & 1 & 1 & 0 \\
0 & 1 & 0 & 1 & 1 & 1 & 1 \\
1 & 0 & 0 & 1 & 0 & 0 & 1 \\
1 & 1 & 1 & 0 & 0 & 1 & 0\\ \hline
x & y & x \land y & x \oplus y & \neg x,\,\overline x & x \rightarrow y \\ \hline
x & y &     -      & (x \lor y) \land \neg (x \land y) & \neg x \lor y & \neg (x \oplus (x \land y)) \\
\hline
\end{array}
\]

The last row describes the algebraic operations in term of propositional logic as applied to Boolean values of \(\{0, 1\}\). I can easily get lost in that row. I tend to prefer 9th grade algebraic expressions.

The last column uses the negative to SUBtract \(y\) from \(x\). We might be tempted to say that \(x-y= x+ (1+y) = 1+x+y=1+(x+y)\) so that the subtraction operator is ``NOT the ADD'' operator. Nice try! We must always go back to the most literal, most primitive operations, namely the addition and multiplication rules which govern these more derived expressions.

That next to the last column will deserves some important attention since it forms the foundation of conditional (reasonably expected) probability. But first some very helpful derived relationships will make our work going forward a lot smoother.

\[
\begin{array}{c c | c c c c c}
x & x & xx=x^2 & xxx=x^3 & x+x=2x & x+x+x=3x & x-x=x+x\\ \hline
0 & 0 & 0      & 0       & 0      & 0        & 0      \\
0 & 0 & 0      & 0       & 0      & 0        & 0      \\
1 & 1 & 1      & 1       & 0      & 1        & 0      \\
1 & 1 & 1      & 1       & 0      & 1        & 0      \\ \hline
  & = & x      & x       & 0      & x        & 0      \\ \hline
\end{array}
\]

What happens here is that there are no monomial terms of any higher degree than 1; no quadratic or cubic terms at all. Two propositions literally cancel each other, as the Germans might say an \emph{Aufhebung} event. But three return the single proposition. All of this is the result of the modulo 2 arithmetic to which we constrain ourselves.

We now study \(y|x=1+x+xy\) in three moves. The first move is to realize that, at least for binary data, Aristotle discovered four logical forms, two in the affirmative, two negative; two universal, two contingent. Medieval logicians called the two affirmative Forms A and I for the first two vowels in \emph{AffIrmo}, Latin for ``I affirm,'' and the two negative forms E and O from the Latin \emph{nEgO}, ``I deny.'' Together they form the \textbf{Square of Opposition}. Here S is the subject and P the predicate. Any subject S signifies what it is we are talking about, say, rain. Any predicate P signifies what the subject is about, say, falling to the ground.

Equations and identities do not have a subject or a predicate and themselves might be the subject or predicate. But all propositions do have a subject and a predicate, just like in 3rd grade when we learned to write and speak in complete sentences, that is, in propositions, which contain a subject (usually a noun) and a predicate (usually a verb). We assume that when we apply the forms to concrete examples of propositions, the content of the form, that is, the S and the P, exist. For Thomas Aquinas signs are physical manifestations that allow us to understand something beyond their immediate appearance, like a footprint manifesting someone's presence or smoke manifesting fire. This something with an immediate appearance we will assume without further bother, that it somehow exists. Perhaps we append the particle \emph{any} to S and P to get any rain and any falling (of rain).

We build a table of logical forms in this first move. In the table, \emph{decisions} is the subject S, what we are talking about, and \emph{are rational} is the predicate P, what the subject is about.

\[
\begin{array}{c | c | c | c }
Form  & Proposition & Sentence & Algebra \\
\hline
A     & \text{All S is P}        & \text{"All decisions are rational."}         & a(1+b) = 0      \\ 
E     & \text{No S is P}         & \text{"Not all decisions are rational."}     & ab = 0          \\
I     & \text{Some S is P}       & \text{"Some decisions are rational."}        & ab \neq 0       \\
O     & \text{Some S is not P}   & \text{"Some decisions are not rational."}    & a(1+b) \neq 0   \\
\hline
\end{array}
\]

The second move is to parse the Form A proposition ``All decisions (a) are rational (b).'' Logically \(a\) and not \(b\) is false (0) in Form A, that is, algebraicly, \(a(1+b) = 0\). ``Decisions'' and ``not-rational'' is false, that is, inconsistent according to Form A. The obverse must be true, that ``No-decisions are not-rational,'' as if we might be able to interpret this double negative. One more swipe at interpretation is called for. To say that ``all of anything is something else'' is, effectively to identify ``anything'' with ``something else.'' If this statement is true, as we are positing here in the form, then it cannot be true that both ``anything = \(a\)'' and ``not-something else = \((1+b)\)'' can coexist. Thus the logical Form A seems to admit a very basic principle we would hold alongside all other principles (Aristotle called this \emph{metaphysics}), namely, the \emph{Principle of Non-Contradiction}. Yes, we cannot, so far it seems, to be in two places at the same time.

In a third move, we use the NOT operation on Form A. We recall we are using ``+'' as exclusive OR, XOR with this algebraic rearrangement.

\[
\begin{array}{c| c l}
statement & reason \\ 
\hline
1 & a(1+b) = 0     & \text{Form A definition}  \\
2 & 1 + a(1+b) = 1 & \text{Symmetric property} \\
3 & 1 + a + ab = 1 & \text{Distribution of multiplication over addition property} \\
\hline
4 & a \rightarrow b = b \mid a & \text{a gives some information to b} \\ 
\hline
\end{array}
\]

Simply negating (using 1+) the Form A definition reveals a conditional relationship between \(a\) and \(b\). Saying that \(a\) AND \(not-b\) is possible when we negate the Form A requirement, means \(a\) does share something with \(b\). So-called ``implication'' means that a shares a's information with b. This is the primary meaning we ascribe to ``a conditions b'', \(b \mid a\). We now have all the ingredients to use this framework to discover a principled path to the expectations approach to probability, that is, counting the ways in which data are consistent with hypotheses. But before we enter that particular room, we need to argue a bit more, that is, discover valid and invalid logical arguments.

\hypertarget{making-a-valid-point}{%
\section{Making a valid point}\label{making-a-valid-point}}

Let's begin with any two propositions or statements or messages \(A\) and \(B\). Specifically, let's talk about the weather. We let

\[
\begin{array}{r l}
A &= \text{You get an A.} \\
B &= \text{I give you a lollipop.}
\end{array}
\]

The implication is the statement \emph{If A, then B}. We will label A the \emph{antecedent} and B the \emph{consequent} Some of us might even want to go further than the literal logic and interpret, allegorically, A as the \emph{cause} and B the \emph{effect}. This will also be our starting point with the coherence and validity of our reasoning as well as plausibility. But we must first begin even more strongly with a guaranteed statement from the following deduction that stems from the implicating condition, the if-then. And by guaranteed we will mean with certainty. Yes, we can still talk about maybe and plausibility if we grant that the proposition ``this is plausible'' is TRUE or FALSE. First no quibbling with ``If it you get an A, then it \emph{must} happen that I will give you a lollipop.'' But I can also quibble that, ``If it you might get an A, then it \emph{must} happen that''I might give you a lollipop." Or even this (not so nice, perhaps passive aggressive approach) bit of wobbling that ``If it you do get an A, then it \emph{must} happen that''I might give you a lollipop."

The full name of structure of our first syllogism is \textbf{modus ponendo ponens} which is Latin for \emph{the way that, by affirming, affirms} , also known as \emph{affirming the antecedent} . And by \emph{syllogism} we mean a linking together of at least two separate propositions, messages, symbols, tokens, and meanings.

\[
\begin{array}{r | c}
\text{Modus Ponens}                 &                 \\ \hline  
\text{if A is (plausibly) true, then B is (plausibly) true} & A \rightarrow B \\
\text{and A is (plausibly) true}                    & A               \\ \hline
\text{therefore, B is (plausibly) true}         & \therefore B
\end{array}
\]

With or without the qualifier \emph{plausible} we will claim to have a deductive inference guaranteed to be consistent with the logic of implication.\footnote{It is invalid to affirm the consequent, that is say B is true, so A must be true, a frequent and very misleading and logically illegal argument on talk shows, especially when the subject is political.} We will demonstrate this claim shortly, actually prove it. For now let's just have a rather informal chat about TRUE and FALSE in this syllogism.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Suppose it is true that you get an A \emph{and} it is true that I give you a lollipop. Since I kept my promise, the implication, the statement, is true. Doesn't this sound like a logical conjunction? Both A and B are true, that is \(AB = 1 \times 1 = 1 = TRUE\) according to the hard work we did in building a binary algebra.
\item
  Suppose it's true that you get an A but it's false that I give you a lollipop. Since I didn't keep my promise, the implication is false. This truth values again act like a logical conjunction. If A is true and B is false, we have \(AB = 1 \times 0 = 0 = FALSE\).
\item
  What if it is false that you get an A? Whether or not I give you a lollipop (Q is true or Q is false), I have not broken my promise. Thus, the implication cannot be false, so the statement must be true. This case is no longer a simple logical conjunction, but we can use the negation to help us out with the idea that If A is false it is not-A, when A would otherwise be true. The conjunction \(\neg{A}B = 0 \times 1 = 0 = FALSE\) or equivalently \(1+AB=1 +1 \times 1=2 mod 2=0=FALSE\).
\end{enumerate}

Let's use our Algebra of Boole to check the reasoning in our informal chat.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We set out the structure. We let \(P:\) ``you get an A'' and \(Q:\) ``I'll give you a lollipop.'' With this assignment of propositions we state the argument as \([\,P \,\, \text{AND} \,\, (P \,\, \text{IMPLIES} \,\, Q)\,] \,\, \text{IMPLIES} \,\, Q\).
\item
  we translate this structure into algebra. For any \(A\) and \(B\), \(A \,\, \text{AND} \,\, B = AB\), and \(A \,\, \text{IMPLIES} \,\, B = 1 + B + BA\). Also we know that \(A^2 = AA = A\) and \(A + A = 0\) in the mod 2 arithmetic of the algebra of Boole. This means that our argument can be represented algebraicly as this expression.
\item
  Then at last we reduce all expressions algebraicly. If we end up with a 1, then we have logical satisfaction.
\item
  We apply common sense and existential import.
\end{enumerate}

\[
\begin{array}{r l}
[\,P \,\, \text{AND} \,\, (P \,\, \text{IMPLIES} \,\, Q)\,] \,\, \text{IMPLIES} \,\, Q & \text{[modus ponens]}\\ \hline
P(1+P+PQ) &\text{IMPLIES}\,\, Q \\
P + PP + PPQ &\text{IMPLIES}\,\, Q \\
P + P + PQ &\text{IMPLIES}\,\, Q \\
0 + PQ &\text{IMPLIES}\,\, Q \\ 
1 + PQ + PQ(Q) &= 1 + PQ + PQ \\
               &= 1 + 0 \\
               &= 1
\end{array}
\]

That wasn't so bad! We just proved, quite logically, that this argument returns a 1, to which we assign the value TRUE. Yes, it is valid logically.

Let's leap into the other mainstay of valid logic, the \textbf{modus tollens} argument structure. The full name is \textbf{modus tollendo tollens} which is Latin for \emph{the way that, by denying, denies} , also known as \emph{denying the consequent}. Grades and lollipops exist, and notwithstanding, the procedure is the same as for \emph{modus ponens}. The structure looks like this.

\[
\begin{array}{r | c}
\text{Modus Tollens}                 &                 \\ \hline  
\text{if A is true, then B is true} & A \rightarrow B \\
\text{and B is not true}                    & \neg B               \\ \hline
\text{therefore, A is not true}         & \therefore \neg A
\end{array}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We set out the structure. We let \(P:\) ``you get an A'' and \(Q:\) ``I'll give you a lollipop.'' With this assignment of propositions we state the \emph{modus tollens} argument as \([\,\text{NOT} \,\, Q \,\, \text{AND} \,\, (P \,\, \text{IMPLIES}\,\, Q)\,] \,\,\text{IMPLIES} \,\, \text{NOT} \,\, P\).
\item
  We translate this structure into algebra. Again for any \(A\) and \(B\), \(A \,\, \text{AND} \,\, B = AB\), and \(A \,\, \text{IMPLIES} \,\, B = 1 + B + BA\). Also we use the operator \(\text{NOT}\,\, A = 1+A\) This means that our argument can be represented algebraicly as this expression.
\item
  We reduce all expressions algebraicly. If we end up with a 1, we have logical satisfaction: the argument is logically consistent.
\item
  We apply common sense and existential import.
\end{enumerate}

\[
\begin{array}{r l}
[\,\text{NOT} \,\, Q \,\, \text{AND} \,\, (P \,\, \text{IMPLIES}\,\, Q)\,] \,\,\text{IMPLIES} \,\, \text{NOT} \,\, P & \text{[modus tollens]}\\ \hline
(1+Q)(1+P+PQ) &\text{IMPLIES}\,\, 1+P \\
1 + P + PQ + Q + QP + PQQ &\text{IMPLIES}\,\, 1+P \\
1 + P + Q + PQ &\text{IMPLIES}\,\, 1+P \\
= 1+ (1 + P + Q + PQ) + (1 + P + Q + PQ)(1+P)  \\
=  1 + (1 + P + Q + PQ) + (1+ P + Q + PQ + P + PP + PQ + PPQ) \\
= 1 + 0 \\
= 1
\end{array}
\]

TRUE again! We have yet another valid logical argument \emph{structure} only if step 4, \emph{common sense}, invokes a result. I have to be able to get a grade of A (it is possible at least) to get an actual lollipop (sugar-free and no sugar substitutes). Grades of A and lollipops exist and are at least somewhat accessible.

\hypertarget{nay-and-yay}{%
\section{Nay and Yay}\label{nay-and-yay}}

It turns out that much rhetoric, and some marketing campaigns and technology projects, use as least two invalid arguments: affirming the consequent and denying the antecedent. The first is a fallacy of \emph{modus ponens}: Affirm the Consequent. The second is a fallacy of \emph{modus tollens}: Deny the Antecedent. We shall focus our attention on splitting the hairs of the \emph{modus tollens} fallacy. We might want to examine the symmetry of these arguments as well.

Using our grade-for-lollipop scheme \emph{modus tollens} states that ``P: If you get an A, then Q: you receive a lollipop (or other appropriate bribe/incentive).''; but ``NOT Q: you did not get a lollipop''; so, ``NOT P: you did not get an A.'' Common sense will arrive at lots of, reasons why you did not get a lollipop, for example, local lollipop supply chain disruptions. But there is only one reason you did not get a lollipop even admissible here (that is, in the premise).

Here is the fallacy portion of our program for \emph{modus tollens}: Deny the Antecedent (P: a grade of A). This structure follows the line of thinking that ``P: If you get an A, then Q: you receive a lollipop (or other appropriate bribe/incentive).''; but ``NOT P: you did not get an A''; so, ``NOT Q: you did not get an lollipop.'' Polling nine out of ten (on average) groups of board members, project teams, classrooms full of students, over a span of almost 50 years say ``this makes sense. So what is the big deal?'' At least let's examine, from our principled perch on the binary branch of the algebra of Boole, whether or not this is so (yet another binary choice: Latin, \emph{an sit?}, ``is it?'' ).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We set out the structure. We let \(P:\) ``you get an A'' and \(Q:\) ``I'll give you a lollipop.'' With this assignment of propositions we state the \emph{modus tollens: denying the antecedent} argument as \([\,\text{NOT} \,\, P \,\, \text{AND} \,\, (P \,\, \text{IMPLIES}\,\, Q)\,] \,\,\text{IMPLIES} \,\, \text{NOT} \,\, Q\).
\item
  We translate this structure into algebra. Again for any \(A\) and \(B\), \(A \,\, \text{AND} \,\, B = AB\), and \(A \,\, \text{IMPLIES} \,\, B = 1 + B + BA\). Also we use the operator \(\text{NOT}\,\, A = 1+A\) This means that our argument can be represented algebraicly as this expression.
\item
  We reduce all expressions algebraicly. If we end up with a 1, we have logical satisfaction: the argument is logically consistent.
\item
  We apply common sense and our canon of existential import.
\end{enumerate}

\[
\begin{array}{r l}
[\,\text{NOT} \,\, P \,\, \text{AND} \,\, (P \,\, \text{IMPLIES}\,\, Q)\,] \,\,\text{IMPLIES} \,\, \text{NOT} \,\, Q & \text{[denying the antecedent]}\\ \hline
(1+P)(1+P+PQ) &\text{IMPLIES}\,\, 1+Q \\
1 + P + PQ + P + PP + PPQ &\text{IMPLIES}\,\, 1+Q \\
1 + P &\text{IMPLIES}\,\, 1+Q \\
= 1+ (1 + P) + (1 + P)(1 + Q)  \\
=  1 + (1 + P) + (1+ Q + P + PQ) \\
= 1 + P + PQ \\ 
\neq 1 \\
\end{array}
\]

With our logical Form A \(P + PQ = P(1+Q) = 0\) but someone arguing by denying the antecedent finds on the chalkboard that \(1+P + PQ = 1 + P(1+Q) \neq 1\), and then without any nudging from the instructor subtracts 1 from both sides to get \(P(1+Q) \neq 0\) violating Form A's principle of non-contradiction. We can't be in two places at the same time, at least in Newtonian space-time. But look at Form O, ``Some P is NOT-Q'' \(\, = P(1+Q) \neq 0\)," and we discover the reason why denying the antecedent is a fallacy: sometimes A's do not yield lollipops, in contradistinction to the premise of this whole argument. The only way the argument holds any water is to highly qualify the premise itself. But is that not, in philosophy class at least, putting ``des-Cartes before the horse,'' (all puns intended)?

Why belabor this point? First, in my casual sampling experience over 5 decades I find that my hypothesis that 9 out of 10 people with quite a bit of standardized and highly structured education believe that denying the antecedent is a valid piece of logic. Second, some of these folks, and nearly all who attend schools and colleges of business and engineering, have been drilled with Null Hypothesis Significance Testing as the only approach for practical statistical inference. Third, NHST uses a \emph{modus tollens} valid piece of argumentation, but in many instances, these same structurally educated folk, often deploy arguments which deny the antecedent.
\footnote{To get an idea of the order of magnitude of the potential usage of NHST, \href{https://www.aacsb.edu/insights/reports/2024/masters-enrollment-trends-at-aacsb-schools}{according to recent enrollment trends published by AACSB} about 1,000,000 students world-wide annually (on a very rough average over the decade spanning 2014-2024) and using \href{https://nces.ed.gov/pubs93/93442.pdf}{U.S. Department of Education data for business school graduates} with an estimate of 166,000 graduates in 1974, the 50 year linear average is about 600,000 graduates per year. Thus in a recent 10-year period approximately \((0.9)(0.8)(1,000,000)(10)=7,200,000\) students and in a 50 year span a very rough order of magnitude estimate of \((0.9)(0.8)(600,000)(50)=21,600,000\) students will have taken core curriculum statistics courses mostly (say 80\% to be conservative) based on the null hypothesis / frequentist paradigm with my very rough posterior rate of 90\% who identify the \emph{modus tollens deny the antecedent} fallacy as logically virtuous.}

In a regression of wages dependent on education, we suppose it is found that the impact of education is not consistent with being zero, somehow throughout most of the sample. We waive our rights to determine the thresholds inherent in the judgment of consistency, or even what a sample is and what \emph{most of} entails. Under the null hypothesis we suppose that impact is, for all intents and purposes, nil. Otherwise it is not. P: If nil, then Q: accept the null hypothesis of no impact. However, we find NOT-P: not nil. Therefore, NOT-Q: do not accept the null hypothesis as true. In fact, under null hypothesis lore, not to accept the null is to reject the null and accept the alternative, here that there is an impact (statistically significant, etc., usw, ktl, and so on). But, as we have already seen, this fallacious argument boils down to FORM O relief that ``Some P: impacts are Q: not significant (null hypothesis).'' \footnote{Recent work by \citet{nakhle4781598efficacy} in clinical medical settings exploit this idea. While both null hypothesis testing and Bayesian approaches yielded the same binary result, clinicians benefit from a knowledge of the ``strength of evidence'' available only in the Bayesian approach. \citet{mertens2023new} develop proposals for obviating known and emerging threats to interpretation of empirical research in information systems based on abuses of hypothesis testing approaches.}

There is a ``yay'' in the offing, namely, affirming the consequent, a fallacy of the logical abuse of \emph{modus ponens}. Here is the fallacy portion of our program for \emph{modus ponens}: Affirming the Consequent (Q: lollipop bribe).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We set out the structure. We let \(P:\) ``you get an A'' and \(Q:\) ``I'll give you a lollipop.'' With this assignment of propositions we state the affirmation of the consequent argument as \([\,Q \,\, \text{AND} \,\, (P \,\, \text{IMPLIES} \,\, Q)\,] \,\, \text{IMPLIES} \,\, P\).
\item
  we translate this structure into algebra. For any \(A\) and \(B\), \(A \,\, \text{AND} \,\, B = AB\), and \(A \,\, \text{IMPLIES} \,\, B = 1 + B + BA\). Also we know that \(A^2 = AA = A\) and \(A + A = 0\) in the mod 2 arithmetic of the algebra of Boole. This means that our argument can be represented algebraicly as this expression.
\item
  Then at last we reduce all expressions algebraicly. If we end up with a 1, then we have logical satisfaction.
\item
  We apply common sense and existential import.
\end{enumerate}

\[
\begin{array}{r l}
[Q \,\, \text{AND} \,\, (P \,\, \text{IMPLIES} \,\, Q)] \,\, \text{IMPLIES} \,\, P & \text{[modus ponens]}\\ \hline
Q(1+P+PQ) &\text{IMPLIES}\,\, P \\
Q + QP + PQQ &\text{IMPLIES}\,\, P \\
Q + PQ + PQ &\text{IMPLIES}\,\, P \\
Q + 0 &\text{IMPLIES}\,\, P \\ 
1 + Q + QP &\neq 1 \\
\end{array}
\]

This was a little easier to set up and subsequently also understand. We have one-way causality from getting an A to receiving a lollipop. That's the admissible rule we must abide by. In fact it is a constraint imposed on this argument, a requirement with which we must comply (at least here). The \emph{modus ponens} affirmation of the consequent is yet another example of a logical inversion. The reply to this fallacy is that the fallacy titrates to making the consequent the antecedent. I'm pretty sure that if I gave my son a lollipop he might, or he might not, get an A on his mixed fractions test in 4th grade. In fact after I gave him the lollipop he broke out in a fever (not from the lollipop), but from yet another classroom plague of viruses. So he did not get his A.

Our rampant enchantment with connectivity technology is this affirmation of the consequent inversion with this structure. We let \(P:\) ``humans'' and \(Q:\) ``technology'' With this assignment of propositions we state the affirmation of the consequent argument as \([\,Q \,\, \text{AND} \,\, (P \,\, \text{IMPLIES} \,\, Q)\,] \,\, \text{IMPLIES} \,\, P\). We have two subjects, humans and technology. The predicate which links these two subjects is the implicating action ``govern.'' Thus \(P\,\,IMPLIES \,\, Q =\,\) ``humans govern technology.'' The affirmation of the consequent ``technology'' results in the inversion \(Q\,\,IMPLIES \,\, P = \,\) ``technology governs humans.'' Perhaps \citet{capek1923rur} made a good point about the relationship between humans and their \emph{roboti}.

\hypertarget{on-the-horns}{%
\section{On the horns}\label{on-the-horns}}

That is, the \emph{horns of the dilemma}, is what we ride on. We have dilemmas when we have options which conflict with one another, and each option has its own pros and cons. The pros and cons might be shared across options, but not completely, or we have literally, no dilemma to annoy us. Here are three examples, all conflicts.

\begin{itemize}
\item
  The infamous \textbf{Trolley} problem, for example, testing perceptions of harm in \citet{cushman2006role}. We might imagine (that is, perform a thought experiment) whether to sacrifice one person to save a larger number.
\item
  A variant of the trolley is the \textbf{conflict of interest} dilemma. Here we imagine we might impose our personal beliefs on others, sacrificing their interests for ourselves.
\item
  Here's an \textbf{incentive conflict}. Conscientious workers will endeavor to meet their obligations without the threat of a loss in pay, and indifferent workers won't endeavor to meet their obligations study and learn even with the threat of an exam. So, incentives serve no purpose. The reasoning might be valid, but inconsistent with considering the existential import of what is a good versus a bad worker.
\item
  \textbf{Option overload or aversion?} In this dilemma when presented with only a single option, really no option except the status quo, people may be unlikely to buy, even if it's an option they like, thus aversion. On the other hand, given too many, and possibly the double-barrel of too complex, options, can lead to \emph{analysis paralysis}, no choice, and reliance, again, on the status quo.
\end{itemize}

All of the work we have done to help us relate various propositions, even test their validity and coherence, pull together the essential components of \emph{dialectical reasoning}. In what follows we will build \emph{constructive} (negative propositions not allowed), and \emph{destructive}, (many, even double negatives abound to confuse coherence). These arguments will be \emph{simple} (one consequent allowed) or (exclusive or) \emph{complex} (more than one consequent allowed). We will construct a propositional logic table first, then translate to our algebraic formulation.

\hypertarget{dialectical-algebra}{%
\subsection{Dialectical Algebra}\label{dialectical-algebra}}

Using the tools of our propositional logical algebra we can cast the logical space of dialectic like so. First, let's use the standard symbols for those who like to begin from this point of view.

\[
\begin{array}{|c | c | c |} \hline
\text{[propositional logic]}  & simple     & complex    \\ \hline
\text{both true}      & (p \rightarrow q) \land (r \rightarrow q) & (p \rightarrow q) \land (r \rightarrow s) \\
\text{yet, the horns} & p \lor r & p \lor r \\ \hline
constructive          & \therefore q & \therefore q \lor s \\ \hline
\text{both true}      & (p \rightarrow q) \land (r \rightarrow q) & (p \rightarrow q) \land (r \rightarrow s) \\
\text{yet, the horns} & \neg q \lor \neg r & \neg q \lor \neg s \\ \hline
destructive  & \therefore \neg p & \therefore \neg p \lor \neg r \\ \hline
\end{array}
\]
Next, we cast the logical symbology into something our computer and our ninth-grade algebra student might recognize.

\[
\begin{array}{|c | c | c |} \hline
\text{[algebraic logic]}  & simple     & complex    \\ \hline
\text{both true}      & (1 + p + pq)(1+r+rq) & (1 + p + pq)(1 + r + rs) \\
\text{yet, the horns} & p + r & p + r \\ \hline
constructive          & \therefore q & \therefore q + s \\ \hline
\text{both true}      & (1 + p + pq)(1 + r + rq) & (1 + p + pq)(1 + r + rs) \\
\text{yet, the horns} & (1 + q) + (1 + r) & (1 + q) + (1 + s) \\ \hline
destructive  & \therefore 1 + p & \therefore (1 + p) + (1 + r) \\ \hline
\end{array}
\]

To illustrate our analysis of the logic, and import, of the \emph{simple-constructive} argument, here is an example. Many of us might recognize this argument against skepticism.\footnote{Plato's \emph{Gorgias} displays an extreme skepticism is really nihilism, It works in three moves: I can't really know anything; and if I did I would not know if it is right or wrong; and if I was not ignorant of the value of knowing, I would not be able to share it anyway -- thus, I can say or do anything. The argument in our dialectial reasoning example is squarely the opposite of the skeptical triple play. Is this not some version of ``I think, therefore I am''? If you answer yes, then you might know of Rene DesCartes.}

\begin{itemize}
\item
  If \emph{p}: ``I know anything at all with certainty,'' then \emph{q}: ``I certainly exist.'' \emph{AND}
\item
  If \emph{r}: ``I am ignorant of any certainty,'' then \emph{q}: ``I certainly exist.''
\item
  \emph{BUT} ``Either \emph{p}: I know anything at all with certainty,'' \emph{OR} " \emph{r}: I am ignorant of any certainty."
\item
  {[}\emph{THEREFORE} \emph{q}: ``I certainly exist.''{]}
\end{itemize}

We know we can translate these propositions, this argument, into our algebraic solution structure just like this.

\[
\begin{array}{r l}
\,(p \,\, \text{IMPLIES} \,\, q) \,\, \text{AND} \,\, (r \,\, \text{IMPLIES} \,\, q)  & \text{[simple-constructive]}\\
\text{AND}\,\, (p \,\,\text{OR}\,\, r) & \text{IMPLIES}\,\, q \\ \hline
(1+p+pq)(1+r+rq)(p+r) &\text{IMPLIES}\,\, q \\
(1+r+rq+p+pr+prq+pq+pqr+pqqr)(p+r)&\text{IMPLIES}\,\, q \\
p+r+rp+rr+prq+rrq+pp+pr+ppr+prr+pprq+prrq &\text{IMPLIES}\,\, q \\
pq+rq &\text{IMPLIES}\,\, q \\ 
1+(pq+rq)+(pq+rq)q & = 1+pq+rq+pqq+rqq \\
&= 1 + 0 + 0 \\
&= 1
\end{array}
\]
Well worth the effort. All of the top-side propositions: both the two implications (conditionals) and the disjunction boil down to \((p+r)q = (p\lor r)\land q\). All of that rhetoric, perhaps necessary in a verbal presentation, can be interpreting as this words and phrases (and remembering we are using the exclusive either, or, not both): ``It is true that both I certainly exist, and, either I know anything at all with certainty, or I do not.'' The two either-or conditions are linked both-and to the one conclusion.

Can we refute this argument given its structure? We always have the maintained \emph{hypothesis}: ``if we believe this structure to be the horns of a dilemma, then we maintain this belief in all we do next with this structure.'' There are only, here, two routes to refutation: destroy the either-or conditions, or deny the both-and linkage. Let's take the denial first. To deny the both-and linkage we would have to establish that any and all knowledge are incompatible with existence. And to do that we would have to deny the existence of a knower who can know either with or without certainty. And this is absurdly against common sense existential import. Someone is typing here! Now how can we possibly destroy the conditionals, the either ``with'' or ``without'' certitude. If we believe that nothing is certain, then the either-or statement is still true; similarly is at least one thing is certain, such as \$1+1=0 \$ in our binary arithmetic, and on our binary clock too. Thus refutation seems absurd again. We may indeed have arrived at a logical immutable truth! This is called a \emph{tautology}, a meaning that is true in its structural self (in Greek, \emph{auton}). The only way we can then refute this tautology is to present alternative structure.

\hypertarget{ride-with-socrates}{%
\subsection{Ride with Socrates}\label{ride-with-socrates}}

We have three strategies with every dilemma: ride the horns (and risk getting gored), go between the horns (and risk getting thrown), and make another bovine to ride new horns (risk yet more horns, goring, and being thrown). On the upside, we have used logic to discover the logical consequences of our thinking. We will call all of this the intellectual process of \emph{reasoning} goaded on by our incessant, undying (always a spark exists) drive to know all that there is (Being, that is), and we will maintain the practical circumstance of limited time and space, so that it is process, not perfection. We will use our process of reasoning to judge whether something is or is not. And we recall that probability, or any other qualifier such as, might, could, should, would, and so on, can be part of any proposition.

We can suppose, for a minute or so, that there are three propositions A, B, and C. They have context in particular situations, for example, in decision making. In this context, we might suppose that A is the proposition ``developers make reasonably successful decisions''. We also suppose that B is the proposition ``product management makes reasonably successful decisions.'' And now we put proposition C into context as ``Teams use data and experience consistent with prevailing recommended practices''. Other than observations made by teams ad something that neither the development manager nor the product management manager might yet any specific idea about, but does have at the very least an intuitive feel about something which has yet to be revealed. Of course, we need at some point to define a bit more precisely what is meant by ``reasonably successful'' and ``consistent with prevailing recommended practices'' all for another time and deep conversation.

Let's put these pieces together and use the apparatus of our logical algebra to discover literally a new insight. To couch our analysis in terms of what we need, or wish for, in our analytical logic robot we require instructions for the robot must hold to these desiderata.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Plausibilities can be ordered as rational numbers. The so called irrational numbers such as the solution \(x\) to the equation \(y=x^2\), can be approximated to any degree of accuracy the robot can determine within the finite bounds of the robot's memory, ability to process, and share with those who use the robot as a tool of discovery.
\item
  Whatever the robot helps us to infer must be subject to common sense in the real world of life and persons who use the robot as a tool.
\item
  Whatever unobserved hypothesis which the robot helps us find is more plausible than another must be consistent with observed data.
\end{enumerate}

Now we can get to work.We ask ``Is A and B true?'' As managers of development and products we surely expect such a result. As usual there are two schools of thought on the matter.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  {[}Case 1{]} B is true given C, that is, the plausibility B\textbar C. Having decided that B is true given C, decide that A is true, that is, the plausibility A\textbar BC.
\item
  {[}Case 2{]} A is true given C, that is, the plausibility A\textbar C. Having decided that A is true given C, we, decide that B is true, that is, the plausibility B\textbar AC.
\end{enumerate}

We will show that Case 1 is equivalent to Case 2 logically even though they each represent two seemingly different perspectives. Case 1 is the perspective of product managers who launch products for a living, while Case 2 is the perspective of developers who, well, develop products for a living. Those of use you attempt to manage developers and product managers would very much like to know that AB is true.

Let's compute, with our robot's mechanisms, the joint proposition ``A\textbar BC and B\textbar C''. With our algebra of logic we have \(x= A|BC\) and \(y=B|C\) so that both-and amounts to multiplying \(xy=(A|BC)(B|C)\). We also know through study of logical givenness, also known as implication, that for any propositions a and b \(b|a = 1+a+ab\). Now we can calculate.

For Case 1 we have this calculation.

\[
\begin{array}{c | r r | l}
\hline
1 &        & 1+BC+ABC                                & \text{definition of } A|BC \\
2 & \times & 1+\,C+\,BC                              & \text{definition of } B|C \\ 
\hline
3 &        & BC +BC + ABC            & \text{since } X^2=X \\
4 &    +   & C+BC+ABC               & \text{ditto }   \\
5 &    +   & 1+BC+ABC                & \text{ditto }   \\ 
\hline
6 &    =   & 1+C+ABC                 & \text{since } X+X=0 \\ 
\hline 
7 & \therefore & AB|C = (A|BC)(B|C)  & \text{definition of } 1+C+ABC \\
\hline
\end{array}
\]

For Case 2 we have this calculation.

\[
\begin{array}{c | r r | l}
\hline
1 &        & 1+AC+ABC                                & \text{definition of } B|AC \\
2 & \times & 1+\,C+\,AC                              & \text{definition of } A|C \\ \hline
3 &        & AC+AC+ABC    & \text{since } X^2=X \\
4 &    +   & C+AC+ABC              & \text{ditto }   \\
5 &    +   & 1+AC+ABC                       & \text{ditto }   \\ \hline
6 &    =   & 1+C+ABC                                 & \text{since } X+X=0 \\ \hline 
7 & \therefore & BA|C = (B|AC)(A|C)                  & \text{definition of } 1+C+ABC \\ \hline
\end{array}
\]

Since BA=AB by the commutativity of the both-and operation we now also have shown that Case 1 is logically equivalent to Case 2. Here is the new knowledge, an insight, from all of these deductions.

\[
\begin{array}{c | r c | l}
\hline
1 &                &  AB|C = (A|BC)(B|C)         & \text{ Case 1 is true } \\
2 & \text{and}     &  BA|C = (B|AC)(A|C)         & \text{ Case 2 is true } \\
3 & \text{however} &  AB=BA                      & \text{commutativity of } AB \\
4 & \text{so that} &  AB|C = BA|C                & \text{substitution of equals} \\    
\hline
5 & \therefore     &  (A|BC)(B|C) = (B|AC)(A|C)  & \text{transitivity of equality}\\
  &                &                             & \text{if a=c, and b=d, and a=b, then c=d} \\
\hline
\end{array}
\]

We have just also unveiled the vaunted product rule of probability. We have but one more step to apply this rule to the work of our managers, namely, the making of reasonable decisions, where reasonable at the very least means logically valid deductions about the consistency of data with hypotheses. Here a decision will take the form of a hypothesis, which, if true, will indicate a direction in which managers might reasonably act.

\hypertarget{torturing-data-with-hypotheses}{%
\section{Torturing Data with Hypotheses}\label{torturing-data-with-hypotheses}}

So line 5 of our conditional event proof is true. By desiderata 1, namely, that plausibilities can be ordered as rational numbers, we assign numbers to logical expressions which in turn represent events. For comparison purposes across wildly disparate events, plausibility scores will lie between 0 and 1 such that, for a mapping \(P(x)\) of plausibility of the truth of a logical expression \(x\) to the interval of rational numbers (and approximations of so-called irrational numbers) between 0 and 1 inclusive, we have for rational number \(0< r \leq 1\)

\[
P(x) = \begin{cases}
            0, & \text{if } x \text{ is false}\\
      r, & \text{if } x \text{ is true}
         \end{cases}
\]

Yes, we are optimistic, looking for the plausible truth value of a logical proposition.\footnote{We could just as easily have picked \(0 < r \leq 100\). As long as there is a strict ordering of pairs of numbers in the interval, what the mathematicians call a monotonic sequence, then we can generally use whatever interval we want.} Now it gets very serious since we now examine what exactly we will mean by reasonable, also known as rational.

We let \(H=A\) for our ordered list of hypotheses \(H\), \(D=B\) for our list of data \(D\), \(X=C\) for our list of instances of prior experience, previous choices and data \(X\). With these substitutions we now have this expression.

\[
(H|DX)(D|X) = (D|HX)(H|X)
\]
Since the logical expressions in the parentheses are mapped to a plausibility index \(P(x)\), we can use the normal rules of arithmetic and algebraic to divide both sides by \((D|X)\).

\[
\frac{(H|DX)(D|X)}{(D|X)} = \frac{(D|HX)(H|X)}{(D|X)}
\]

Since \((D|X)/(D|X) =1\) and anything times 1 is itself we arrive at our signal result with new notation \((x) = P(x)\).

\[
P(H|DX) = \frac{P(D|HX)P(H|X)}{P(D|X)}
\]

We can even calculate \((D|X)\) using some common sense and our logical algebra. First of all lets simple consider that X is true and calculate \(P(D)\) by itself. Then we consider that \((D)\) will be a mixture of \((D|H)\) and \((D|\overline{H})\)to account for all of the way D might occur. Thus

\[
P(D) = P(D|H)P(H) + P(D|\overline{H})P(\overline{H}) 
\]

The algebra works out to simplifying the RHS of this expression all by remembering that 1+1=0 and \(a+b=0\) in mod 2 arithmetic and algebra. We just need to show that the RHS = LHS, other words, one side implies the other.

\[
\begin{array}{r l}
D &= (1+H+DH)H + (1+(1+H)+(1+H)D)(1+H) \\
  &= H+H+DH+1+1+H+D+DH+H+H+H+DH+DH \\
  &= D
\end{array}
\]

We have a match. We have only now logically justified Bayes Rule. It would appear deductions can provide us with new knowledge and insights into our decisions and the data which consistently will help us guide our actions. We also have an alternative to the potentially misleading use of Null Hypothesis Significance testing.

\hypertarget{ever-satisfied}{%
\section{Ever satisfied?}\label{ever-satisfied}}

We can use Gaussian elimination manipulations of matrix rows to deduce what an invertible logical expression implies. This is a process called logical satisfaction.

\begin{verbatim}
Given formula logical expression

(a + c + d)(b + (1+c) + d)(a + b + (1+d))(a + (1+b) + (1+c))

("1" means TRUE, "0" means FALSE)
Each clause leads to one equation.
a   +   c   +   d   = 1
b   +   (1+c)   +   d   = 1
a   +   b    + (1+d )= 1
a   + (1+b) + (1+c) = 1

This next equation would contradict the last equation above.

(1+a)   +   b    + c = 1 [Contradictory Clause]

Since 1+1 = 1-1 = 0 for the Boolean ring, we have

a   +   c   +   d   = 1
b   +   c   +   d   = 0
a   +   b   + d = 0
a   + b + c = 1

a   b   c   d       line
 
1   0   1   1   1   A
0   1   1   1   0   B
1   1   0   1   0   C
1   1   1   0   1   D

a   b   c   d       operation
 
1   0   1   1   1   A
1   1   0   1   0   C
1   1   1   0   1   D
0   1   1   1   0   B (swapped)
 
1   0   1   1   1   A
0   1   1   0   1   E = C+A
0   1   0   1   0   F = D+A
0   1   1   1   0   B
 
1   0   1   1   1   A
0   1   1   0   1   E
0   0   1   1   1   G = F+E
0   0   0   1   1   H = B+E

a   b   c   d       operation
 
1   0   1   0   0   I = A+H
0   1   1   0   1   E
0   0   1   0   0   J = G+H
0   0   0   1   1   H
 
1   0   0   0   0   K = I+J
0   1   0   0   1   L = E+J
0   0   1   0   0   J
0   0   0   1   1   H

Solution:

If the Contradictory Clause is present: Unsolvable

Else:   

a = 0 = FALSE
b = 1 = TRUE
c = 0 = FALSE
d = 1 = TRUE
As a consequence:

R(a,c,d)R(b,(1+c),d)R(a,b,(1+d)R(a,(1+b),(1+c))R((1+c)a,b,c)
is not 1-in-3-satisfiable,

while (a + c + d)(b + (1+c) + d)(a + b + (1+d))(a + (1+b) + (1+c))
is 3-satisfiable with a=c=FALSE and b=d=TRUE.
\end{verbatim}

\hypertarget{how-many-ways-1}{%
\section{How many ways?}\label{how-many-ways-1}}

We began this whole discussion with the two schools of thought proposed by \citet{cox1946probability}. Here we implement the school of thought which states that inference is all about the expectation of an outcome. We condition the expectation with observed data and (un)observed hypotheses relevant to the question which gives rise to the inference. We then implement the computation of an expectation by simply counting the ways in which the expectation (data and hypotheses conditions) can occur. We effectively answer a question for reflection, of inference,that is, the question ``whether it is?'', with the frequency (the count) of the number of ways data are consistent with hypotheses. In this way we release ourselves both from the tyrannies of data alone and hypotheses alone. In a very realistic sense we have bridged the polar gaps, pun intended, of pure empiricism ( \emph{sola data} ) and pure idealism ( \emph{sola hypotheses} ).

Let's use a simple example. We have four voters in an upcoming election. They may be red or blue voters. Three of us go out and talk to three voters at random, that is, indiscriminately. One of us happens to come upon a blue voter, another of us, independently, happens to find a red voter, and the other separately finds a blue voter. This is the very definition of a random sample. Each of the finders does not know what the other is doing, all three do know that there are four voters out there and they happened to have independently talked to two blue and one red voter. How many red voters and how many blue voters are there?

Here are all of the possible conjectures we can make for \(blue = {\color{blue}\Box}\) and \(red = {\color{red}\Box}\) voters.

\[
\begin{array}{c | c c c c }
hypotheses & 1 & 2 & 3 & 4 \\ \hline
1 & {\color{blue}\Box} & {\color{blue}\Box} & {\color{blue}\Box} & {\color{blue}\Box} \\
2 & {\color{red}\Box} & {\color{blue}\Box} & {\color{blue}\Box} & {\color{blue}\Box} \\
3 & {\color{red}\Box} & {\color{red}\Box} & {\color{blue}\Box} & {\color{blue}\Box} \\
4 & {\color{red}\Box} & {\color{red}\Box} & {\color{red}\Box} & {\color{blue}\Box} \\
5 &{\color{red}\Box} & {\color{red}\Box} & {\color{red}\Box} & {\color{red}\Box}
\end{array}
\]

Reading this we see that there are 4 voters, which we can call ways the voters are arranged, and 5 different voter compositions ranging from all red to all blue, each of which we call a hypothesis. Our sample is 2 blue voters and 1 red voter, so we can very safely eliminate the first and fifth conjectures from our analysis, but for the moment just keep them for completeness sake.

For each of the three remaining conjectures we may ask how many ways is the conjecture \emph{consistent} with the collected data. For this task a tree is very helpful. Let's take the first realistic conjecture the \({\color{blue}\Box}\), \({\color{red}\Box}\), \({\color{red}\Box}\), \({\color{red}\Box}\) hypothesis and check if, when we sample all of the four voters, what are all of the ways this conjecture fans out. So here we go.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We sampled a \({\color{blue}\Box}\) first. How many \({\color{blue}\Box}\)'s are in this version of the composition of voters? only 1.
\item
  We then sampled independently a \({\color{red}\Box}\). How many \({\color{red}\Box}\)'s are in this conjecture? Quite a few, 3.
\item
  Finally we sampled a \({\color{blue}\Box}\) at random. We know there is only one \({\color{blue}\Box}\) in this version of the truth.
\end{enumerate}

So, it is just counting the ways: 1 \({\color{blue}\Box}\) way x 3 \({\color{red}\Box}\) ways x 1 \({\color{blue}\Box}\) way = \(1 \times 3 \times 1 = 3\) ways altogether.

When asked, many surmise that the 2 blue and 3 red conjecture is the right one. Are they right? Here is a table of the ways each conjecture pans out. We then in a separate column compute the contribution of each conjecture to the total number of ways across the conjectures, which is 3 + 8 + 9 = 20 ways. Also each of the conjecture propose a proportion \(p\) of the successes, that is, the blue voters in this context.

\[
\begin{array}{c | c c c c | c | c }
\hline
\text{h} & 1 & 2 & 3 & 4 & \text{ways: }2 \, {\color{blue}\Box}\, 1 \, {\color{red}\Box} & \text{relative ways} \\ \hline
1 & {\color{blue}\Box} & {\color{blue}\Box} & {\color{blue}\Box} & {\color{blue}\Box} & 0 \times 4 \times 0 = 0 & 0.00 \\
2 & {\color{red}\Box} & {\color{blue}\Box} & {\color{blue}\Box} & {\color{blue}\Box}  & 1 \times 3 \times 1 = 3 & 0.15 \\
3 & {\color{red}\Box} & {\color{red}\Box} & {\color{blue}\Box} & {\color{blue}\Box}   & 2 \times 2 \times 2 = 8 & 0.40 \\
4 & {\color{red}\Box} & {\color{red}\Box} & {\color{red}\Box} & {\color{blue}\Box}    & 3 \times 1 \times 3 = 9 & 0.45 \\
5 &{\color{red}\Box} & {\color{red}\Box} & {\color{red}\Box} & {\color{red}\Box}      & 0 \times 4 \times 0 = 0 & 0.00 \\
\hline
\end{array}
\]

We cannot help but note that the proportion of ways for each conjecture can range from 0, perhaps to 1, since the proportions add up to 1. The number of ways also expresses the number of true consistencies of the data with the conjecture, an enumeration of the quality of the logical compatibility of conjectures with what we observe.

We might now revise our common sensical surmise that 2 blue and 2 red is the better conjecture. However, if we use the criterion that the conjecture with the most ways consistent with the data is the best choice for a conjecture, then clearly here we would say that there are 3 blues and 1 red. Perhaps we have a better criterion that would choose our equinanimous choice of 2 blues and 2 reds? It does not appear to be so.

Ways are the count, the frequencies of logical occurrence of a hypothesis \emph{given} the data. The data includes the knowledge that there are possibly blues and reds, that there are 4 voters, and that we sampled 2 blues and 1 red. The relative frequency of the ways in which our conjectures are consistent with the data is what we will finally call \emph{probability}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The plausibility is a measure between and including 0 and 1.
\item
  The sum of all plausibilities is 1.
\item
  Common sense, again, rules the day, and the plausibility.
\end{enumerate}

We have just quantified the \emph{plausibility} of logical truth values. We have also found a very compelling criterion for the choice of a conjecture given the data and circumstances surrounding our inquiry.

\hypertarget{how-informative}{%
\section{How informative?}\label{how-informative}}

Now the piece de resistance we have been waiting for is at hand. We have the tools: information measurement, logic of plausible inference, probability as plausibility, a criterion for making a decision? If we were to say, pick the most plausible, now also the most probable inference, is this not really to say that it is the most informative and thus the most surprising inference?

We have \(i=5\) conjectures \(c_i\), each with an assignment of the probability of finding the inference in data, thus 5 probabilities \(p_i\). We also agree that each conjecture \(c_i\) is independent of the other, that is mutually exclusive, and also mutually exhaustive. In this way the probabilities indeed will add up to 1.

Each conjecture has \(N=4\) symbols in it. There are only 2 symbols to work with, \(B=blue\) and \(R=red\) in these messages. There are \(N!=4 x 3 x 2 x 1 = 24\) ways to arrange symbols. But these are constrained by two symbols in this pile so we have on average this many combinations:

\[
\frac{(N + s - 1)!}{N!(s-1)!} = \frac{5!}{4!1!} = \frac{120}{24}=5\,\,conjectures
\]

In the numerator are the four voters, two kinds of voters, less one. This gives us \(5!=120\) total ways to arrange this set of conjectures. But there are \(4!(2-1)!=\) ways to pick 2 kinds of voters from a population of 4 voters. That comes from the notion that there are \(4!\) ways to choose 4 voters and having chosen the first voter from the voter urn, there are \((2-1)!\) more ways to choose voters, either blue or red, each and every time we randomly go out to the voting population, all with replacement. Phew!

Now that we have our sea legs in combinatorics, each conjecture of length \(N=4\) has two symbols \(s_1=blue\) and \(s_2=red\) with probabilities of occurrence in the message \(p_1=p(blue)\) and \(p_2=p(red)\) here \(0.50\) each. The portion of the messages that are blue are \(Np_1=4 \times 0.5=2\) and red are \(Np_2=4 \times 0.5=2\) as well. The ratio

\[
\frac{N!}{(Np_1)!(Np_2)!} = \frac{4!}{2!2!}=\frac{24}{(2)(2)}=6
\]

in turn gives us the number of combinations of symbols and messages. This is the average number of ways a message can be formed. The expected amount of information in these messages, as conjectures, is

\[
<I> = log_2\left(\frac{N!}{(Np_1)!(Np_2)!}\right)
\]

For large enough \(N\) we can use Stirling's Approximation\footnote{Named after James Stirling, this approximation was first stated by Abraham de Moivre without the use of \(\pi\) as \(ln(n!) = n\,\,ln (n) - n + error\), very accurate even for small \(n\).}

\[
<I> = N \sum_{i=1}^2 (-p_i\,log_2 \,p_i)
\]

So the amount of informativeness in each of the voter conjectures is

\[
<I> = 4 \sum_{i=1}^2 (-0.5 \,log_2 \,0.5) = 4
\]

This is intuitively correct: 4 bits of information in a population of 4 with 2 equally likely choices.

What about the 5 conjecture? Now \(N=5\) and we only need \(p_2=0.15,\, p_3=0.40,\, p_4=0.45\) since the other probabilities are 0.

\[
<I> = 4 \sum_{i=1}^3 (-p_i \,log_2 \,p_i) = 4(1.5)=5.8
\]

This is nearly 2 bits more informative than any single conjecture. The best conjecture contributes 2.1 bits of information on its own or 36\% of the informativeness of the total distribution of conjectures we reviewed.

\hypertarget{further-thoughts}{%
\section{Further thoughts}\label{further-thoughts}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The location and scale parameters \(\mu\) and \(\sigma\) we often calculate are not the mean and standard deviation of data \(x\) in the expectations school of thought. They are not properties of the physical representation of events called data. These parameters do carry information about the probability distribution of the representation of physical reality in data. To say \(\mu\) is the mean of the data is to ascribe a mind's eye idea to the physical reality, to invest in physical, objective reality, a property that only exists in the mind. This is an example of the \emph{mind projection} or \emph{reification} fallacy much in vogue in the circles of \emph{fake}, or better yet, \emph{chop logic}. In the same way, probabilities only exist in our minds: there is no physical reality that is a probability, just a mental construct that helps us think through potential outcomes abstracting similar and dissimilar properties as we observe from data.
\item
  When we say \emph{A implies B} we mean that \(A\) and \(AB\) have the same truth value. In logic every true statement \emph{implies} every other true statement. Just knowing that \(A\) and \(B\) are true does not imply a relationship exists between A and B, except that the propositions are true.
\end{enumerate}

\hypertarget{in-the-end}{%
\section{In the end}\label{in-the-end}}

We can now study any arrangement of events in logical order and map them to plausibility values. The mapping, the logical order, the propositional space we have constructed from first principles all ground our notion and implementation of forming consistent, relevant, and necessary consistencies between what we expect and what we observe. Along the way we discovered a method, perhaps we can call this a technique, to satisfy our desire to determine the existence, and degree, of truthfulness of any set of propositions, encapsulating terms, and manifesting in an argument. We pulled together basic inferential structures and applied these to a first look at hypothesis testing, dialectical reasoning, and Bayesian inference. We need always remember that this approach requires at the outset a contextual matrix against which we analysts can interpret our results, examine their relevance, and inform the consumers of our analysis.

\hypertarget{references}{%
\section{References}\label{references}}

\hypertarget{language-and-food}{%
\chapter{Language and Food}\label{language-and-food}}

Daniel Nettle is a behavioral scientist who specializes in linguistic aspects of human development. His 1998 paper on language diversity, \href{https://www.danielnettle.org.uk/download/009.pdf}{which we can access here} poses this question.

\begin{quote}
This paper, then, asks the question of, what, in general, determines the size of language communities found in a human population.
\end{quote}

Language is a mask for regional integration or disintegration of societies. Powerful forces can unite, or divide, whole groups of people, often identifiable by the language of their region of origin. Food security is one of several fundamental sources of movements of people from one region to another. Ecological risk, as measured by the length of the mean growing season in a region, correlates with food security. Prominently for the time, Nettle published his paper with the original data he used to conduct the analysis.

The values in \texttt{data(nettle)} are data on language diversity in 74 nations. The meaning of each column is given below.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  country: Name of the country
\item
  num.lang: Number of recognized languages spoken
\item
  area: Area in square kilometers
\item
  k.pop: Population, in thousands
\item
  num.stations: Number of weather stations that provided data for the next two columns
\item
  mean.growing.season: Average length of growing season, in months
\item
  sd.growing.season: Standard deviation of length of growing season, in months
\end{enumerate}

Use these data to evaluate the hypothesis that language diversity is partly a product of food security. The notion is that, in productive ecologies, people don't need large social networks to buffer them against a risk of food shortfalls. This means cultural groups can be smaller and more self-sufficient,leading to more languages per capita. In this way we use the number of languages per capita as the outcome.

Use the logarithm of this new variable as your regression outcome. We can continue this model using counts and the enhanced Poisson distribution. For now we use the similarly convergent Gaussian distribution.

We will evaluate the effects of both \texttt{mean.growing.season} and \texttt{sd.growing.season}, as well as their two-way interaction.

Here are three parts to help.

\begin{itemize}
\item
  First, we examine the hypothesis that language diversity, as measured by \texttt{log(lang.per.cap)}, is positively associated with the average length of the growing season, \texttt{mean.growing.season}. Consider \texttt{log(area)} in the models(s) as a covariate (not an interaction).
\item
  Next we consider the hypothesis that language diversity is negatively associated with the standard deviation of length of growing season, \texttt{sd.growing.season}. This hypothesis follows from uncertainty in harvest favoring social insurance through larger social networks and therefore fewer languages. Again, consider \texttt{log(area)} as a covariate (not an interaction). Interpret your results.
\item
  Finally, we assess the hypothesis that \texttt{mean.growing.season} and
  \texttt{sd.growing.season} interact to synergistically reduce language diversity. The notion is that, in nations with longer average growing seasons, high variance makes storage and redistribution even more important than it would be otherwise. That way, people can cooperate to preserve and protect harvest windfalls to be used during the droughts.
\end{itemize}

\hypertarget{a-first-model-set-1}{%
\subsection{A first model set}\label{a-first-model-set-1}}

We fit three models to begin to analyze the hypothesis

\begin{quote}
Language diversity, as measured by \texttt{log(lang.per.cap)}, is positively associated with the average length of the growing season, \texttt{mean.growing.season}.
\end{quote}

The models:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We attempt to predict \texttt{log\_lang\_per\_capita} (\(L\)) naively with a constant and no other predictors.
\item
  We build on the naive model with only mean growing season (\(M\)) as a predictor.
\item
  We can expand on this model by including \texttt{log(area)} (\(A\)) to mediate growing season or to independently influence language diversity.
\end{enumerate}

The mediation model has this formulation.

\[
\begin{align}
L & \sim \operatorname{Normal}(\mu_L, \sigma_L) \\
\mu_L & = \alpha_L + \beta_{LM} M + \beta_{LA} A\\
\alpha_L & \sim \operatorname{Normal}(median(L),2) \\
\beta_{LM} & \sim \operatorname{Normal}(0,2) \\
\beta_{LA} & \sim \operatorname{Normal}(0,2) \\
\sigma_L & \sim \operatorname{Exponential}(1) \\
M & \sim \operatorname{Normal}(\mu_M, \sigma_M) \\
\mu_M & = \alpha_M + \beta_{M} A\\
\alpha_M & \sim \operatorname{Normal}(median(M),2) \\
\beta_M & \sim \operatorname{Normal}(0,2) \\
\sigma_M & \sim \operatorname{Exponential}(1) \\
\end{align}
\]

In this model we literally run a regression (\(\mu_M = \alpha_M + \beta_{M} A\)) to help with an other regression (\(\mu_L = \alpha_L + \beta_{LM} M + \beta_{LA} A\)) where \(\mu_M, \sigma_M\) mediate movements in the main attraction \(\mu_L, \sigma_L\).

All of this will deserve a comparison. We will use WAIC. A lower WAIC in one model indicates a lower loss of information measured as deviancy relative to other models. WAIC, similar to the likelihood ratio, the F-test, and even \(R^2\), is a predictive statistical inference of a sort. Adding more variates increases the complexity of the models and likely will decrease information loss.

A second model set would involve the standard deviation of the growing season. A third model set would divide the world into meaningful regions to examine the differences and similarities of language diversity globally.

We begin with the data.

\begin{verbatim}
##        country      num.lang        area             k.pop       
##  Algeria   : 1   Min.   :  1   Min.   :  12189   Min.   :   102  
##  Angola    : 1   1st Qu.: 17   1st Qu.: 167708   1st Qu.:  3829  
##  Australia : 1   Median : 40   Median : 434796   Median :  9487  
##  Bangladesh: 1   Mean   : 90   Mean   : 880698   Mean   : 33574  
##  Benin     : 1   3rd Qu.: 94   3rd Qu.:1080316   3rd Qu.: 24744  
##  Bolivia   : 1   Max.   :862   Max.   :8511965   Max.   :849638  
##  (Other)   :68                                                   
##   num.stations   mean.growing.season sd.growing.season
##  Min.   :  1.0   Min.   : 0.00       Min.   :0.00     
##  1st Qu.: 10.0   1st Qu.: 5.35       1st Qu.:0.94     
##  Median : 20.5   Median : 7.36       Median :1.69     
##  Mean   : 37.9   Mean   : 7.04       Mean   :1.70     
##  3rd Qu.: 44.8   3rd Qu.: 9.28       3rd Qu.:2.11     
##  Max.   :272.0   Max.   :12.00       Max.   :5.87     
## 
\end{verbatim}

We note:

\begin{itemize}
\item
  There are 68 countries represented.
\item
  The sample exhibits widely varying language diversity, area, and population.
\item
  We note the extreme distances between the 75th quantiles and the maximum. Scaling will be important in the regularization of the model.
\end{itemize}

\hypertarget{models-1}{%
\subsection{Models}\label{models-1}}

\begin{verbatim}
##        mean    sd  5.5% 94.5%
## aL    -5.32 0.175 -5.60 -5.04
## sigma  1.50 0.123  1.31  1.70
\end{verbatim}

\begin{verbatim}
##         mean     sd    5.5%  94.5%
## aL    -6.558 0.3859 -7.1747 -5.941
## bLM    0.159 0.0503  0.0786  0.239
## sigma  1.397 0.1134  1.2159  1.578
\end{verbatim}

\begin{verbatim}
##         mean     sd    5.5%   94.5%
## aL    -4.623 1.1734 -6.4982 -2.7476
## bLM    0.153 0.0498  0.0738  0.2330
## bLA   -0.148 0.0849 -0.2840 -0.0127
## sigma  1.379 0.1120  1.1998  1.5577
\end{verbatim}

\begin{verbatim}
##          mean     sd    5.5%   94.5%
## aL     -4.620 1.1731 -6.4948 -2.7450
## bLM     0.153 0.0498  0.0738  0.2328
## bLA    -0.148 0.0849 -0.2841 -0.0129
## sigma   1.378 0.1118  1.1993  1.5567
## aM      9.943 1.7148  7.2026 12.6838
## bM     -0.230 0.1336 -0.4438 -0.0166
## sigmaM  2.969 0.2414  2.5833  3.3550
\end{verbatim}

\begin{verbatim}
##    WAIC   SE dWAIC  dSE pWAIC  weight
## m2  267 15.9 0.000   NA  4.35 0.39754
## m3  267 16.1 0.132 0.27  4.47 0.37207
## m1  268 15.9 1.117 2.93  3.86 0.22745
## m0  277 17.1 9.811 7.24  2.96 0.00294
\end{verbatim}

\includegraphics{_main_files/figure-latex/unnamed-chunk-180-1.pdf}

More covariates usually results in less deviancy and more predictivity. But very much more?

\begin{itemize}
\item
  The naive model, \texttt{m0}, is least predictive.
\item
  The two complicated models, \texttt{m2} and \texttt{m3}, are plausibly tied for second place.
\item
  The mid-complex model, \texttt{m1}, is the least deviant from an information loss point of view.
\end{itemize}

Here we visualize the predictions with WAIC's cousin PSIS. Both use the Bayesian predictive likelihood. PSIS here will also provide cross-validation with the leave-one-out procedure. The horizontal axis measures the degree of unpredictability using the Pareto power \(k\) parameter. The vertical axis measures the volatility of observations using the penalty component of the PSIS calculation.

\emph{Commentary}

(Note structural and statistical differences from the other models.)

Model: m1

\emph{Commentary}

(Note structural and statistical differences from the other models.)

\hypertarget{model-m2-1}{%
\section{Model: m2}\label{model-m2-1}}

\emph{Commentary}

(Note structural and statistical differences from the other models.)

\hypertarget{model-m3-1}{%
\section{Model: m3}\label{model-m3-1}}

\emph{Commentary}

(Note structural and statistical differences from the other models.)

\hypertarget{overall-assessment-1}{%
\section{Overall assessment}\label{overall-assessment-1}}

\href{https://ggplot2-book.org/arranging-plots.html}{Arrange plots in a grid using the \textbf{patchwork} package for ease of comparison.}

\includegraphics{_main_files/figure-latex/unnamed-chunk-189-1.pdf}

  \bibliography{cites.bib}

\end{document}
