# Appendix B - Canons of Empirical Method {-}

## Canons of Empirical Method

Our approach to decision making is not ethereal, it is concrete, yes, but supplemented with insight, so much so we feel compelled to decide on something different. Our horizons will remain static and decay; or they will expand and grow. We might ponder this question using Bernard @lonergan_insight **Canons of Empirical Method**. Here is a synopsis of these guidelines. There are six: selection, operations, relevance, followed by complete explanation, parsimony, and statistical residues.

1. **Selection.**. This canon would restrict us to sensible data, data we can observe as existing outside of our mental images of the data in our minds. 

- We can only use propositions against which we can apply the restricted sensible data. 

- If we cannot do this, we cannot possibly search for intelligible anything in objective reality, namely the pattern in the data susceptible to description, explanation, and perhaps prediction. Operations is a many-headed hydra.

2. **Operations.** This canon is responsible for expanding our consciousness about data using the cumulative font of previously verified, and unverified, work. This means we use prior research to inform answers to current questions, but it must be relevant. Operations also helps us construct mentally, for all analysis is a figment of the imagination and the intellect, but applied to the ever expanding font of previously verified work. 

- Constructions are cumulative and so too are they verified. We keep what works and throw the rest into a bin for reuse, if only to remember what does not work. In this way the accumulation of works are also systematic in that different pieces of constructions provide integrity, harmony, and even clarity to any analysis. And if they do not, then they go into the bin for reuse, repurposing. With operational constructions we understand and have a history of previous endeavors, how well they did or did not work, what we can use or discard from previous analyses. 

- In a word or two, operations allow us to transcend our current state of operations and envision a different, hopefully better by some rubric, way to proceed. All of this is the answer to a question about the shape or form of the analysis. It is ultimately the work of re-selection and the harbinger of relevance.

3. **Relevance.** Relevant data, propositions, criteria for choosing one intelligible pattern rather than another, one decision alternative than another, all presuppose that data can be applied to different sets of hypotheses, inform different species of decisions, each require different sets of criteria. Data are raw and that flexible. Using statistics as derived data can be even more problematic. Averages of transactions can wash out very high and very low levels of measures, both informative in their extremes. Perhaps they might be the only relevant part of a data set, whereas the averages tell us nothing much interesting about the pattern of the data we call intelligibility. And thus when applied to a decision the analysis is irrelevant, plausibly. 

- A most important part of the Canon of Relevance is that it views data as emanating from beings as they relate to one another, not to our measurements, the instruments by which we measure and record, even to the observers.

- This canon is a first step to a principled notion of objectivity a built on the immediately restrictive selection of terms, propositions, arguments, and verified operational explanations. There are immanent patterns latent in all data which has nothing to do with how the analysis is run, why we are analyzing something in the first case, even what are the materials and resources involved in the analysis. These immanent patterns in sensible data we are collecting are part and parcel of the formal causality of the analysis. Nascent, inchoate somewhat, yes, but immanently and thus relevantly critical to our analysis mission.

4. **Complete Explanation.** The end of all inquiry is to gain as complete an explanation of observed data given our various conjectures, theories, and hypotheses about the data. But the data is not only how many widgets we make a minute in several facilities linked by rail, it is also the data of what guides our priorities in deciding what widgets, facilities, linkages are more important than others. We thus have observed data, so-called hard data, and more latent preferences based on norms, guidelines, conscience, in a word, values. This is the bedrock of making good decisions. The necessary triad of selection, operations, and relevance then erupts into the best of all possible current worlds, including our bias, of a way forward.

5. **Parsimony.** Apparently William of Ockham never had the proverbial razor. What he actually stated was "Numquam pluralitas non est ponenda sine necessitate." (roughly now: any plurality is not to be posited without necessity.) This principle is a component of the Canon of Parsimony, wherein any scientist must eliminate any statement which is not verifiable and include only those statements which, currently, are. There are two sides to the parsimony coin.

- We have just seen that the Canon of Relevance also requires the parsimonious inclusion of any insights which add to the raw data. While laws of nature, including human behavior in managerial decisions, might in a particular context seem to fail, repeated experiments, contexts, situations should be able to come to a statistical law which can be affirmed as verified. The key is to understand the difference between a law and an event. 

- Events can deviate from laws (trends, hypotheses, models), but to be verifiable they cannot deviate in any systematic way. If they do the other side of parsimony directs us to consider including the perhaps newly discovered deviation into a new formulation of the law, trend, or model. ^[This is an often used principle of logic and selection of the minimal set of hypotheses needed to answer a question. St. Thomas Aquinas uses this principle to set up an objection to the existence of God, which he roundly refutes by the superiority  of causal over simple explanations of anything. @aquinas_st, *Summa Theologiae*, q.3, a.3, ad 2.

6. **Statistical Residues.** Yes, outliers finally. These are particular, concrete situations not contemplated by all of the systematic analysis we try to rely upon. Yet, we make the decision, not knowing exactly how the decision will play out. We reserve for failures, and often omit the asteroid-event we missed along the way. We are not only imperfect reasoners, we are also imperfect, incomplete decision makers.

- Our brilliant ideas are abstractions from practical reality. Thus they can only be further explained and used to predict when yoked with the concrete circumstances of new events as they unfold, thus the need for a Canon of Operations. Thus also more work by the imperfect human reasoner is always needed to determine further insights mashed together with concrete situations. Abstraction cannot simply be the impoverished replicas of observed data. They must exceed, transcend the data, by operating on the data with new insights.

- As if this is not enough, if a new situation arises, then unsystematic deviations might need either to be incorporated into new models or else discarded as residues, at least relative to the abstraction called a model. But one last point is that if the abstract understanding called a model begins to degenerate, there is the "inverse insight" that this degeneration is also intelligible and must be reported as a claim to be acted upon in a decision. Thus this canon seems to point to another application of the Canon of Relevance to focus our attention on what claims, insights, theories, add to the data and its (relatively) complete explanation.

## Rules to think by

@Zellner1971 espouses a Bayesian analysis of inferential deductive logic for hypotheses as well as for decisions. He follows @Jeffreys1966 and his nine rules of inductive inference. We can restate them here using the epistemological language of @lonergan_insight and his canons of general empirical method.^[The canons are listed, related to one another, and explicated in _Insight_, chapter 3. Therein is a thematic approach to the inference rules set out by Jeffreys, Jaynes and Zellner.]

**Rule 1.** "All hypotheses used may be explicitly stated, and the conclusions must follow from the hypotheses." (@Jeffreys1966, p. 8) A complete set of hypotheses, decision alternatives, must be explicit to the analyst and the consumer of the decision maker. But further, these hypotheses must deductively result in a "complete explanation" (@lonergan_insight, p. 107-109) in the sense that for a universe of discourse carved out by the analyst and decision maker, the hypotheses must be able to result in a comprehensive set of conclusions. One imagines the wrangle over the logically inconsistent use of _modus tollens_ to base inferences with the Null Hypothesis Significance Test framework. If _modus tollens_ is $\text{IF A, THEN B, AND NOT B, THEN NOT A}$ then our deduction that $\text{NOT A}$ is valid. However if we deny the antecedent and argue $\text{IF A, THEN B, AND NOT A, THEN NOT B}$ we deduce inconsistently by denying the antecedent.^[See this note in the _Oxford Reference_ site: <https://www.oxfordreference.com/display/10.1093/oi/authority.20110803095711627>] 

```{example}
_______________________________________________
**If burglars, who do not have a key, entered by the front door ($A$ the antecedent), then they must have forced the lock, which requires a key ($B$ the consequent).** 

Now suppose we have data which indicates that **the lock was not forced, $\sim B$, denying the consequent**. We can deduce from this data, namely, that the burglars did not force the lock, $\sim B$, that thus they also did not enter by the front door, $\sim A$. This is _**modus tollens**_ or **denying the consequent**.

If the data we have is that the burglars did not enter by the front door, $\sim A$ denying the antecedent, then it is illogical to deduce that they did not force the lock. $\sim B$. This is the fallacy of **denying the antecedent**. They might have forced the lock, was startled by the owners' attack cat, and decided to enter the house by another route.
_______________________________________________
```

**Rule 2.** "The theory must be self-consistent; that is, it must not be possible to derive contradictory conclusions from the postulates and any given set of observational data." (@Jeffreys1966, p. 5) This rule works in tandem with the next rule.

**Rule 3.** "Any rule must be applicable in practice. A definition is useless unless the thing defined can be recognized in terms of the definition when it occurs. The existence of a thing or the estimate of a quantity must not involve an impossible experiment." (@Jeffreys1966, p. 5) Here are @lonergan_insight's Canons of Selection, Relevance and Operations at work. We can unpack this rule by a three-way division of labor. But we will only stop by the Canon of Relevance here. We defer to Rule 5 for Canons of Selection and Operations, not that they do not apply here! There is strong connection between Rules 3 and 5 bridged by Operations and Selection within the context of Operations. Both of these point to Relevance, here. In fact it seems that Relevance and Operations are flip-sides of the same coin. 

Relevant data, propositions, criteria for choosing one intelligible pattern rather than another, one decision alternative than another, all presuppose that data can be applied to different sets of hypotheses, inform different species of decisions, each require different sets of criteria. Data are raw and that flexible. Using statistics as derived data can be even more problematic. Averages of transactions can wash out very high and very low levels of measures, both informative in their extremes. Perhaps they might be the only relevant part of a data set, whereas the averages tell us nothing much interesting about the pattern of the data we call intelligibility. And thus when applied to a decision the analysis is irrelevant, plausibly. A most important part of the Canon of Relevance is that it views data as emanating from beings as they relate to one another, not to our measurements, the instruments by which we measure and record, even to the observers. This canon is a first step to a principled notion of objectivity. There is are immanent patterns latent in all data which has nothing to do with how the analysis is run, why we are analyzing something in the first case, even what are the materials and resources involved in the analysis. These immanent patterns in sensible data we are collecting are part and parcel of the formal causality of the analysis. Nascent, inchoate somewhat, yes, but immanently and thus relevantly critical to our analysis mission.

**Rule 4.** "The theory must provide explicitly for the possibility that inference made by it may turn out to be wrong." (@Jeffreys1966, p. 6) Jeffreys goes on to state, categorically, we might be wrong do to error, incomplete and evolving data, and an attitude of never allowing for revision. On the same page he goes on to say "... [W]e have a certain amount of confidence that it will be right in any particular case, though this confidence does not amount to logical certainty." ( _ibid._) @lonergan_insight agrees and uses this notion to buttress an uncertainty principle in our analytical work. "[While i]t is true enough that data are hazy, that measurements are not perfectly accurate, that measuring can distort the measured object ... [, yet one] can affirm them [and] continue to misconceive classical laws[, such falling bodies.]" (p. 125) How do we characterize the ways of affirming the indeterminant nature of laws in the concrete? Not imaginatively, but as the "indeterminancy of the abstract." (p. 125) 

**Rule 5.** "The theory must not deny any empirical proposition _a priori_; any precisely stated empirical proposition must be formally capable of being accepted, in the sense of the last rule [4], given a moderate amount of relevant evidence." (@Jeffreys1966, p. 6) Yes, only relevant propositions are allowed. The only relevant propositions are those that have some sort of chance of surviving the rigors of the Canon of Selection (@lonergan_insight, p. 94-97) and the Canon of Operations (p. 97-99) implicit in our formal approach to finding patterns, intelligibility in otherwise unintelligible data. Selection would restrict us to sensible data, data we can observe as existing outside of our mental images of the data in our minds. This canon and here Rule 5 restrict us to propositions against which we can apply the restricted sensible data. If we cannot do this, we cannot possibly search for intelligible anything in objective reality, namely the pattern in the data susceptible to description, explanation, and perhaps prediction. Operations is a many-headed hydra.

Operations would help us expand our consciousness about data using the cumulative font of previous work. This means we use prior research to inform answers to current questions, but it must be relevant. Operations also helps us construct mentally, for all analysis is a figment of the imagination and the intellect. Constructions are cumulative and so are they verified. We keep what works and throw the rest into a bin for reuse. In this way they are also systematic in that different pieces of constructions provide integrity, harmony, and even clarity to any analysis. And if they do not, then they go into the bin for reuse, repurposing. With operational constructions we understand and have a history of previous endeavors, how well they did or did not work, what we can use or discard from previous analyses. In a word or two, operations allow us to transcend our current state of operations and envision a different, hopefully better by some rubric, way to proceed. All of this is the answer to a question about the shape or form of the analysis.

These five rules are considered by many, including Jeffreys and Zellner, to be _essential_. Jeffreys also indicates three more useful rules.

**Rule 6.** "The number of postulates should be reduced to a minimum." (@Jeffreys1966, p. 6) This is often stated as a variant of William of Occam's Razor. What William actually stated was apparently "Numquam pluralitas non est ponenda sine necessitate." (Any plurality is not to be posited without necessity.) This principle is a component of the Canon of Parsimony for @lonergan_insight, p.102, wherein any scientist must eliminate any statement which is not verifiable and include only those statements which, currently, are. On the other hand, Lonergan's Canon of Relevance (p. 103) also requires the albeit parsimonious inclusion of any insights which add to the raw data. While laws of nature might in a particular experiment seem to fail, repeated experiments should be able to come to a statistical law which can be affirmed as verified. The key is to understand the difference between a law and an event. Events can deviate from laws (trends, hypotheses, models), but to be verifable they cannot deviate in any systematic way. If they do the other side of parsimony directs us to consider including the perhaps newly discovered deviatoin into a new formulation of the law, trend, or model. ^[This is an often used principle of logic and selection of the minimal set of hypotheses needed to answer a question. St. Thomas Aquinas uses this principle to set up an objection to the existence of God, which he roundly refutes by the superiority  of causal over simple explanations of anything. _Summa Theologiae_, q.3, a.3, ad 2. ]

**Rule 7**. "While we do not regard the human mind as a perfect reasoner, we must accept it as the only one available. The theory need not represent actual thought processes in detail, but should agree with them in outline."" (@Jeffreys1966, p. 9) As with @lonergan_insight, p. 124-125, our brilliant ideas are abstractions from practical reality. Thus they can only be further explained and used to predict when yoked with the concrete circumstances of new events as they unfold. Thus also more work by Jeffreys' imperfect human reasoner is always needed to determine further insights mashed together with concrete situations. As if this is not enough, Lonergan goes on to imply if a new situation arises, then unsystematic deviations might need either to be incorporated into new models or else discarded as residues, at least relative to the abstraction called a model. But one last point is that if the abstract understanding called a model begins to degenerate, there is the "inverse insight" (p. 125) that this degeneration is also intelligible and must be reported as a claim. Thus this rule seems to be a species of Lonergans Canon of Statistical Residues. But it also seems to require another Canon of Relevance to "fix our attention on what insight adds to data." (p. 125)

Something must be said about the imperfect human reasoner. And this will not be a foray as much as into psychology as it will be into epistemology, the study of how it is we know anything at all. The object is all being, asking the question _what is it?_ presupposing perhaps but at the least alongside the more basic question _is it?_ and all of this incompletely (we do not know all the rules of this road) and imperfectly (we will at times erroneous apply whatever rules we seem to know). The first question is _formal causality_, what is the nature of, blueprint for, scaffolding, framework, approach we will use for our analysis. The second question is about the existence of the object of our various desires to understand. But it raises a question of _final causality_ called _why bother?_ or _what's the purpose?_ the end we would like to achieve on this mission, should we accept it. Both of these raise two more questions of _material causality_, the what's in the process, the people, technology platforms, resources, materials, inputs, and _efficient causality_, the how the process of inputs, activities, technology, know-how, conventional wisdom, yielding desired outputs by decision makers. Let's apply this causal heuristic to get a thumbnail sketch of the imperfect human reasoner.

- The reasoner, or community of reasoners for that matter, are _aware_ (not not, remember the adjective imperfect) of raw data, use operations to observe and record the data, or at least somehow remember the data. 

- The reasoner then _understands_ the data by throwing provisional hypotheses like darts onto a known dart board. There is a center on this dart board, the goal, the finality of the understanding. Some darts stick (they are consistent with the data), others eventually will fall off to the floor. Whatever is left is the pattern of data we might call the beginnings of an understanding. 

- While there are several pointed projectiles on the dartboard only those close enough to the center are very plausible, others not so. We can even measure the distances of darts to the center to quantify our analysis. We might say at this point that we can make a first judgment based on the _is it?_ question of _yes it is_. 

There is a pattern in the data. This is what we will mean by _systematic_. It might also answer something about the nature of the data, the _what is it?_. Attributes like long-lived, very complicated, and it is blue by the way can be verified on the dartboard. We have verified in data, using a reasonable approach to belief in plausibility, called a distance. We can even add up all of the distances to normalize the distance in a metric we call a probability. Quite a leap and all part of the answer to the question of _what is it?_ and the blueprint we are using to answer questions at all.

**Rule 8.** "In view of the great complexity of induction, we cannot help to develop it more thoroughly than deduction." (@Jeffreys1966, p. 10) And thus the role of the use of the logic of pure mathematics. But if we use pure mathematics in this logical way, then the notion of probability is not that of empirical frequencies as we usually view them in elementary statistics courses, rather the ordered degrees of reasonable, rational, belief about a claim. This means also that any inductively constructed claim should emanate from a deductively valid logic. One builds on the other. One verifies the other in concrete events. One provides a deduced ordering of hypotheses when hypotheses are confronted by data. There is thus some sort of _uncertainty principle_ lurking here as Lonergan points out: "For the concrete includes a nonsystematic component, and so the concrete cannot be deduced in its full determinancy from any set of systematic premises." (@lonergan_insight, p. 123)